{"meta":{"title":"荒原饮露","subtitle":"可能是未来的架构师，也可能送外卖。","description":"","author":"荒原饮露","url":"https://www.missf.top","root":"/"},"pages":[{"title":"","date":"2021-09-02T03:08:51.235Z","updated":"2020-09-11T10:56:10.000Z","comments":false,"path":"categories/index.html","permalink":"https://www.missf.top/categories/index.html","excerpt":"","text":""},{"title":"","date":"2021-09-02T03:08:51.052Z","updated":"2020-04-13T10:37:42.000Z","comments":false,"path":"tags/index.html","permalink":"https://www.missf.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"运维备忘录","slug":"运维备忘录","date":"2021-05-22T08:53:50.000Z","updated":"2021-05-22T08:53:50.000Z","comments":true,"path":"post/78df5ed2.html","link":"","permalink":"https://www.missf.top/post/78df5ed2.html","excerpt":"","text":"运维备忘录 由于我们脑⼒有限，忘性却很好，所以就有了这个文档。 Kubernetes 在容器中执行命令 官方示例 kubectl exec (POD | TYPE/NAME) [-c CONTAINER] [flags] -- COMMAND [args...] kubectl exec -it $i -- date 等同于 kubectl exec -it $i -- sh -c 'date' (COMMAND==sh -c,args=='date') 记录缘由 for i in $(kubectl get pod | grep tomcat | awk '{print $1}'); do kubectl exec -it $i -- sh -c 'rm -rf webapps;mv webapps.dist webapps'; done 不生成文件末尾的换行符 [root@k8s-master ~/kubernetes/configmap]# echo 'qdqfewf' > test [root@k8s-master ~/kubernetes/configmap]# cat -A test qdqfewf$ [root@k8s-master ~/kubernetes/configmap]# echo -n 'qdqfewf' > test [root@k8s-master ~/kubernetes/configmap]# cat -A test qdqfewf[root@k8s-master ~/kubernetes/configmap]#","categories":[],"tags":[]},{"title":"Kubernetes 学习笔记","slug":"Kubernetes 学习笔记","date":"2021-03-08T10:46:59.000Z","updated":"2021-08-27T06:59:56.000Z","comments":true,"path":"post/7f42a634.html","link":"","permalink":"https://www.missf.top/post/7f42a634.html","excerpt":"","text":"概述 此文档仅为个人的学习笔记，记录下来是为了以后遗忘时可以翻阅。最好的 Kubernetes 文档在 kubernetes.io，请大家去官网学习！ Kubernetes 是什么？Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。 名称 Kubernetes 源于希腊语，意为“舵手”或“飞行员”。Google 在 2014 年开源了 Kubernetes 项目， Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验 的基础上，结合了社区中最好的想法和实践。 时光回溯让我们回顾一下为什么 Kubernetes 如此有用。 传统部署时代： 早期，各个组织机构在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。 例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况， 结果可能导致其他应用程序的性能下降。 一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展， 并且维护许多物理服务器的成本很高。 虚拟化部署时代： 作为解决方案，引入了虚拟化。虚拟化技术允许你在单个物理服务器的 CPU 上运行多个虚拟机（VM）。 虚拟化允许应用程序在 VM 之间隔离，并提供一定程度的安全，因为一个应用程序的信息不能被另一应用程序随意访问。 虚拟化技术能够更好地利用物理服务器上的资源，并且因为可轻松地添加或更新应用程序而可以实现更好的可伸缩性，降低硬件成本等等。 每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。 容器部署时代： 容器类似于 VM，但是它们具有被放宽的隔离属性，可以在应用程序之间共享操作系统（OS）。 因此，容器被认为是轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。 容器因具有许多优势而变得流行起来。下面列出的是容器的一些好处： 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建/发布时而不是在部署时创建应用程序容器镜像， 从而将应用程序与基础架构分离。 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。 跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。 Kubernetes 能做什么？容器是打包和运行应用程序的最好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果由系统处理此行为，会不会更容易？ 这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。 Kubernetes 为你提供： 服务发现和负载均衡 Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。 存储编排 Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。 自动部署和回滚 你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。 自动完成装箱计算 Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。 自我修复 Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 密钥与配置管理 Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。 Kubernetes 不是什么？Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。 由于 Kubernetes 在容器级别而不是在硬件级别运行，它提供了 PaaS 产品共有的一些普遍适用的功能， 例如部署、扩展、负载均衡、日志记录和监视。 但是，Kubernetes 不是单体系统，默认解决方案都是可选和可插拔的。 Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。 Kubernetes： 不限制支持的应用程序类型。 Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。 如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。 不部署源代码，也不构建你的应用程序。 持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。 不提供应用程序级别的服务作为内置服务，例如中间件（例如，消息中间件）、 数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存、集群存储系统 （例如，Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在 Kubernetes 上的应用程序通过可移植机制（例如， 开放服务代理）来访问。 不要求日志记录、监视或警报解决方案。 它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。 不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API， 该声明性 API 可以由任意形式的声明性规范所构成。 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。 此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。 编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。 相比之下，Kubernetes 包含一组独立的、可组合的控制过程， 这些过程连续地将当前状态驱动到所提供的所需状态。 如何从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用 且功能更强大、系统更健壮、更为弹性和可扩展。 Kubernetes 组件当你部署完 Kubernetes, 即拥有了一个完整的集群。 一个 Kubernetes 集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。 工作节点托管作为应用负载的组件的 Pod 。控制平面管理集群中的工作节点和 Pod 。 为集群提供故障转移和高可用性，这些控制平面一般跨多主机运行，集群跨多个节点运行。 本文档概述了交付正常运行的 Kubernetes 集群所需的各种组件。 控制平面组件（Control Plane Components）控制平面的组件对集群做出全局决策（比如调度），以及检测和响应集群事件（例如，当不满足部署的 replicas 字段时，启动新的 pod）。 控制平面组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机（一般使用 master 节点）上启动所有控制平面组件，并且不会在此计算机上运行用户容器。 kube-apiserverAPI 服务器是 Kubernetes 控制面的组件， 该组件公开了 Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。 Kubernetes API 服务器的主要实现是 kube-apiserver。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。 etcdetcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。 你的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。 要了解 etcd 更深层次的信息，请参考 etcd 文档。 kube-scheduler控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。 调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。 kube-controller-manager在主节点上运行控制器的组件。 从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。 这些控制器包括: 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应 副本控制器（Replication Controller）: 负责为系统中的每个副本控制器对象维护正确数量的 Pod 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod) 服务帐户和令牌控制器（Service Account &amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌 Node 组件节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。 kubelet一个在集群中每个节点（node）上运行的代理。 它保证容器（containers）都运行在 Pod 中。 kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。 kube-proxykube-proxy 是集群中每个节点上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。 kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。 如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。 容器运行时（Container Runtime）容器运行环境是负责运行容器的软件。 Kubernetes 支持多个容器运行环境: Docker、 containerd、CRI-O 以及任何实现 Kubernetes CRI (容器运行环境接口)。 插件（Addons）插件使用 Kubernetes 资源（DaemonSet、 Deployment等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 kube-system 命名空间。 DNS尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该有集群 DNS， 因为很多示例都需要 DNS 服务。 集群 DNS 是一个 DNS 服务器，和环境中的其他 DNS 服务器一起工作，它为 Kubernetes 服务提供 DNS 记录。 Kubernetes 启动的容器自动将此 DNS 服务器包含在其 DNS 搜索列表中。 Web 界面（仪表盘）Dashboard 是Kubernetes 集群的通用、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。 Ingress Controller为了让 Ingress 资源工作，集群必须有一个正在运行的 Ingress 控制器。 容器资源监控容器资源监控将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面，参考 Prometheus 项目 结合 Grafana 项目。 集群层面日志集群层面日志 机制负责将容器的日志数据保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。 Kubernetes APIKubernetes 控制面的核心是 API 服务器。 API 服务器负责提供 HTTP API，以供用户、集群中的不同部分和集群外部组件相互通信。 Kubernetes API 使你可以查询和操纵 Kubernetes API 中对象（例如：Pod、Namespace、ConfigMap 和 Event）的状态。 大部分操作都可以通过 kubectl 命令行接口或类似 kubeadm 这类命令行工具来执行， 这些工具在背后也是调用 API。不过，你也可以使用 REST 调用来访问这些 API。 使用 Kubernetes 对象理解 Kubernetes 对象本页说明了 Kubernetes 对象在 Kubernetes API 中是如何表示的，以及如何在 .yaml 格式的文件中表示。 理解 Kubernetes 对象在 Kubernetes 系统中，Kubernetes 对象是持久化的实体。 Kubernetes 使用这些实体去表示整个集群的状态。特别地，它们描述了如下信息： 哪些容器化应用在运行（以及在哪些节点上运行） 可以被应用使用的资源 关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略 Kubernetes 对象是 “目标性记录” —— 一旦创建对象，Kubernetes 系统将持续工作以确保对象存在。 通过创建对象，本质上是在告知 Kubernetes 系统，所需要的集群工作负载看起来是什么样子的， 这就是 Kubernetes 集群的期望状态（Desired State）。 操作 Kubernetes 对象 —— 无论是创建、修改，或者删除 —— 都需要使用 Kubernetes API。 比如，当使用 kubectl 命令行接口时，CLI 会执行必要的 Kubernetes API 调用， 也可以在程序中使用 客户端库直接调用 Kubernetes API。 对象规约（Spec）与状态（Status）几乎每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置： 对象 spec（规约）和对象 status（状态） 。 对于具有 spec 的对象，你必须在创建对象时设置其内容，描述你希望对象所具有的特征： 期望状态（Desired State） 。 status 描述了对象的当前状态（Current State），它是由 Kubernetes 系统和组件设置并更新的。在任何时刻，Kubernetes 控制平面 都一直积极地管理着对象的实际状态，以使之与期望状态相匹配。 描述 Kubernetes 对象创建 Kubernetes 对象时，必须提供对象的规约，用来描述该对象的期望状态， 以及关于对象的一些基本信息（例如名称）。 当使用 Kubernetes API 创建对象时（或者直接创建，或者基于kubectl）， API 请求必须在请求体中包含 JSON 格式的信息。 大多数情况下，需要在 .yaml 文件中为 kubectl 提供这些信息。kubectl 在发起 API 请求时，会将这些信息转换成 JSON 格式。 必需字段在想要创建的 Kubernetes 对象对应的 .yaml 文件中，需要配置如下的字段： apiVersion - 创建该对象所使用的 Kubernetes API 的版本 kind - 想要创建的对象的类别 metadata - 帮助唯一性标识对象的一些数据，包括一个 name 字符串、UID 和可选的 namespace Kubernetes 对象管理命令式命令用命令式命令时，用户可以在集群中的活动对象上进行操作。用户将操作传给 kubectl 命令作为参数或标志。 这是开始或者在集群中运行一次性任务的最简单方法。因为这个技术直接在活动对象上操作，所以它不提供以前配置的历史记录。 通过创建 Deployment 对象来运行 nginx 容器的实例： kubectl run nginx --image nginx 命令式对象配置在命令式对象配置中，kubectl 命令指定操作（创建，替换等），可选标志和至少一个文件名。指定的文件必须包含 YAML 或 JSON 格式的对象的完整定义。 创建配置文件中定义的对象： kubectl create -f nginx.yaml 对象名称和 IDs集群中的每一个对象都有一个名称来标识在同类资源中的唯一性。 每个 Kubernetes 对象也有一个 UID 来标识在整个集群中的唯一性。 比如，在同一个名字空间中有一个名为 myapp-1234 的 Pod, 但是可以命名一个 Pod 和一个 Deployment 同为 myapp-1234. 对于用户提供的非唯一性的属性，Kubernetes 提供了 标签（Labels）和 注解（Annotation）机制。 名字空间Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。 这些虚拟集群被称为名字空间。 何时使用多个名字空间名字空间适用于存在很多跨多个团队或项目的用户的场景。对于只有几到几十个用户的集群，根本不需要创建或考虑名字空间。当需要名称空间提供的功能时，请开始使用它们。 名字空间为名称提供了一个范围。资源的名称需要在名字空间内是唯一的，但不能跨名字空间。 名字空间不能相互嵌套，每个 Kubernetes 资源只能在一个名字空间中。 名字空间是在多个用户之间划分集群资源的一种方法（通过资源配额）。 不需要使用多个名字空间来分隔轻微不同的资源，例如同一软件的不同版本： 使用标签来区分同一名字空间中的不同资源。 使用名字空间名字空间的创建和删除在名字空间的管理指南文档描述。 说明： 避免使用前缀 kube- 创建名字空间，因为它是为 Kubernetes 系统名字空间保留的。 查看名字空间你可以使用以下命令列出集群中现存的名字空间： kubectl get namespace Kubernetes 会创建四个初始名字空间： default 没有指明使用其它名字空间的对象所使用的默认名字空间。 kube-system Kubernetes 系统创建对象所使用的名字空间。 kube-public 这个名字空间是自动创建的，所有用户（包括未经过身份验证的用户）都可以读取它。 这个名字空间主要用于集群使用，以防某些资源在整个集群中应该是可见和可读的。 这个名字空间的公共方面只是一种约定，而不是要求。 kube-node-lease 此名字空间用于与各个节点相关的租期（Lease）对象； 此对象的设计使得集群规模很大时节点心跳检测性能得到提升。 为请求设置名字空间要为当前请求设置名字空间，请使用 --namespace 参数。 例如： kubectl run nginx --image=nginx --namespace= kubectl get pods --namespace= 设置名字空间偏好你可以永久保存名字空间，以用于对应上下文中所有后续 kubectl 命令。 名字空间和 DNS当你创建一个服务 时， Kubernetes 会创建一个相应的 DNS 条目。 该条目的形式是 &lt;服务名称&gt;.&lt;名字空间名称&gt;.svc.cluster.local，这意味着如果容器只使用 &lt;服务名称&gt;，它将被解析到本地名字空间的服务。这对于跨多个名字空间（如开发、分级和生产） 使用相同的配置非常有用。如果你希望跨名字空间访问，则需要使用完全限定域名（FQDN）。 并非所有对象都在名字空间中大多数 kubernetes 资源（例如 Pod、Service、副本控制器等）都位于某些名字空间中。 但是名字空间资源本身并不在名字空间中。而且底层资源，例如 节点 和持久化卷不属于任何名字空间。 查看哪些 Kubernetes 资源在名字空间中，哪些不在名字空间中： # 位于名字空间中的资源 kubectl api-resources --namespaced=true # 不在名字空间中的资源 kubectl api-resources --namespaced=false 标签和选择算符标签（Labels）是附加到 Kubernetes 对象（比如 Pods）上的键值对。 标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。 标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。 每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。 动机标签使用户能够以松散耦合的方式将他们自己的组织结构映射到系统对象，而无需客户端存储这些映射。 服务部署和批处理流水线通常是多维实体（例如，多个分区或部署、多个发行序列、多个层，每层多个微服务）。 管理通常需要交叉操作，这打破了严格的层次表示的封装，特别是由基础设施而不是用户确定的严格的层次结构。 示例标签： &quot;release&quot; : &quot;stable&quot;, &quot;release&quot; : &quot;canary&quot; &quot;environment&quot; : &quot;dev&quot;, &quot;environment&quot; : &quot;qa&quot;, &quot;environment&quot; : &quot;production&quot; &quot;tier&quot; : &quot;frontend&quot;, &quot;tier&quot; : &quot;backend&quot;, &quot;tier&quot; : &quot;cache&quot; &quot;partition&quot; : &quot;customerA&quot;, &quot;partition&quot; : &quot;customerB&quot; &quot;track&quot; : &quot;daily&quot;, &quot;track&quot; : &quot;weekly&quot; 这些只是常用标签的例子; 你可以任意制定自己的约定。请记住，对于给定对象标签的键必须是唯一的。 语法和字符集标签是键值对。有效的标签键有两个段：可选的前缀和名称，用斜杠（/）分隔。 名称段是必需的，必须小于等于 63 个字符，以字母数字字符（[a-z0-9A-Z]）开头和结尾， 带有破折号（-），下划线（_），点（ .）和之间的字母数字。 前缀是可选的。如果指定，前缀必须是 DNS 子域：由点（.）分隔的一系列 DNS 标签，总共不超过 253 个字符， 后跟斜杠（/）。 如果省略前缀，则假定标签键对用户是私有的。 向最终用户对象添加标签的自动系统组件（例如 kube-scheduler、kube-controller-manager、 kube-apiserver、kubectl 或其他第三方自动化工具）必须指定前缀。 kubernetes.io/ 前缀是为 Kubernetes 核心组件保留的。 有效标签值必须为 63 个字符或更少，并且必须为空或以字母数字字符（[a-z0-9A-Z]）开头和结尾， 中间可以包含破折号（-）、下划线（_）、点（.）和字母或数字。 标签选择算符与名称和 UID 不同， 标签不支持唯一性。通常，我们希望许多对象携带相同的标签。 通过 标签选择算符，客户端/用户可以识别一组对象。标签选择算符是 Kubernetes 中的核心分组原语。 API 目前支持两种类型的选择算符：基于等值的 和 基于集合的。标签选择算符可以由逗号分隔的多个 需求 组成。 在多个需求的情况下，必须满足所有要求，因此逗号分隔符充当逻辑 与（&amp;&amp;）运算符。 空标签选择算符或者未指定的选择算符的语义取决于上下文， 支持使用选择算符的 API 类别应该将算符的合法性和含义用文档记录下来。 基于等值的需求基于等值 或 基于不等值 的需求允许按标签键和值进行过滤。 匹配对象必须满足所有指定的标签约束，尽管它们也可能具有其他标签。 可接受的运算符有 = 、 == 和 != 三种。 前两个表示相等（并且只是同义词），而后者表示不相等。例如： environment = production tier != frontend 基于集合的需求基于集合 的标签需求允许你通过一组值来过滤键。 支持三种操作符：in、notin 和 exists (只可以用在键标识符上)。例如： environment in (production, qa) tier notin (frontend, backend) partition !partition 第一个示例选择了所有键等于 environment 并且值等于 production 或者 qa 的资源。 第二个示例选择了所有键等于 tier 并且值不等于 frontend 或者 backend 的资源，以及所有没有 tier 键标签的资源。 第三个示例选择了所有包含了有 partition 标签的资源；没有校验它的值。 第四个示例选择了所有没有 partition 标签的资源；没有校验它的值。 类似地，逗号分隔符充当 与 运算符。因此，使用 partition 键（无论为何值）和 environment 不同于 qa 来过滤资源可以使用 partition, environment notin（qa) 来实现。 基于集合的标签选择算符是相等标签选择算符的一般形式，因为 environment=production 等同于 environment in（production）；!= 和 notin 也是类似的。 基于集合的要求可以与基于相等的要求混合使用。例如：partition in (customerA, customerB),environment!=qa。 注解你可以使用 Kubernetes 注解为对象附加任意的非标识的元数据。客户端程序（例如工具和库）能够获取这些元数据信息。 为对象附加元数据你可以使用标签或注解将元数据附加到 Kubernetes 对象。 标签可以用来选择对象和查找满足某些条件的对象集合。 相反，注解不用于标识和选择对象。 注解中的元数据，可以很小，也可以很大，可以是结构化的，也可以是非结构化的，能够包含标签不允许的字符。 注解和标签一样，是键/值对: \"metadata\": { \"annotations\": { \"key1\" : \"value1\", \"key2\" : \"value2\" } } 语法和字符集注解（Annotations）存储的形式是键/值对。有效的注解键分为两部分： 可选的前缀和名称，以斜杠（/）分隔。 名称段是必需项，并且必须在63个字符以内，以字母数字字符（[a-z0-9A-Z]）开头和结尾， 并允许使用破折号（-），下划线（_），点（.）和字母数字。 前缀是可选的。如果指定，则前缀必须是DNS子域：一系列由点（.）分隔的DNS标签， 总计不超过253个字符，后跟斜杠（/）。 如果省略前缀，则假定注解键对用户是私有的。 由系统组件添加的注解 （例如，kube-scheduler，kube-controller-manager，kube-apiserver，kubectl 或其他第三方组件），必须为终端用户添加注解前缀。 kubernetes.io/ 和 k8s.io/ 前缀是为 Kubernetes 核心组件保留的。 例如，下面是一个 Pod 的配置文件，其注解中包含 imageregistry: https://hub.docker.com/： apiVersion: v1 kind: Pod metadata: name: annotations-demo annotations: imageregistry: \"https://hub.docker.com/\" spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 字段选择器字段选择器（Field selectors）允许你根据一个或多个资源字段的值 筛选 Kubernetes 资源。 下面是一些使用字段选择器查询的例子： metadata.name=my-service metadata.namespace!=default status.phase=Pending 下面这个 kubectl 命令将筛选出 status.phase 字段值为 Running 的所有 Pod： kubectl get pods --field-selector status.phase=Running 支持的字段你可在字段选择器中使用 =、==和 != （= 和 == 的意义是相同的）操作符。 例如，下面这个 kubectl 命令将筛选所有不属于 default 命名空间的 Kubernetes 服务： kubectl get services --all-namespaces --field-selector metadata.namespace!=default 链式选择器同标签和其他选择器一样， 字段选择器可以通过使用逗号分隔的列表组成一个选择链。 下面这个 kubectl 命令将筛选 status.phase 字段不等于 Running 同时 spec.restartPolicy 字段等于 Always 的所有 Pod： kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always 多种资源类型你能够跨多种资源类型来使用字段选择器。 下面这个 kubectl 命令将筛选出所有不在 default 命名空间中的 StatefulSet 和 Service： kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default Kubernetes 架构节点Kubernetes 通过将容器放入在节点（Node）上运行的 Pod 中来执行你的工作负载。 节点可以是一个虚拟机或者物理机器，取决于所在的集群配置。 每个节点包含运行 Pods 所需的服务， 这些 Pods 由控制面负责管理。 通常集群中会有若干个节点；而在一个学习用或者资源受限的环境中，你的集群中也可能只有一个节点。 节点上的组件包括 kubelet、 容器运行时以及 kube-proxy。 管理向 API 服务器添加节点的方式主要有两种： 节点上的 kubelet 向控制面执行自注册。 你，或者别的什么人，手动添加一个 Node 对象。 在你创建了 Node 对象或者节点上的 kubelet 执行了自注册操作之后， 控制面会检查新的 Node 对象是否合法。例如，如果你使用下面的 JSON 对象来创建 Node 对象： { \"kind\": \"Node\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"10.240.79.157\", \"labels\": { \"name\": \"my-first-k8s-node\" } } } Kubernetes 会在内部创建一个 Node 对象作为节点的表示。Kubernetes 检查 kubelet 向 API 服务器注册节点时使用的 metadata.name 字段是否匹配。 如果节点是健康的（即所有必要的服务都在运行中），则该节点可以用来运行 Pod。 否则，直到该节点变为健康之前，所有的集群活动都会忽略该节点。 控制面到节点通信本文列举控制面节点（确切说是 API 服务器）和 Kubernetes 集群之间的通信路径。 目的是为了让用户能够自定义他们的安装，以实现对网络配置的加固，使得集群能够在不可信的网络上（或者在一个云服务商完全公开的 IP 上）运行。 节点到控制面Kubernetes 采用的是中心辐射型（Hub-and-Spoke）API 模式。 所有从集群（或所运行的 Pods）发出的 API 调用都终止于 apiserver（其它控制面组件都没有被设计为可暴露远程服务）。 apiserver 被配置为在一个安全的 HTTPS 端口（443）上监听远程连接请求， 并启用一种或多种形式的客户端身份认证机制。 一种或多种客户端鉴权机制应该被启用， 特别是在允许使用匿名请求 或服务账号令牌的时候。 应该使用集群的公共根证书开通节点，这样它们就能够基于有效的客户端凭据安全地连接 apiserver。 一种好的方法是以客户端证书的形式将客户端凭据提供给 kubelet。 请查看 kubelet TLS 启动引导 以了解如何自动提供 kubelet 客户端证书。 想要连接到 apiserver 的 Pod 可以使用服务账号安全地进行连接。 当 Pod 被实例化时，Kubernetes 自动把公共根证书和一个有效的持有者令牌注入到 Pod 里。 kubernetes 服务（位于所有名字空间中）配置了一个虚拟 IP 地址，用于（通过 kube-proxy）转发 请求到 apiserver 的 HTTPS 末端。 控制面组件也通过安全端口与集群的 apiserver 通信。 这样，从集群节点和节点上运行的 Pod 到控制面的连接的缺省操作模式即是安全的， 能够在不可信的网络或公网上运行。 控制面到节点从控制面（apiserver）到节点有两种主要的通信路径。 第一种是从 apiserver 到集群中每个节点上运行的 kubelet 进程。 第二种是从 apiserver 通过它的代理功能连接到任何节点、Pod 或者服务。 API 服务器到 kubelet从 apiserver 到 kubelet 的连接用于： 获取 Pod 日志 挂接（通过 kubectl）到运行中的 Pod 提供 kubelet 的端口转发功能。 这些连接终止于 kubelet 的 HTTPS 末端。 默认情况下，apiserver 不检查 kubelet 的服务证书。这使得此类连接容易受到中间人攻击， 在非受信网络或公开网络上运行也是 不安全的。 为了对这个连接进行认证，使用 –kubelet-certificate-authority 标志给 apiserver 提供一个根证书包，用于 kubelet 的服务证书。 如果无法实现这点，又要求避免在非受信网络或公共网络上进行连接，可在 apiserver 和 kubelet 之间使用 SSH 隧道。 最后，应该启用 kubelet 用户认证和/或鉴权 来保护 kubelet API。 apiserver 到节点、Pod 和服务从 apiserver 到节点、Pod 或服务的连接默认为纯 HTTP 方式，因此既没有认证，也没有加密。 这些连接可通过给 API URL 中的节点、Pod 或服务名称添加前缀 https: 来运行在安全的 HTTPS 连接上。 不过这些连接既不会验证 HTTPS 末端提供的证书，也不会提供客户端证书。 因此，虽然连接是加密的，仍无法提供任何完整性保证。 这些连接目前还不能安全地在非受信网络或公共网络上运行。 工作负载工作负载是在 Kubernetes 上运行的应用程序。 无论你的负载是单一组件还是由多个一同工作的组件构成，在 Kubernetes 中你可以在一组 Pods 中运行它。 在 Kubernetes 中，Pod 代表的是集群上处于运行状态的一组容器。 Kubernetes Pods 有确定的生命周期。 例如，一旦某 Pod 在你的集群中运行，Pod 运行所在的节点出现致命错误时， 所有该节点上的 Pods 都会失败。Kubernetes 将这类失败视为最终状态：即使该节点后来恢复正常运行，你也需要创建新的 Pod 来恢复应用。 不过，为了让用户的日子略微好过一些，你并不需要直接管理每个 Pod。 相反，你可以使用负载资源来替你管理一组 Pods。 这些资源配置控制器来确保合适类型的、处于运行状态的 Pod 个数是正确的，与你所指定的状态相一致。 Kubernetes 提供若干种内置的工作负载资源： Deployment 和 ReplicaSet （替换原来的资源 ReplicationController）。 Deployment 很适合用来管理你的集群上的无状态应用，Deployment 中的所有 Pod 都是相互等价的，并且在需要的时候被换掉。 StatefulSet 让你能够运行一个或者多个以某种方式跟踪应用状态的 Pods。 例如，如果你的负载会将数据作持久存储，你可以运行一个 StatefulSet，将每个 Pod 与某个 PersistentVolume 对应起来。你在 StatefulSet 中各个 Pod 内运行的代码可以将数据复制到同一 StatefulSet 中的其它 Pod 中以提高整体的服务可靠性。 DaemonSet 定义提供节点本地支撑设施的 Pods。这些 Pods 可能对于你的集群的运维是非常重要的，例如作为网络链接的辅助工具或者作为网络插件的一部分等等。每次你向集群中添加一个新节点时，如果该节点与某 DaemonSet 的规约匹配，则控制面会为该 DaemonSet 调度一个 Pod 到该新节点上运行。 Job 和 CronJob。 定义一些一直运行到结束并停止的任务。Job 用来表达的是一次性的任务，而 CronJob 会根据其时间规划反复运行。 podsPod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。 Pod（就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个） 容器； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器， 这些容器是相对紧密的耦合在一起的。 在非云环境中，在相同的物理机或虚拟机上运行的应用类似于在同一逻辑主机上运行的云应用。 除了应用容器，Pod 还可以包含在 Pod 启动期间运行的 Init 容器。 你也可以在集群中支持临时性容器 的情况外，为调试的目的注入临时性容器。 什么是 Pod？说明： 除了 Docker 之外，Kubernetes 支持很多其他容器运行时（容器运行时是负责运行容器的软件）， Docker 是最有名的运行时， 使用 Docker 的术语来描述 Pod 会很有帮助。 Pod 的共享上下文包括一组 Linux 名字空间、控制组（cgroup）和可能一些其他的隔离方面，即用来隔离 Docker 容器的技术。 在 Pod 的上下文中，每个独立的应用可能会进一步实施隔离。 就 Docker 概念的术语而言，Pod 类似于共享名字空间和文件系统卷的一组 Docker 容器。 使用 Pod通常你不需要直接创建 Pod，甚至单实例 Pod。 相反，你会使用诸如 Deployment 或 Job 这类工作负载资源来创建 Pod。如果 Pod 需要跟踪状态， 可以考虑 StatefulSet 资源。 Kubernetes 集群中的 Pod 主要有两种用法： 运行单个容器的 Pod。“每个 Pod 一个容器” 模型是最常见的 Kubernetes 用例； 在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。 运行多个协同工作的容器的 Pod。 Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。 这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众， 而另一个单独的 “挂斗”（sidecar）容器则刷新或更新这些文件。 Pod 将这些容器和存储资源打包为一个可管理的实体。 说明： 将多个并置、同管的容器组织到一个 Pod 中是一种相对高级的使用场景。 只有在一些场景中，容器之间紧密关联时你才应该使用这种模式。 每个 Pod 都旨在运行给定应用程序的单个实例。如果希望横向扩展应用程序（例如，运行多个实例以提供更多的资源），则应该使用多个 Pod，每个实例使用一个 Pod。 在 Kubernetes 中，这通常被称为副本（Replication）。 通常使用一种工作负载资源及其控制器来创建和管理一组 Pod 副本。 工作负载资源工作负载是指在 Kubernetes 上运行的应用程序，一个 pod 可以运行一个或多个工作负载。工作负载资源是用来管理一组 pod 的，确保这些被管理的 pod ，处于我们期望的运行状态。 Deployments一个 Deployment 为 Pods （pod 表示集群上正在运行的一组容器） 和 ReplicaSets （下一代副本控制器）提供声明式的更新能力。 你负责描述 Deployment 中的目标状态，而 Deployment 控制器（Controller）以受控速率更改实际状态， 使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment， 并通过新的 Deployment 收养其资源。 ReplicaSetReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。 因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。 StatefulSetsStatefulSet 是用来管理有状态应用的工作负载 API 对象。 StatefulSet 用来管理某 Pod 集合的部署和扩缩， 并为这些 Pod 提供持久存储和持久标识符。 和 Deployment 类似， StatefulSet 管理基于相同容器规约的一组 Pod。但和 Deployment 不同的是， StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的， 但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。 DaemonSetDaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时， 也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 DaemonSet 的一些典型用法： 在每个节点上运行集群守护进程 在每个节点上运行日志收集守护进程 在每个节点上运行监控守护进程 一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet。 一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet；每个具有不同的标志， 并且对不同硬件类型具有不同的内存、CPU 要求。 JobsJob 会创建一个或者多个 Pods，并将继续重试 Pods 的执行，直到指定数量的 Pods 成功终止。 随着 Pods 成功结束，Job 跟踪记录成功完成的 Pods 个数。 当数量达到指定的成功个数阈值时，任务（即 Job）结束。 删除 Job 的操作会清除所创建的全部 Pods。 一种简单的使用场景下，你会创建一个 Job 对象以便以一种可靠的方式运行某 Pod 直到完成。 当第一个 Pod 失败或者被删除（比如因为节点硬件失效或者重启）时，Job 对象会启动一个新的 Pod。 CronJobCronJob 创建基于时间调度的 Jobs。 一个 CronJob 对象就像 crontab (cron table) 文件中的一行。 它用 Cron 格式进行编写， 并周期性地在给定的调度时间执行 Job。 ReplicationControllerReplicationController 确保在任何时候都有特定数量的 Pod 副本处于运行状态。 换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的 ReplicationController 的替代方案ReplicaSetReplicaSet 是下一代 ReplicationController， 支持新的基于集合的标签选择算符。 它主要被 Deployment 用来作为一种编排 Pod 创建、删除及更新的机制。 请注意，我们推荐使用 Deployment 而不是直接使用 ReplicaSet，除非 你需要自定义更新编排或根本不需要更新。 Deployment （推荐） Deployment 是一种更高级别的 API 对象， 它以类似于 kubectl rolling-update 的方式更新其底层 ReplicaSet 及其 Pod。 如果你想要这种滚动更新功能，那么推荐使用 Deployment，因为与 kubectl rolling-update 不同， 它们是声明式的、服务端的，并且具有其它特性。 部署 Kubernetes 集群Kubernetes 官方提供多种部署方式： 云解决方案 使用部署工具安装 Kubernetes Windows Kubernetes 我们这里使用部署工具来安装 Kubernetes，官方提供的工具主要有以下几个： Kubeadm：官方维护的为了给创建 Kubernetes 集群提供最佳实践的一个工具，涉及集群生命周期管理等知识。 Kops ：在 AWS 上安装 Kubernetes 集群。 Kubespray：Ansible 部署，OS 级别通用的部署方式，可以是裸机和云的环境。 这里我们使用 Kubeadm 部署，这是官方推荐的部署方式，Kubeadm 可用于生产级别的集群部署。 准备开始 一台或多台运行着下列系统的机器： Ubuntu 16.04+ Debian 9+ CentOS 7+ Red Hat Enterprise Linux (RHEL) 7+ Fedora 25+ HypriotOS v1.0.1+ Flatcar Container Linux （使用 2512.3.0 版本测试通过） 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存) 2 CPU 核或更多 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以) 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。 开启机器上的某些端口（Kubernetes 服务所占用的端口必须开启） 禁用交换分区。为了保证 kubelet 正常工作，你必须禁用交换分区。 环境配置节点准备 k8s-master 10.10.110.190 k8s-ndoe1 10.10.110.191 k8s-node2 10.10.110.192 Operating System: Ubuntu 18.04.5 LTS 结构图 配置节点 # 关闭防火墙 systemctl stop ufw ufw disable # 检查所有节点网络接口的mac地址和product_uuid唯一性 ip link cat /sys/class/dmi/id/product_uuid # 禁用swap分区 swapoff -a # 设置主机名 hostnamectl set-hostname [hostname] # 配置hosts解析 cat >> /etc/hosts < EOF 10.10.110.190 k8s-master 10.10.110.191 k8s-node1 10.10.110.192 k8s-node2 EOF # 允许iptables检查桥接流量 modprobe br_netfilter lsmod | grep br_netfilter cat 10.244.169.160:53 Masq 1 0 0 TCP 10.96.0.10:9153 rr -> 10.244.36.73:9153 Masq 1 0 0 -> 10.244.169.160:9153 Masq 1 0 0 TCP 10.97.105.36:80 rr TCP 10.98.47.133:443 rr -> 10.244.36.72:8443 Masq 1 0 0 TCP 10.110.221.168:8000 rr -> 10.244.169.159:8000 Masq 1 0 0 TCP 10.244.235.192:30023 rr -> 10.244.36.72:8443 Masq 1 0 0 TCP 127.0.0.1:30023 rr -> 10.244.36.72:8443 Masq 1 0 0 UDP 10.96.0.10:53 rr -> 10.244.36.73:53 Masq 1 0 0 -> 10.244.169.160:53 Masq 1 0 0 IngressIngress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。 Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。 Ingress 是什么？Ingress 公开了从集群外部到集群内服务的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。 下面是一个将所有流量都发送到同一 Service 的简单 Ingress 示例： 可以将 Ingress 配置为服务提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及提供基于名称的虚拟主机等能力。 Ingress 控制器 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。 Ingress 不会公开任意端口或协议。 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 Service.Type=NodePort 或 Service.Type=LoadBalancer 类型的服务。 环境准备你必须具有 Ingress 控制器 才能满足 Ingress 的要求。 仅创建 Ingress 资源本身没有任何效果。 你可能需要部署 Ingress 控制器，例如 ingress-nginx。 你可以从许多 Ingress 控制器 中进行选择。 理想情况下，所有 Ingress 控制器都应符合参考规范。但实际上，不同的 Ingress 控制器操作略有不同。 Ingress 资源一个最小的 Ingress 资源示例： apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: minimal-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /testpath pathType: Prefix backend: service: name: test port: number: 80 与所有其他 Kubernetes 资源一样，Ingress 需要使用 apiVersion、kind 和 metadata 字段。 Ingress 对象的命名必须是合法的 DNS 子域名名称。 有关使用配置文件的一般信息，请参见部署应用、 配置容器、 管理资源。 Ingress 经常使用注解（annotations）来配置一些选项，具体取决于 Ingress 控制器，例如 重写目标注解。 不同的 Ingress 控制器 支持不同的注解。查看文档以供你选择 Ingress 控制器，以了解支持哪些注解。 Ingress 规约 提供了配置负载均衡器或者代理服务器所需的所有信息。 最重要的是，其中包含与所有传入请求匹配的规则列表。 Ingress 资源仅支持用于转发 HTTP 流量的规则。 Ingress 规则每个 HTTP 规则都包含以下信息： 可选的 host。在此示例中，未指定 host，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。 如果提供了 host（例如 foo.bar.com），则 rules 适用于该 host。 路径列表 paths（例如，/testpath）,每个路径都有一个由 serviceName 和 servicePort 定义的关联后端。 在负载均衡器将流量定向到引用的服务之前，主机和路径都必须匹配传入请求的内容。 backend（后端）是 Service 文档 中所述的服务和端口名称的组合。 与规则的 host 和 path 匹配的对 Ingress 的 HTTP（和 HTTPS ）请求将发送到列出的 backend。 通常在 Ingress 控制器中会配置 defaultBackend（默认后端），以服务于任何不符合规约中 path 的请求。 DefaultBackend没有 rules 的 Ingress 将所有流量发送到同一个默认后端。 defaultBackend 通常是 Ingress 控制器 的配置选项，而非在 Ingress 资源中指定。 如果 hosts 或 paths 都没有与 Ingress 对象中的 HTTP 请求匹配，则流量将路由到默认后端。 资源后端Resource 后端是一个 ObjectRef，指向同一命名空间中的另一个 Kubernetes，将其作为 Ingress 对象。Resource 与 Service 配置是互斥的，在二者均被设置时会无法通过合法性检查。Resource 后端的一种常见用法是将所有入站数据导向带有静态资产的对象存储后端。 路径类型Ingress 中的每个路径都需要有对应的路径类型（Path Type）。未明确设置 pathType 的路径无法通过合法性检查。当前支持的路径类型有三种： ImplementationSpecific：对于这种路径类型，匹配方法取决于 IngressClass。 具体实现可以将其作为单独的 pathType 处理或者与 Prefix 或 Exact 类型作相同处理。 Exact：精确匹配 URL 路径，且区分大小写。 Prefix：基于以 / 分隔的 URL 路径前缀匹配。匹配区分大小写，并且对路径中的元素逐个完成。 路径元素指的是由 / 分隔符分隔的路径中的标签列表。 如果每个 p 都是请求路径 p 的元素前缀，则请求与路径 p 匹配。 说明： 如果路径的最后一个元素是请求路径中最后一个元素的子字符串，则不会匹配 （例如：/foo/bar 匹配 /foo/bar/baz, 但不匹配 /foo/barbaz）。 示例 类型 路径 请求路径 匹配与否？ Prefix / （所有路径） 是 Exact /foo /foo 是 Exact /foo /bar 否 Exact /foo /foo/ 否 Exact /foo/ /foo 否 Prefix /foo /foo, /foo/ 是 Prefix /foo/ /foo, /foo/ 是 Prefix /aaa/bb /aaa/bbb 否 Prefix /aaa/bbb /aaa/bbb 是 Prefix /aaa/bbb/ /aaa/bbb 是，忽略尾部斜线 Prefix /aaa/bbb /aaa/bbb/ 是，匹配尾部斜线 Prefix /aaa/bbb /aaa/bbb/ccc 是，匹配子路径 Prefix /aaa/bbb /aaa/bbbxyz 否，字符串前缀不匹配 Prefix /, /aaa /aaa/ccc 是，匹配 /aaa 前缀 Prefix /, /aaa, /aaa/bbb /aaa/bbb 是，匹配 /aaa/bbb 前缀 Prefix /, /aaa, /aaa/bbb /ccc 是，匹配 / 前缀 Prefix /aaa /ccc 否，使用默认后端 混合 /foo (Prefix), /foo (Exact) /foo 是，优选 Exact 类型 最基本的 Ingress 资源注意：在此示例中，未指定 host，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。 如果提供了 host（例如 foo.bar.com），则 rules 适用于该 host。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - pathType: Prefix path: \"/nginx\" backend: service: name: nginx-service port: number: 80 defaultBackend: service: name: default-http-backend port: number: 80 基于 URL 路由代理服务配置根据请求的 HTTP URI 将来自同一 IP 地址的流量路由到多个 Service。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: \"www.ingress.com\" http: paths: - pathType: Prefix path: \"/nginx\" backend: service: name: nginx-service port: number: 80 - pathType: Prefix path: \"/tomcat\" backend: service: name: tomcat-service port: number: 80 defaultBackend: # 如果hosts或paths都没有与Ingress对象中的HTTP请求匹配,则流量将路由到默认后端 service: name: default-http-backend port: number: 80 基于名称的虚拟托管基于名称的虚拟主机支持将针对多个主机名的 HTTP 流量路由到同一 IP 地址上。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host spec: rules: - host: \"www.nginx.com\" http: paths: - pathType: Prefix path: \"/\" backend: service: name: nginx-service port: number: 80 - host: \"www.tomcat.com\" http: paths: - pathType: Prefix path: \"/\" backend: service: name: tomcat-service port: number: 80 defaultBackend: service: name: default-http-backend port: number: 80 TLS可以通过设定包含 TLS 私钥和证书的 Secret 来保护 Ingress。 Ingress 只支持单个 TLS 端口 443，并假定 TLS 连接终止于 Ingress 节点 （与 Service 及其 Pod 之间的流量都以明文传输）。 如果 Ingress 中的 TLS 配置部分指定了不同的主机，那么它们将根据通过 SNI TLS 扩展指定的主机名 （如果 Ingress 控制器支持 SNI）在同一端口上进行复用。 TLS Secret 必须包含名为 tls.crt 和 tls.key 的键名。 这些数据包含用于 TLS 的证书和私钥。例如： apiVersion: v1 kind: Secret metadata: name: testsecret-tls namespace: default data: tls.crt: base64 编码的 cert tls.key: base64 编码的 key type: kubernetes.io/tls 在 Ingress 中引用此 Secret 将会告诉 Ingress 控制器使用 TLS 加密从客户端到负载均衡器的通道。 你需要确保创建的 TLS Secret 创建自包含 https-example.foo.com 的公用名称（CN）的证书。 这里的公共名称也被称为全限定域名（FQDN）。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host spec: tls: - hosts: - www.missf.top secretName: www-missf-top rules: - host: www.missf.top http: paths: - pathType: Prefix path: \"/\" backend: service: name: nginx-service port: number: 80 defaultBackend: service: name: default-http-backend port: number: 80 生成 tls 自签证书CFSSL 是 CloudFlare 开源的一款 PKI/TLS 工具。 CFSSL 包含一个命令行工具和一个用于签名、验证并且捆绑 TLS 证书的 HTTP API 服务，使用 Go 语言编写。 安裝 CFSSL 工具 curl -s -L -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 curl -s -L -o /usr/local/bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 chmod +x /usr/local/bin/cfssl* 生成配置证书生成策略文件 cat > ca-config.json &lt;&lt;EOF { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ] } } } } EOF 这个策略，有一个默认的配置和一个 profile，这里的 profile 是 kubernetes。 signing：表示该证书可用于签名其它证书。 server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证。 client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证。 expiry：表示证书的有效期。 生成 CA 证书和私钥配置文件 cat > ca-csr.json &lt;&lt;EOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"GuangDong\", \"L\": \"ShenZhen\", \"O\": \"Kubernetes\", \"OU\": \"Devops\" } ] } EOF CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法 C：Country， 国家 ST：State，州，省 L：Locality，地区，城市 O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group) OU：Organization Unit Name，组织单位名称，公司部门 生成 CA 证书、CA 私钥、CSR 文件 cfssl gencert -initca ca-csr.json | cfssljson -bare ca - # 得到如下文件 [root@k8s-master ~/kubernetes/ssl]$ ll total 28 -rw-r--r-- 1 root root 294 Apr 17 15:46 ca-config.json -rw-r--r-- 1 root root 1013 Apr 17 15:48 ca.csr -rw-r--r-- 1 root root 274 Apr 17 15:47 ca-csr.json -rw------- 1 root root 1679 Apr 17 15:48 ca-key.pem -rw-r--r-- 1 root root 1387 Apr 17 15:48 ca.pem 生成服务端的证书信息 cat > www.missf.top-csr.json &lt;&lt;EOF { \"CN\": \"www.missf.top\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"GuangDong\", \"L\": \"ShenZhen\", \"O\": \"Kubernetes\", \"OU\": \"System\" } ] } EOF 使用 ca 证书签发证书 cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes www.missf.top-csr.json | cfssljson -bare www.missf.top 使用证书创建 secret 资源 kubectl create secret tls www-missf-top --cert=www.missf.top.pem --key=www.missf.top-key.pem 存储Kubernetes 的存储方式有很多，https://kubernetes.io/zh/docs/concepts/storage/ 卷Container 中的文件在磁盘上是临时存放的，这给 Container 中运行的较重要的应用程序带来一些问题。问题之一是当容器崩溃时文件丢失。kubelet 会重新启动容器， 但容器会以干净的状态重启。 第二个问题会在同一 Pod 中运行多个容器并共享文件时出现。 Kubernetes 卷（Volume）这一抽象概念能够解决这两个问题。 背景Docker 也有 卷（Volume） 的概念，但对它只有少量且松散的管理。 Docker 卷是磁盘上或者另外一个容器内的一个目录。 Docker 提供卷驱动程序，但是其功能非常有限。 Kubernetes 支持很多类型的卷。 Pod 可以同时使用任意数目的卷类型。 临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。 因此，卷的存在时间会超出 Pod 中运行的所有容器，并且在容器重新启动时数据也会得到保留。 当 Pod 不再存在时，临时卷也将不再存在。但是持久卷会继续存在。 卷的核心是包含一些数据的一个目录，Pod 中的容器可以访问该目录。 所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放的内容。 使用卷时, 在 .spec.volumes 字段中设置为 Pod 提供的卷，并在 .spec.containers[*].volumeMounts 字段中声明卷在容器中的挂载位置。 容器中的进程看到的是由它们的 Docker 镜像和卷组成的文件系统视图。 Docker 镜像 位于文件系统层次结构的根部。各个卷则挂载在镜像内的指定路径上。 卷不能挂载到其他卷之上，也不能与其他卷有硬链接。 Pod 配置中的每个容器必须独立指定各个卷的挂载位置。 卷类型Kubernetes 支持很多类型的卷，下面主要列举一些常用的卷： emptyDir当 Pod 分派到某个 Node 上时，emptyDir 卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。 就像其名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同，这些容器都可以读写 emptyDir 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。 说明： 容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 emptyDir 卷中的数据是安全的。 取决于你的环境，emptyDir 卷存储在该节点所使用的介质上；这里的介质可以是磁盘或 SSD 或网络存储。但是，你可以将 emptyDir.medium 字段设置为 &quot;Memory&quot;，以告诉 Kubernetes 为你挂载 tmpfs（基于 RAM 的文件系统）。 虽然 tmpfs 速度非常快，但是要注意它与磁盘不同。 tmpfs 在节点重启时会被清除，并且你所写入的所有文件都会计入容器的内存消耗，受容器内存限制约束。 apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-containers image: ikubernetes/myapp:v1 volumeMounts: # 容器的/usr/share/nginx/html目录挂载到emptyDir卷 - mountPath: /usr/share/nginx/html name: html - name: busybox image: busybox volumeMounts: # 容器的/data目录挂载到emptyDir卷 - mountPath: /data name: html command: [\"/bin/sh\", \"-c\", \"while true; do echo $(date) >> /data/index.html; sleep 2;done\"] volumes: # 在pod所在节点定义emptyDir卷 - name: html emptyDir: {} # 同一pod的两个不同的容器挂载到同一个emptyDir卷,实现数据共享和数据交互 hostPathhostPath 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中。 虽然这不是大多数 Pod 需要的，但是它为一些应用程序提供了强大的逃生舱。 除了必需的 path 属性之外，用户可以选择性地为 hostPath 卷指定 type。 支持的 type 值如下： 取值 行为 空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。 DirectoryOrCreate 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。 Directory 在给定路径上必须存在的目录。 FileOrCreate 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。 File 在给定路径上必须存在的文件。 Socket 在给定路径上必须存在的 UNIX 套接字。 CharDevice 在给定路径上必须存在的字符设备。 BlockDevice 在给定路径上必须存在的块设备。 当使用这种类型的卷时要小心，因为： 具有相同配置（例如基于同一 PodTemplate 创建）的多个 Pod 会由于节点上文件的不同而在不同节点上有不同的行为。 下层主机上创建的文件或目录只能由 root 用户写入。你需要在 特权容器 中以 root 身份运行进程，或者修改主机上的文件权限以便容器能够写入 hostPath 卷。 apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: nginx image: nginx volumeMounts: - mountPath: /usr/share/nginx/html # 容器里目录位置 name: html volumes: - name: html hostPath: path: /data # 宿主上目录位置 type: DirectoryOrCreate nfsnfs 卷能将 NFS (网络文件系统) 挂载到你的 Pod 中。 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，nfs 卷的内容在删除 Pod 时会被保存，卷只是被卸载。 这意味着 nfs 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。 注意： 在使用 NFS 卷之前，你必须运行自己的 NFS 服务器并将目标 share 导出备用。 部署 NFS 服务端： apt-get update apt-get install nfs-kernel-server -y mkdir /mnt/nfs/ vim /etc/exports /mnt/nfs 10.10.110.0/24(rw,no_root_squash,no_subtree_check) # 将NFS服务端的/mnt/nfs目录share出去 systemctl restart nfs-server.service 注意： NFS 服务端和客户端之间需要关闭防火墙，Kubernetes 节点必须支持驱动 NFS 存储设备。 客户端（Kubernetes 所有工作节点）： apt-get update apt-get install nfs-common -y # 客户端连接NFS服务器所需的包 mount -t nfs nfs:/mnt/nfs /mnt/nfs # 将nfs节点的/mnt/nfs目录挂载到本地节点的/mnt/nfs目录,创建文件测试两个节点的目录是否共享 创建 NFS 存储类的资源配置清单： apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: nginx image: nginx volumeMounts: - mountPath: /usr/share/nginx/html name: html volumes: - name: html nfs: path: /mnt/nfs server: www.nfs.com 注意：我们的 Kubernetes 工作节点不需要提前挂载 NFS 卷，创建 pod 时会自动挂载，pod 销毁之后 NFS 卷也会自动卸载。 运行 pod 之后，我们可以在 pod 所在节点查看 NFS 挂载情况： df -h | grep nfs nfs:/mnt/nfs 19G 4.5G 14G 26% /var/lib/kubelet/pods/0a87a303-9e44-4388-9e89-4bf848d159cf/volumes/kubernetes.io~nfs/html # nfs:/mnt/nfs 这是pod所在节点挂载NFS服务的目录 # /var/lib/kubelet/pods/0a87a303-9e44-4388-9e89-4bf848d159cf/volumes/kubernetes.io~nfs/html 是pod内/usr/share/nginx/html目录在宿主机上的映射 这时候我们在 NFS 服务器修改 NFS 共享目录 /mnt/nfs ，数据会同步到 pod 内的 /usr/share/nginx/html 目录。 persistentVolumeClaimpersistentVolumeClaim 卷用来将持久卷（PersistentVolume） 挂载到 Pod 中。 持久卷申领（PersistentVolumeClaim）是用户在不知道特定云环境细节的情况下”申领”持久存储 （例如 GCE PersistentDisk 或者 iSCSI 卷）的一种方法。 持久卷（PersistentVolume，PV）是集群中的一块存储，可以由管理员事先供应，或者使用存储类（Storage Class）来动态供应。 持久卷是集群资源，就像节点也是集群资源一样。PV 持久卷和普通的 Volume 一样，也是使用卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。 此 API 对象中记述了存储的实现细节，无论其背后是 NFS、iSCSI 还是特定于云平台的存储系统。 持久卷申领（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。概念上与 Pod 类似。 Pod 会耗用节点资源，而 PVC 申领会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU 和内存）；同样 PVC 申领也可以请求特定的大小和访问模式 （例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载） 每个 PV 对象都包含 spec 部分和 status 部分，分别对应卷的规约和状态： --- apiVersion: v1 kind: PersistentVolume metadata: name: pv1 spec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /mnt/pv1 server: www.nfs.com --- apiVersion: v1 kind: PersistentVolume metadata: name: pv2 spec: capacity: storage: 2Gi volumeMode: Filesystem accessModes: - ReadOnlyMany - ReadWriteOnce persistentVolumeReclaimPolicy: Retain nfs: path: /mnt/pv2 server: www.nfs.com --- apiVersion: v1 kind: PersistentVolume metadata: name: pv3 spec: capacity: storage: 3Gi volumeMode: Filesystem accessModes: - ReadWriteMany - ReadWriteOnce - ReadOnlyMany persistentVolumeReclaimPolicy: Retain nfs: path: /mnt/pv3 server: www.nfs.com 一般而言，每个 PV 卷都有确定的存储容量。 容量属性是使用 PV 对象的 capacity 属性来设置的。 目前，存储大小是可以设置和请求的唯一资源。 未来可能会包含 IOPS、吞吐量等属性。 针对 PV 持久卷，Kuberneretes 支持两种卷模式（volumeModes）：Filesystem（文件系统） 和 Block（块）。 volumeMode 是一个可选的 API 参数。 如果该参数被省略，默认的卷模式是 Filesystem。 volumeMode 属性设置为 Filesystem 的卷会被 Pod 挂载（Mount）到某个目录。 如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前在设备上创建文件系统。 你可以将 volumeMode 设置为 Block，以便将卷作为原始块设备来使用。 这类卷以块设备的方式交给 Pod 使用，其上没有任何文件系统。 这种模式对于为 Pod 提供一种使用最快可能方式来访问卷而言很有帮助，Pod 和卷之间不存在文件系统层。另外，Pod 中运行的应用必须知道如何处理原始块设备。 关于如何在 Pod 中使用 volumeMode: Block 的卷，可参阅 原始块卷支持。 PersistentVolume 卷可以用资源提供者所支持的任何方式挂载到宿主系统上。 如下表所示，提供者（驱动）的能力不同，每个 PV 卷的访问模式都会设置为对应卷所支持的模式值。 例如，NFS 可以支持多个读写客户，但是某个特定的 NFS PV 卷可能在服务器上以只读的方式导出。每个 PV 卷都会获得自身的访问模式集合，描述的是特定 PV 卷的能力。 访问模式有： ReadWriteOnce – 卷可以被一个节点以读写方式挂载； ReadOnlyMany – 卷可以被多个节点以只读方式挂载； ReadWriteMany – 卷可以被多个节点以读写方式挂载； 在命令行接口（CLI）中，访问模式也使用以下缩写形式： RWO - ReadWriteOnce ROX - ReadOnlyMany RWX - ReadWriteMany 目前的回收策略有： Retain – 手动回收（pvc 被删除后，pv 还保留着数据，只是 pv 的状态变为 Released ，并且 pv 不能再次被 pvc 绑定） Recycle – 基本擦除 (pvc 被删除后，pv 不保留数据，pv 可以再次被 pvc 绑定， rm -rf /thevolume/*) Delete – 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除 目前，仅 NFS 和 HostPath 支持回收（Recycle）。 AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。 每个 PVC 对象都有 spec 和 status 部分，分别对应申领的规约和状态： --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: default spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi 申领在请求具有特定访问模式的存储时，使用与卷相同的访问模式约定。 申领使用与卷相同的约定来表明是将卷作为文件系统还是块设备来使用。 申领和 Pod 一样，也可以请求特定数量的资源。在这个上下文中，请求的资源是存储。 卷和申领都使用相同的 资源模型。 申领可以设置标签选择算符来进一步过滤卷集合。只有标签与选择算符相匹配的卷能够绑定到申领上。 选择算符包含两个字段： matchLabels - 卷必须包含带有此值的标签 matchExpressions - 通过设定键（key）、值列表和操作符（operator） 来构造的需求。合法的操作符有 In、NotIn、Exists 和 DoesNotExist。 定义 PersistentVolume 的标签： kubectl get persistentvolume --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS pv1 1Gi RWO,RWX Retain Available default/pvc1 43m release=stable pv2 2Gi RWO,ROX Retain Available 43m pv3 3Gi RWO,ROX,RWX Retain Available 43m PersistentVolumeClaim 通过 PersistentVolume 的标签去进行绑定： --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: default spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi selector: matchLabels: release: \"stable\" matchExpressions: - {key: release, operator: In, values: [stable]} 存储类StorageClass 为管理员提供了描述存储 “类” 的方法。 不同的类型可能会映射到不同的服务质量等级或备份策略，或是由集群管理员制定的任意策略。 Kubernetes 本身并不清楚各种类代表的什么。这个类的概念在其他存储系统中有时被称为 “配置文件”。 StorageClass 资源每个 StorageClass 都包含 provisioner、parameters 和 reclaimPolicy 字段， 这些字段会在 StorageClass 需要动态分配 PersistentVolume 时会使用到。 StorageClass 对象的命名很重要，用户使用这个命名来请求生成一个特定的类。 当创建 StorageClass 对象时，管理员设置 StorageClass 对象的命名和其他参数，一旦创建了对象就不能再对其更新。 存储制备器每个 StorageClass 都有一个制备器（Provisioner），用来决定使用哪个卷插件制备 PV。 该字段必须指定。 卷插件 内置制备器 配置例子 AWSElasticBlockStore ✓ AWS EBS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS - - Cinder ✓ OpenStack Cinder FC - - FlexVolume - - Flocker ✓ - GCEPersistentDisk ✓ GCE PD Glusterfs ✓ Glusterfs iSCSI - - Quobyte ✓ Quobyte NFS - - RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local - Local 动态卷供应动态卷供应允许按需创建存储卷。 如果没有动态供应，集群管理员必须手动地联系他们的云或存储提供商来创建新的存储卷， 然后在 Kubernetes 集群创建PersistentVolume 对象来表示这些卷。 动态供应功能消除了集群管理员预先配置存储的需要。 相反，它在用户请求时自动供应存储。 由于 NFS 卷插件并不支持内置制备器，所以我们用 NFS 作为底层存储去配置动态卷供应时，需要使用第三方的 NFS 插件 nfs-subdir-external-provisioner。 部署 NFS 插件： apiVersion: v1 kind: Namespace metadata: name: nfs-subdir-external-provisioner labels: name: nfs-subdir-external-provisioner --- apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: k8s-sigs.io/nfs-subdir-external-provisioner - name: NFS_SERVER value: 10.10.110.193 # NFS的服务地址 - name: NFS_PATH value: /data/kubernetes # NFS的Export路径 volumes: - name: nfs-client-root nfs: server: 10.10.110.193 path: /data/kubernetes --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # \"false\": 删除pvc之后NFS存储后端不会保留数据目录,\"true\": 删除pvc之后NFS存储后端会保留数据目录 --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 创建 PersistentVolumeClaim 测试动态卷供应： # 指定storageClassName --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi storageClassName: \"managed-nfs-storage\" # 使用annotations --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 annotations: volume.beta.kubernetes.io/storage-class: \"managed-nfs-storage\" spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi # 指定sc作为默认存储后端 kubectl patch storageclass managed-nfs-storage -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi 说明：关于 kubernetes 1.20 版本使用 NFS 插件出现 unexpected error getting claim reference: selfLink was empty, can’t make reference 的报错。这是因为 kubernetes 1.20 版本禁用了 selfLink 。解决方法： vim /etc/kubernetes/manifests/kube-apiserver.yaml ... spec: containers: - command: - kube-apiserver - --feature-gates=RemoveSelfLink=false # 添加这一行 ... 配置此模块 整合了 Kubernetes 所有资源的配置。 配置最佳实践https://kubernetes.io/zh/docs/concepts/configuration/overview/ ConfigMapConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pods 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。 ConfigMap 将您的环境配置信息和 容器镜像 解耦，便于应用配置的修改。 注意：ConfigMap 并不提供保密或者加密功能。 如果你想存储的数据是机密的，请使用 Secret， 或者使用其他第三方工具来保证你的数据的私密性，而不是用 ConfigMap。 动机使用 ConfigMap 来将你的配置数据和应用程序代码分开。 比如，假设你正在开发一个应用，它可以在你自己的电脑上（用于开发）和在云上 （用于实际流量）运行。 你的代码里有一段是用于查看环境变量 DATABASE_HOST，在本地运行时， 你将这个变量设置为 localhost，在云上，你将其设置为引用 Kubernetes 集群中的公开数据库组件的 服务。 这让你可以获取在云中运行的容器镜像，并且如果有需要的话，在本地调试完全相同的代码。 ConfigMap 在设计上不是用来保存大量数据的。在 ConfigMap 中保存的数据不可超过 1 MiB。如果你需要保存超出此尺寸限制的数据，你可能希望考虑挂载存储卷或者使用独立的数据库或者文件服务。 ConfigMap 对象ConfigMap 是一个 API 对象， 让你可以存储其他对象所需要使用的配置。 和其他 Kubernetes 对象都有一个 spec ，不同的是，ConfigMap 使用 data 和 binaryData 字段。这些字段能够接收键-值对作为其取值。data 和 binaryData 字段都是可选的。data 字段设计用来保存 UTF-8 字节序列，而 binaryData 则被设计用来保存二进制数据作为 base64 编码的字串。 ConfigMap 的名字必须是一个合法的 DNS 子域名。 data 或 binaryData 字段下面的每个键的名称都必须由字母数字字符或者 -、_ 或 . 组成。在 data 下保存的键名不可以与在 binaryData 下出现的键名有重叠。 从 v1.19 开始，你可以添加一个 immutable 字段到 ConfigMap 定义中，创建 不可变更的 ConfigMap。 ConfigMaps 和 Pods你可以写一个引用 ConfigMap 的 Pod 的 spec，并根据 ConfigMap 中的数据在该 Pod 中配置容器。这个 Pod 和 ConfigMap 必须要在同一个 名字空间 中。 这是一个 ConfigMap 的示例，它的一些键只有一个值，其他键的值看起来像是配置的片段格式。 apiVersion: v1 kind: ConfigMap metadata: name: configmap-demo data: # 类属性键:每一个键都映射到一个简单的值 nginx_server_port: \"80\" nginx_server_host: \"www.missf.top\" # 类文件键 nginx.config: | location / { root /usr/local/nginx/html/; index index.html; } 你可以使用四种方式来使用 ConfigMap 配置 Pod 中的容器： 在容器命令和参数内 容器的环境变量 在只读卷里面添加一个文件，让应用来读取 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap 这些不同的方法适用于不同的数据使用方式。 对前三个方法，kubelet 使用 ConfigMap 中的数据在 Pod 中启动容器。 第四种方法意味着你必须编写代码才能读取 ConfigMap 和它的数据。然而， 由于你是直接使用 Kubernetes API，因此只要 ConfigMap 发生更改，你的应用就能够通过订阅来获取更新，并且在这样的情况发生的时候做出反应。 通过直接进入 Kubernetes API，这个技术也可以让你能够获取到不同的名字空间里的 ConfigMap。 下面是一个 Pod 的示例，它通过使用 configmap-demo 中的值来配置一个 Pod： apiVersion: v1 kind: Pod metadata: name: configmap-pod labels: app: configmap spec: containers: - name: nginx image: nginx ports: - containerPort: 80 env: - name: NGINX_SERVER_PORT valueFrom: configMapKeyRef: name: configmap-demo key: nginx_server_port - name: NGINX_SERVER_HOST valueFrom: configMapKeyRef: name: configmap-demo key: nginx_server_host volumeMounts: - name: config mountPath: \"/config\" readOnly: true volumes: - name: config configMap: name: configmap-demo items: - key: \"nginx.config\" path: \"nginx.config\" ConfigMap 不会区分单行属性值和多行类似文件的值，重要的是 Pods 和其他对象如何使用这些值。（能否自动更新也是看 Pods 如何去使用 ConfigMap） 上面的例子定义了一个卷并将它作为 /config 文件夹挂载到 demo 容器内， 创建一个文件，/config/nginx.config 使用 ConfigMapConfigMap 可以作为数据卷挂载。ConfigMap 也可被系统的其他组件使用，而不一定直接暴露给 Pod。例如，ConfigMap 可以保存系统中其他组件要使用的配置数据。 ConfigMap 最常见的用法是为同一命名空间里某 Pod 中运行的容器执行配置。 你也可以单独使用 ConfigMap。 比如，你可能会遇到基于 ConfigMap 来调整其行为的 插件 或者 operator。 在 Pod 中将 ConfigMap 当做文件使用 创建一个 ConfigMap 对象或者使用现有的 ConfigMap 对象。多个 Pod 可以引用同一个 ConfigMap。 修改 Pod 定义，在 spec.volumes[] 下添加一个卷。 为该卷设置任意名称，之后将 spec.volumes[].configMap.name 字段设置为对你的 ConfigMap 对象的引用。 为每个需要该 ConfigMap 的容器添加一个 .spec.containers[].volumeMounts[]。 设置 .spec.containers[].volumeMounts[].readOnly=true 并将 .spec.containers[].volumeMounts[].mountPath 设置为一个未使用的目录名， ConfigMap 的内容将出现在该目录中。 更改你的镜像或者命令行，以便程序能够从该目录中查找文件。ConfigMap 中的每个 data 键会变成 mountPath 下面的一个文件名。 下面是一个将 ConfigMap 以卷的形式进行挂载的 Pod 示例： apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo configMap: name: myconfigmap 你希望使用的每个 ConfigMap 都需要在 spec.volumes 中被引用到。 如果 Pod 中有多个容器，则每个容器都需要自己的 volumeMounts 块，但针对每个 ConfigMap，你只需要设置一个 spec.volumes 块。 被挂载的 ConfigMap 内容会被自动更新当卷中使用的 ConfigMap 被更新时，所投射的键最终也会被更新。 kubelet 组件会在每次周期性同步时检查所挂载的 ConfigMap 是否为最新。 不过，kubelet 使用的是其本地的高速缓存来获得 ConfigMap 的当前值。 高速缓存的类型可以通过 KubeletConfiguration 结构 的 ConfigMapAndSecretChangeDetectionStrategy 字段来配置。 ConfigMap 既可以通过 watch 操作实现内容传播（默认形式），也可实现基于 TTL 的缓存，还可以直接经过所有请求重定向到 API 服务器。 因此，从 ConfigMap 被更新的那一刻算起，到新的主键被投射到 Pod 中去，这一 时间跨度可能与 kubelet 的同步周期加上高速缓存的传播延迟相等。 这里的传播延迟取决于所选的高速缓存类型 （分别对应 watch 操作的传播延迟、高速缓存的 TTL 时长或者 0）。 以环境变量方式使用的 ConfigMap 数据不会被自动更新。 更新这些数据需要重新启动 Pod。 不可变更的 ConfigMapKubernetes 不可变更的 Secret 和 ConfigMap 提供了一种将各个 Secret 和 ConfigMap 设置为不可变更的选项。对于大量使用 ConfigMap 的集群（至少有数万个各不相同的 ConfigMap 给 Pod 挂载）而言，禁止更改 ConfigMap 的数据有以下好处： 保护应用，使之免受意外（不想要的）更新所带来的负面影响。 通过大幅降低对 kube-apiserver 的压力提升集群性能，这是因为系统会关闭对已标记为不可变更的 ConfigMap 的监视操作。 此功能特性由 ImmutableEphemeralVolumes 特性门控 来控制。你可以通过将 immutable 字段设置为 true 创建不可变更的 ConfigMap。 例如： apiVersion: v1 kind: ConfigMap metadata: ... data: ... immutable: true 一旦某 ConfigMap 被标记为不可变更，则无法逆转这一变化，也无法更改 data 或 binaryData 字段的内容。你只能删除并重建 ConfigMap。 因为现有的 Pod 会维护一个对已删除的 ConfigMap 的挂载点，建议重新创建这些 Pods。 SecretSecret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像 中来说更加安全和灵活。 参阅 Secret 设计文档 获取更多详细信息。 Secret 是一种包含少量敏感信息例如密码、令牌或密钥的对象。 这样的信息可能会被放在 Pod 规约中或者镜像中。 用户可以创建 Secret，同时系统也创建了一些 Secret。 注意：Kubernetes Secret 默认情况下存储为 base64-编码的、非加密的字符串。 默认情况下，能够访问 API 的任何人，或者能够访问 Kubernetes 下层数据存储（etcd） 的任何人都可以以明文形式读取这些数据。 为了能够安全地使用 Secret，我们建议你（至少）： 为 Secret 启用静态加密； 启用或配置 RBAC 规则来限制对 Secret 的读写操作。 要注意，任何被允许创建 Pod 的人都默认地具有读取 Secret 的权限。 Secret 概览要使用 Secret，Pod 需要引用 Secret。 Pod 可以用三种方式之一来使用 Secret： 作为挂载到一个或多个容器上的 卷 中的文件。 作为容器的环境变量 由 kubelet 在为 Pod 拉取镜像时使用 Secret 对象的名称必须是合法的 DNS 子域名。 在为创建 Secret 编写配置文件时，你可以设置 data 与/或 stringData 字段。 data 和 stringData 字段都是可选的。data 字段中所有键值都必须是 base64 编码的字符串。如果不希望执行这种 base64 字符串的转换操作，你可以选择设置 stringData 字段，其中可以使用任何字符串作为其取值。 Secret 的类型在创建 Secret 对象时，你可以使用 Secret 资源的 type 字段，或者与其等价的 kubectl 命令行参数（如果有的话）为其设置类型。 Secret 的类型用来帮助编写程序处理 Secret 数据。 Kubernetes 提供若干种内置的类型，用于一些常见的使用场景。 针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。 内置类型 用法 Opaque 用户定义的任意数据 kubernetes.io/service-account-token 服务账号令牌 kubernetes.io/dockercfg ~/.dockercfg 文件的序列化形式 kubernetes.io/dockerconfigjson ~/.docker/config.json 文件的序列化形式 kubernetes.io/basic-auth 用于基本身份认证的凭据 kubernetes.io/ssh-auth 用于 SSH 身份认证的凭据 kubernetes.io/tls 用于 TLS 客户端或者服务器端的数据 bootstrap.kubernetes.io/token 启动引导令牌数据 通过为 Secret 对象的 type 字段设置一个非空的字符串值，你也可以定义并使用自己 Secret 类型。如果 type 值为空字符串，则被视为 Opaque 类型。 Kubernetes 并不对类型的名称作任何限制。不过，如果你要使用内置类型之一， 则你必须满足为该类型所定义的所有要求。 创建 Secret有几种不同的方式来创建 Secret： 使用 kubectl 命令创建 Secret 一个 Secret 可以包含 Pod 访问数据库所需的用户凭证。 例如，由用户名和密码组成的数据库连接字符串。 你可以在本地计算机上，将用户名存储在文件 ./username.txt 中，将密码存储在文件 ./password.txt 中。 echo -n 'admin' > ./username.txt echo -n 'Er34ff5ghoo' > ./password.txt 在这些命令中，-n 标志确保生成的文件在文本末尾不包含额外的换行符。 这一点很重要，因为当 kubectl 读取文件并将内容编码为 base64 字符串时，多余的换行符也会被编码。 kubectl create secret 命令将这些文件打包成一个 Secret 并在 API 服务器上创建对象。 kubectl create secret generic db-user-pass --from-file=user=username.txt --from-file=pass=password.txt 输出类似于： secret/db-user-pass created 默认密钥名称是文件名。 你可以选择使用 --from-file=[key=]source 来设置密钥名称。例如： kubectl create secret generic db-user-pass \\ --from-file=username=./username.txt \\ --from-file=password=./password.txt 检查 secret 是否已创建： kubectl get secrets 你可以查看 Secret 的描述： kubectl describe secrets/db-user-pass 要查看创建的 Secret 的内容，运行以下命令： kubectl get secret db-user-pass -o jsonpath='{.data}' 输出类似于： {\"pass\":\"RXIzNGZmNWdob28=\",\"user\":\"YWRtaW4=\"} 现在你可以解码 pass 的数据： echo \"RXIzNGZmNWdob28=\" | base64 --decode 输出类似于： Er34ff5ghoo 使用配置文件来创建 Secret 你可以先用 JSON 或 YAML 格式在文件中创建 Secret，然后创建该对象。 Secret 资源包含2个键值对： data 和 stringData。 data 字段用来存储 base64 编码的任意数据。 提供 stringData 字段是为了方便，它允许 Secret 使用未编码的字符串。 data 和 stringData 的键必须由字母、数字、-，_ 或 . 组成。 例如，要使用 Secret 的 data 字段存储两个字符串，请将字符串转换为 base64 ，如下所示： echo -n 'admin' | base64 echo -n 'Er34ff5ghoo' | base64 输出类似于： YWRtaW4= RXIzNGZmNWdob28= 编写一个 Secret 配置文件，如下所示： apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: RXIzNGZmNWdob28= 对于某些场景，你可能希望使用 stringData 字段。 这字段可以将一个非 base64 编码的字符串直接放入 Secret 中， 当创建或更新该 Secret 时，此字段将被编码。 例如，如果你的应用程序使用以下配置文件: apiUrl: \"https://my.api.com/api/v1\" username: \"&lt;user>\" password: \"&lt;password>\" 你可以使用以下定义将其存储在 Secret 中: apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque stringData: config.yaml: | apiUrl: \"https://my.api.com/api/v1\" username: &lt;user> password: &lt;password> 使用 kustomize 来创建 Secret 你可以在 kustomization.yaml 中定义 secreteGenerator，并在定义中引用其他现成的文件，生成 Secret。 例如：下面的 kustomization 文件 引用了 ./username.txt 和 ./password.txt 文件： secretGenerator: - name: db-user-pass files: - username.txt - password.txt 你也可以在 kustomization.yaml 文件中指定一些字面量定义 secretGenerator。 例如：下面的 kustomization.yaml 文件中包含了 username 和 password 两个字面量： secretGenerator: - name: db-user-pass literals: - username=admin - password=1f2d1e2e67df 注意，上面两种情况，你都不需要使用 base64 编码。 使用 kubectl apply 命令应用包含 kustomization.yaml 文件的目录创建 Secret。 kubectl apply -k . 编辑 Secret你可以通过下面的命令编辑现有的 Secret： kubectl edit secrets mysecret 使用 SecretSecret 可以作为数据卷被挂载，或作为 环境变量 暴露出来以供 Pod 中的容器使用。它们也可以被系统的其他部分使用，而不直接暴露在 Pod 内。 例如，它们可以保存凭据，系统的其他部分将用它来代表你与外部系统进行交互。 将 Secret 作为 Pod 中的环境变量使用： 创建一个 Secret 或者使用一个已存在的 Secret。多个 Pod 可以引用同一个 Secret。 修改 Pod 定义，为每个要使用 Secret 的容器添加对应 Secret 键的环境变量。 使用 Secret 键的环境变量应在 env[x].valueFrom.secretKeyRef 中指定要包含的 Secret 名称和键名。 更改镜像并／或者命令行，以便程序在指定的环境变量中查找值。 这是一个使用来自环境变量中的 Secret 值的 Pod 示例： apiVersion: v1 kind: Pod metadata: name: secret-env-pod spec: containers: - name: nginx image: nginx env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Never 在 Pod 中使用存放在卷中的 Secret： 创建一个 Secret 或者使用已有的 Secret。多个 Pod 可以引用同一个 Secret。 修改你的 Pod 定义，在 spec.volumes[] 下增加一个卷。可以给这个卷随意命名， 它的 spec.volumes[].secret.secretName 必须是 Secret 对象的名字。 将 spec.containers[].volumeMounts[] 加到需要用到该 Secret 的容器中。 指定 spec.containers[].volumeMounts[].readOnly = true 和 spec.containers[].volumeMounts[].mountPath 为你想要该 Secret 出现的尚未使用的目录。 修改你的镜像并且／或者命令行，让程序从该目录下寻找文件。 Secret 的 data 映射中的每一个键都对应 mountPath 下的一个文件名。 这是一个在 Pod 中使用存放在挂载卷中 Secret 的例子： apiVersion: v1 kind: Pod metadata: name: secret-volume-pod spec: containers: - name: nginx image: nginx volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: mysecret 挂载的 Secret 会被自动更新当已经存储于卷中被使用的 Secret 被更新时，被映射的键也将被更新。 组件 kubelet 在周期性同步时检查被挂载的 Secret 是不是最新的。 但是，它会使用其本地缓存的数值作为 Secret 的当前值。 StatefulSetStatefulSet 是用来管理有状态应用的工作负载 API 对象。 StatefulSet 用来管理某 Pod 集合的部署和扩缩， 并为这些 Pod 提供持久存储和持久标识符。 和 Deployment 类似， StatefulSet 管理基于相同容器规约的一组 Pod。但和 Deployment 不同的是， StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的， 但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。 StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值： 稳定的、唯一的网络标识符。 稳定的、持久的存储。 有序的、优雅的部署和缩放。 有序的、自动的滚动更新。 在上面描述中，“稳定的”意味着 Pod 调度或重调度的整个过程是有持久性的。 如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如 Deployment 或者 ReplicaSet 可能更适用于你的无状态应用部署需要。 创建 StatefulSet作为开始，使用如下示例创建一个 StatefulSet。它和 StatefulSets 概念中的示例相似。 它创建了一个 Headless Service statefulset-service 用来发布 StatefulSet web 中的 Pod 的 IP 地址。 apiVersion: v1 kind: Service metadata: name: statefulset-service namespace: default labels: app: statefulset spec: type: ClusterIP clusterIP: None ports: - port: 80 protocol: TCP targetPort: 80 selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"statefulset-service\" replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: storageClassName: \"managed-nfs-storage\" accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 2Gi 顺序创建 Pod对于一个拥有 N 个副本的 StatefulSet，Pod 被部署时是按照 {0 …… N-1} 的序号顺序创建的。 在第一个终端中使用 kubectl get 检查输出。这个输出最终将看起来像下面的样子。 kubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 0/1 ContainerCreating 0 2s web-0 1/1 Running 0 18s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 1s web-1 1/1 Running 0 17s web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 0/1 ContainerCreating 0 1s web-2 1/1 Running 0 17s 请注意在 web-0 Pod 处于 Running和Ready 状态后 web-1 Pod 才会被启动。 StatefulSet 中的 PodStatefulSet 中的 Pod 拥有一个唯一的顺序索引和稳定的网络身份标识。 检查 Pod 的顺序索引获取 StatefulSet 的 Pod。 kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 5m11s web-1 1/1 Running 0 4m53s web-2 1/1 Running 0 4m36s 如同 StatefulSets 概念中所提到的， StatefulSet 中的 Pod 拥有一个具有黏性的、独一无二的身份标志。 这个标志基于 StatefulSet 控制器分配给每个 Pod 的唯一顺序索引。 Pod 的名称的形式为&lt;statefulset name&gt;-&lt;ordinal index&gt;。 webStatefulSet 拥有两个副本，所以它创建了三个 Pod：web-0，web-1 和 web-2。 使用稳定的网络身份标识每个 Pod 都拥有一个基于其顺序索引的稳定的主机名。使用kubectl exec在每个 Pod 中执行hostname。 for i in 0 1; do kubectl exec \"web-$i\" -- sh -c 'hostname'; done web-0 web-1 web-2 使用 kubectl run 运行一个提供 nslookup 命令的容器，该命令来自于 dnsutils 包。 通过对 Pod 的主机名执行 nslookup，你可以检查他们在集群内部的 DNS 地址。 kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm 这将启动一个新的 shell。在新 shell 中，运行： # Run this in the dns-test container shell nslookup statefulset-service 输出类似于： Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: statefulset-service Address 1: 10.244.169.165 web-0.statefulset-service.default.svc.cluster.local Address 2: 10.244.36.98 web-1.statefulset-service.default.svc.cluster.local Address 3: 10.244.169.166 web-2.statefulset-service.default.svc.cluster.local 即使 Pod 重建之后 IP 发生改变，Headless Service 还是能够根据 web-{0-1}.statefulset-service.default.svc.cluster.local 这个 DNS A 记录来找到每个 pod 的 IP 地址。 标准 Service 和 Headless Service 的区别这里要提到 无状态应用控制器 和 有状态应用控制器 的设计理念，无状态的 Pod 是完全相等的，提供相同的服务，可以飘移在任意节点，例如三个 NGINX Pod 所提供的 Web 服务。而像一些分布式应用程序，例如 zookeeper 集群、etcd 集群、mysql 主从等服务，每个实例都会维护着一种状态，每个实例都有自己的数据，并且每个实例之间必须有固定的访问地址（组建集群），这就是有状态应用。由于标准 Service 是通过访问 ClusterIP 负载均衡到一组 Pod 上，这是没有办法指定访问到某个 Pod 的（由 iptables 决定）。所以这里就出现了 Headless Service ，而且 Headless Service 不需要 ClusterIP ，它是通过访问 Pod DNS 名称解析到对应的 Pod IP，为每一个 Pod 都固定一个 DNS 名称，即使 Pod 的 IP 发生改变，Pod 的 DNS 名称还是指向对应的 Pod IP 地址。 写入稳定的存储Kubernetes 为每个 VolumeClaimTemplate 创建一个 PersistentVolume。 在上面的 nginx 示例中，每个 Pod 将会得到基于 StorageClass managed-nfs-storage 提供的 2 Gib 的 PersistentVolume。如果没有声明 StorageClass，就会使用默认的 StorageClass。 当一个 Pod 被调度（重新调度）到节点上时，它的 volumeMounts 会挂载与其 PersistentVolumeClaims 相关联的 PersistentVolume。 请注意，当 Pod 或者 StatefulSet 被删除时，与 PersistentVolumeClaims 相关联的 PersistentVolume 并不会被删除。要删除它必须通过手动方式来完成。 获取 StatefulSet 创建的 PersistentVolumeClaims。 kubectl get pvc -l app=nginx 输出类似于： NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-83489225-7d08-4506-9f75-1fefd3aee287 2Gi RWO managed-nfs-storage 23h www-web-1 Bound pvc-e5afdbfa-952e-4120-8885-00cf5a524eb0 2Gi RWO managed-nfs-storage 23h www-web-2 Bound pvc-21e9a5b1-a040-41c6-9b94-6bcf7a8a8966 2Gi RWO managed-nfs-storage 23h StatefulSet 控制器创建了三个 PersistentVolumeClaims，绑定到三个 PersistentVolumes。由于本教程使用的集群配置为动态提供 PersistentVolume，所有的 PersistentVolume 都是自动创建和绑定的，对于动态配置的 PersistentVolumes 来说，默认回收策略为 “Delete”。 NGINX web 服务器默认会加载位于 /usr/share/nginx/html/index.html 的 index 文件。 StatefulSets spec 中的 volumeMounts 字段保证了 /usr/share/nginx/html 文件夹由一个 PersistentVolume 支持。 将 Pod 的主机名写入它们的index.html文件并验证 NGINX web 服务器使用该主机名提供服务。 for i in 0 1 2; do kubectl exec \"web-$i\" -- sh -c 'echo $(hostname) > /usr/share/nginx/html/index.html'; done for i in 0 1 2; do kubectl exec -i -t \"web-$i\" -- curl http://localhost/; done 在另一个终端删除 StatefulSet 所有的 Pod。 kubectl delete pod -l app=nginx 验证所有 web 服务器在继续使用它们的主机名提供服务。 for i in 0 1 2; do kubectl exec -i -t \"web-$i\" -- curl http://localhost/; done 虽然 Podweb-{0-2} 被重新调度了，但它们仍然继续监听各自的主机名，因为和它们的 PersistentVolumeClaim 相关联的 PersistentVolume 被重新挂载到了各自的 volumeMount 上。 不管 Pod 被调度到了哪个节点上，它们的 PersistentVolumes 将会被挂载到合适的挂载点上。 认证、授权与准入控制在任何将资源或服务提供给有限使用者的系统上，认证和授权是两个必不可少的功能，前者用于身份鉴别，负责验证“来者是谁”，而后者则实现权限分派，负责验证“他有权做什么事”。Kubernetes 系统完全分离了身份验证和授权功能，将二者分别以多种不同的插件实现，而且还有特有的准入控制机制，能在“写”请求上辅助完成更为精细的操作验证及变异功能。 Kubernetes 访问控制API Server 作为 Kubernetes 集群系统的网关，是访问及管理资源对象的唯一入口，它默认监听 TCP 的 6443 端口，通过 HTTPS 协议暴露了一个 RESTful 风格的接口。所有需要访问集群资源的集群组件或客户端，包括 kube-controller-manager、kube-scheduler、kubelet 和 kube-proxy 等集群基础组件，CoreDNS 等集群附加组件，以及 kubectl 命令等都必须要经过网关请求与集群通信。所有客户端均要经由 API Server 访问或改变集群状态以及完成数据存储，并且 API Server 会对每一次的访问请求进行合法检验，包括用户身份鉴别，操作权限验证以及操作是否符合全局规范的约束等。所有检查均正常完成且对象配置信息合法性检验无误后才能访问或存入数据到后端存储系统 ETCD 中。 客户端认证操作由 API Server 配置的一到多个认证插件完成。收到请求后，API Server 依次调用配置的认证插件来校验客户端的身份，直到其中一个插件可以识别出请求者的身份为止。授权操作则由一到多个授权插件完成，这些插件负责确定通过认证的用户是否有权限执行发出的资源请求，该类操作包括创建、读取、删除或修改指定的对象等。随后通过授权检测的用户请求修改相关的操作还要经由一到多个准入控制插件的遍历式检测，例如使用默认值补足要创建的目标资源对象中未定义的各个字段、检查目标 Namespace 资源对象是否存在、检查请求创建的 Pod 对象是否违反系统资源限制等，其中的任何检查失败都可能导致写入操作失败。 用户账号与用户组Kubernetes 系统上的用户账号及用户组的实现机制与常规应用略有不同。Kubernetes 集群将那些通过命令行工具 kubectl 、客户端库或者直接使用 RESTful 接口向 API Server 发起请求的客户端上的请求主体分为两个不同的类别：现实中的“人”和 Pod 对象，它们的用户身份分别对应用户账号（User Account，也称普通用户）和服务账号（Service Account，简称 SA）。 用户账户：其使用主体往往是“人”，一般由外部的用户管理系统存储和管理，Kubernetes 本身不维护这一类的任何用户账户信息，他们不会存储到 API Server 之上，仅仅用于检验用户是否有权限执行其所请求的操作。 服务账号：其使用主体是“应用程序”，专用于为 Pod 资源中的服务进程提供访问 Kubernetes API 时的身份标识（identity），Service Account 资源通常要绑定到特定的名称空间，它们由 API Server 自动创建或通过 API 调用，由管理员手动创建，通常附带着一组访问 API Server 的认证凭据 —— Secret，可由同一名称空间的 Pod 应用访问 API Server 时使用。 用户账号通常是用于复杂的业务逻辑管控，作用于系统全局，因而名称必须全局唯一。Kubernetes 并不会存储由认证插件从客户端请求中提取的用户及所属的组信息，因而也就没有办法对普通用户进行身份认证，他们仅仅用于检验该操作主体是否有权限执行其所请求的操作。相比较来说，服务账号则隶属于名称空间级别，仅用于实现某些特定操作任务，因此功能上要轻量得多。这两类账号都可以隶属于一个或多个用户组。 对 API Server 来说，来自客户端的请求要么与用户账户进行绑定，要么以某个服务账户的身份进行，否则会被视为匿名请求。这意味着集群内部或外部的每个进程，包括由人类用户使用 kubectl，以及各节点上运行的 kubelet 进程，再到控制平面的成员组件，必须在向 API Server 发出请求时进行身份验证。 认证、授权、准入控制基础Kubernetes 使用身份验证插件对 API Server 请求进行身份验证，它允许管理员自定义服务账号和用户账号要启用或禁用的插件，并支持各自同时启用多种认证机制。具体设定时，至少应该为服务账号和用户账号各自启用一个认证插件。 如果启用了多种认证机制，账号认证过程由认证插件以串行的方式进行，直到其中一种认证机制成功完成即结束。若认证失败，服务器则返回 401 状态码，反之，请求者就会被 Kubernetes 识别为某个具体的用户（以其用户名进行标识），并且该连接上随后的操作都会以此用户身份进行。API Server 对于接收到的每个访问请求会调用认证插件，尝试将以下属性与访问请求相关联。 用户名：用户名，例如 Kubernetes-admin 等。 用户 ID：用户的数字标签符，用于确保用户身份的唯一性。 用户组：用户所属的组，用于权限指派和继承， 常见的值可能是 system:masters 或者 devops-team 等。 附加字段：键值数据类型的字符串，用于提供认证需要用到的额外信息。 API Server 支持以下几种具体的认证方式，其中所有的令牌认证机制通常被统称为“承载令牌认证”。 X509 客户证书认证：通过给 API 服务器传递 --client-ca-file=SOMEFILE 选项，就可以启动客户端证书身份认证。 所引用的文件必须包含一个或者多个证书机构，用来验证向 API 服务器提供的客户端证书。 如果提供了客户端证书并且证书被验证通过，则 subject 中的公共名称（Common Name）就被作为请求的用户名。 静态令牌文件认证：当 API 服务器的命令行设置了 --token-auth-file=SOMEFILE 选项时，会从文件中读取持有者令牌。目前，令牌会长期有效，并且在不重启 API 服务器的情况下无法更改令牌列表。 启动引导令牌认证：一种动态管理承载令牌进行身份认证的方式，常用于简化组建新 Kubernetes 集群时将节点加入集群的认证过程，需要由 Kube-apiserver 通过 –enable-bootstrap-token-auth 选项启用，新的工作节点首次加入时，Master 使用引导令牌确认节点身份的合法性之后自动为其签署数字证书以用于后续的安全通信，kubeadm 初始化的集群也是这种认证方式。 Server Account 令牌：该认证方式会由 kube-apiserver 程序自动启用，它同样使用签名的承载令牌来验证请求，该认证方式还支持通过可选项 –service-account-key-file 加载签署承载令牌的秘钥文件，未指定时将使用 API Server 自己的 TLS 私钥，Server Account 通常由 API Server 自动创建，并通过 Server Account 准入控制器将其注入 Pod 对象，包括 Server Account 上的承载令牌，容器中的应用程序请求 API Server 的服务时以此完成身份认证。 那些未能被任何验证插件明确拒绝的请求中的用户即为匿名用户，该类用户会被冠以 system：anonymous 用户名，隶属于 system：unauthenticated 用户组。若 API Server 启用了除 Always Allow 以外的认证机制，则匿名用户处于启用状态，但是，处于安全因素考虑，建议管理员通过 –anonymous-auth=false 选项将其禁用。 除了身份信息，请求报文还需要提供操作方法及其目标对象，例如针对某 Pod 资源对象进行的创建、查看、修改或者删除操作等。具体包含以下信息。 API：用于定义请求的目标是否为一个 API 资源。 Request path：请求的非资源路径，例如 /api 或 /healthz。 API group：要访问的 API 组，仅对资源型请求有效，默认为 core API group。 Namespace：目标资源的名称空间，仅对于隶属于名称空间类型的资源有效。 API request verb：API 请求类的操作，即资源请求，包括 get、list、create、update、patch、watch、delete 等。 HTTP request verb：HTTP 请求类的操作，即非资源类请求要执行的操作，如 get、post、put、delete 等。 Resource：请求的目标资源的 ID 或名称。 Subersource：请求的子资源。 为了核验用户的操作许可，成功通过身份认证后的操作请求还需要转交给授权插件进行许可权限检查，以确保其拥有相应操作的许可。API Server 只要支持使用 4 类内置的授权插件来定义用户的操作权限。 Node：基于 Pod 资源的目标调度节点来实现对 kubelet 的访问控制。 ABAC：Attribute-based access control，基于属性的访问控制。 RBAC：Role-based access control，基于角色的访问控制。 Webhook：基于 HTTP 回调机制实现外部 REST 服务检查，确认用户授权的访问控制。 另外，还有 AlwaysDeny 和 AlwaysAllow 两个特殊的授权插件，其中 AlwaysDeny（总是拒绝）仅用于测试，而 AlwaysAllow（总是允许），则用于不期望进行授权检查时直接在授权检查阶段放行所有的操作请求。–authorization-mode 选项用于定义 API Server 要启用的授权机制，多个选项值彼此间以逗号进行分隔。 而准入控制器则用于在客户端请求经过身份验证和授权检查之后，将对象持久化存储到 etcd 之前拦截请求，从而实现在资源的创建，更新和删除操作期间强制执行对象的语义验证等功能，而读取资源信息的操作请求则不会经由准入控制器检查。API Server 内置了许多准入控制器，常用的包含下面列出的几种。 AlwaysAdmin 和 AlwaysDeny：前者允许所有请求，后者则拒绝所有请求。（已废弃，仅了解） AlwaysPullmages：总是下载镜像，即每次创建 Pod 对象之前都要去下载镜像。 NamespaceLifecycle：拒绝在不存在的名称空间中创建资源，而删除名称空间则会级联删除其下的所有其他资源。 LimitRanger：可用资源范围界定，用于对设置了 LimitRange 的对象所发出的所有请求进行监控，以确保其资源请求不会超限。 ServiceAccount：用于实现服务账号管控机制的自动化，实现创建 Pod 对象时自动为其附加相关的 Service Account 对象。 DefaultStorageClass：监控所有创建 PVC 对象的请求，以保证那些没有附加任何专用 StorageClass 的请求会被自动设定一个默认值。 ResourceQuota：用于为名称空间设置可用资源上限，并确保当其中创建的任何设置了资源限额的对象时，不会超出名称空间的资源配额。 早期的准入控制器代码需要由管理员编译进 kube-apiserver 中才能使用，实现方式缺乏灵活性。于是 Kubernetes 自 v1.7 版本引入了 Initializers 和 External Admin Webhooks 来尝试突破此限制，而且 v1.9 版本起，External Admin Webhooks 被分为 Mutating-Admission Webhooks 和 ValidatingAdmission Webhooks 两种类型，分别用于在 API 中执行对象配置的变异和验证操作。检查期间，仅那些顺利通过所有准入控制器检查的资源操作请求的结果才能保存到 etcd 中，而任何一个准入控制器的拒绝都将导致写入请求失败。 ServiceAccount 及认证 Kubernetes 原生的应用程序意味着专为运行于 Kubernetes 系统之上而开发的应用程序，这些程序托管运行在 Kubernetes 之上，能够直接与 API Server 进行交互，并进行资源状态的查询或更新，例如 Flannel 和 CoreDNS 等。显然，API Server 同样需要对来自 Pod 资源中的客户端程序进行身份验证，服务账号也是专用于这类场景的账号。ServiceAccount 资源一般由用户身份信息及保存了认证信息的 Secret 对象组成。 ServiceAccount 自动化我们创建的每个 Pod 资源都自动关联了一个 Secret 存储卷，并由其容器挂载至 /var/run/secret/kubernetes.io/serviceaccount 目录。各容器的挂载点目录通常存在 3 个文件：ca.crt、namespace 和 token，其中，token 文件保存了 ServiceAccount 的认证令牌，容器中的进程使用该账户认证到 API Server ，进而由认证插件完成用户认证并将其用户名传递给授权插件。 每个 Pod 对象只有一个服务账号，若创建 Pod 资源时未予以明确指定，则 ServiceAccount 准入控制器会为其自动附加当前名称空间中默认的服务账号，其名称通常为 default。 Kubernetes 系统通过 3 个独立的组件相互协作实现了上面描述的 Pod 对象服务账号的自动化过程：ServiceAccount 准入控制器、令牌控制器和 ServiceAccount 控制器。ServiceAccount 控制器负责为名称空间管理相应的资源对象，它需要确保每个名称空间中都存在一个名为 default 的服务账号对象。ServiceAccount 准入控制器内置在 API Server 中，负责在创建或更新 Pod 时按需进行 ServiceAccount 资源对象相关信息的修改，这包括如下操作。 若 Pod 没有显式定义使用的 ServiceAccount 对象，则将其设置为 default。 若 Pod 显式引用了 ServiceAccount，则负责检查被引用的对象是否存在，不存在时将拒绝 Pod 资源的创建请求。 若 Pod 中不包含 ImagePullSecret，则把 ServiceAccount 的 ImagePullSecret 附加其上。 为带有访问 API 的令牌的 Pod 对象添加一个存储卷。 为 Pod 对象中的每个容器添加一个 volumeMount，将 ServiceAccount 的存储卷挂载至 /var/run/secret/kubernetes.io/serviceaccount。 令牌控制器是控制平面组件 Controller Manager 中的一个专用控制器，它工作于异步模式，负责完成如下任务。 监控 ServiceAccount 的创建操作，并为其添加用于访问 API 的 Secret 对象。 监控 ServiceAccount 的删除操作，并删除其相关的所有 ServiceAccount 令牌秘钥。 监控 Secret 对象的添加操作，确保其引用的 ServiceAccount 存在，并在必要时为 Secret 对象添加认证令牌。 监控 Secret 对象的删除操作，以确保删除每个 ServiceAccount 对此 Secret 的引用。 ServiceAccount 基础应用ServiceAccount 是 Kubernetes API 上的一种资源类型，它属于名称空间级别，用于让 Pod 对象内部的应用程序在与 API Server 通信时完成身份认证。 命令式 ServiceAccount 资源创建： kubectl create serviceaccount 命令能够快速创建自定义的 ServiceAccount 资源，我们仅需要在命令后给出目标 ServiceAccount 资源的名称。 [root@k8s-master ~]# kubectl create serviceaccount my-service-account serviceaccount/my-service-account created Kubernetes 会为创建的 ServiceAccount 资源自动生成并附加一个 Secret 对象，该对象以 ServiceAccount 资源名称为前缀。该 Secret 对象属于特殊的 kubernetes.io/service-account-token 类型，它包含 ca.crt、namespace 和 secret 这 3 个数据项，它们分别是 Kubernetes Root CA 证书、Secret 对象所在名称空间和访问 API Server 的令牌。 ServiceAccount 资源清单： 更完善的创建 ServiceAccount 资源的方式是使用资源规范，该规范比较简单，它没有 spec 字段，仅指定了资源名称，以及允许 Pod 对象将其自动挂载为存储卷，引用的 Secret 对象则由系统自动生成。 apiVersion: v1 kind: ServiceAccout metadata: name: sa-demo namespace: default automountServiceAccountToken: true # 是否让Pod自动挂载API令牌 kubeconfig 配置文件基于无状态协议 HTTP/HTTPS 的 API Server 需要验证每次连接请求中的用户身份，因而 kube-controller-manager、kube-scheduler 和 kube-proxy 等各类客户端组件必须能自动完成身份认证信息的提交，但通过程序选项来提供这些信息会导致敏感信息泄露。另外，管理员还面临着使用 kubectl 工具接入不同集群时的认证及认证信息映射难题。为此，Kubernetes 设计了一种称为 kubeconfig 的配置文件，它保存有接入一到多个 Kubernetes 集群的相关配置信息，并允许管理员按需在各配置间灵活切换。 kubernetes cluster1 API Server kubectl ---> kubeconfig ---> kubernetes cluster2 API Server kubernetes cluster3 API Server 客户端程序可以通过默认路径、–kubeconfig 选项或者 KUBECONFIG 环境变量自定义要加载的 kubeconfig 文件，从而能够在每次的访问请求中可认证到目标 API Server。 kubeconfig 文件格式kubeconfig 文件中，各集群的接入端点以列表形式定义在 clusters 配置段中，每个列表项代表一个 Kubernetes 集群，并拥有名称识别；各身份认证信息定义在 users 配置段中，每个列表项代表一个能够认证到某 Kubernetes 集群的凭据。将身份凭据与集群分开定义以便复用，具体使用时还要以 context（上下文）在二者之间按需建立映射关系，各 context 以列表形式定义在 context 配置段中，而当前使用的映射关系则定义在 current-context 配置段中。 clusters: - cluster: name: kubernetes ...... users: - name: kubernetes-admin ...... contexts: - context: name: kubernetes-admin@kubernetes ...... current-context: kubernetes-admin@kubernetes 使用 kubeadm 初始化 Kubernetes 集群过程中，在 Master 节点上生成的 /etc/kubernetes/admin.conf 文件就是一个 kubeconfig 格式的文件，它由 kubeadm init 命令自动生成，可由 kubectl 加载后接入当前集群的 API Server。kubeconfig 文件的默认加载路径为 $HOME/.kube/config，在 kubeadm init 命令初始化集群过程中有一个步骤便是将 /etc/kubenetes/admin.conf 复制为该默认搜索路径上的文件。当然也可以通过 –kubeconfig 选项或 KUBECONFIG 环境变量将其修改为其他路径。 kubectl config view 命令能打印 kubeconfig 文件的内容，下面的命令结果显示了默认路径下的文件配置，包括集群列表、用户列表、上下文列表以及当前使用的上下文等。 [root@k8s-master ~]# kubectl config view apiVersion: v1 kind: Config preferences: {} clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://10.10.110.190:6443 name: kubernetes users: - name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes 用户可以在 kubeconfig 配置文件中按需自定义相关的配置信息，以实现使用不同的用户账号接入集群等功能。kubeconfig 是一个文本文件，尽管可以使用文本处理工具直接编辑，但强烈建议用户使用 kubectl config 及其子命令进行该文件的设定，以便利用其它自动进行语法检测等额外功能。kubectl config 的常用子命令有如下几项。 view：打印 kubeconfig 文件内容。 set-cluster：设定新的集群信息，以单独的列表项保存于 cluster 配置段。 set-credentials：设置认证凭据，保存为 users 配置段的一个列表项。 set-context：设置新的上下文信息，保存为 context 配置段的一个列表项。 use-context：设定 current-context 配置段，确定当前以哪个用户的身份接入到哪个集群当中。 delete-cluster：删除 cluster 中指定的列表项。 delete-context：删除 context 中指定的列表项。 get-cluster：获取 cluster 中定义的集群列表。 get-context：获取 context 中定义的上下文列表。https://www.vcbank.app/change 自定义 kubeconfig 文件通常，一个完整 kubeconfig 配置文件的定义至少应该包括集群、身份凭证、上下文以及当前上下文 4 项，但在保存有集群身份和身份凭据的现有 kubeconfig 文件基础上添加新的上下文时，可能只需要提供身份凭据而复用现有的集群定义，具体操作步骤需要按实际情况判定。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://www.missf.top/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://www.missf.top/tags/Kubernetes/"}]},{"title":"Ubuntu 18.04 系统初始化","slug":"Ubuntu 18.04 系统初始化","date":"2021-03-02T05:26:52.000Z","updated":"2021-07-26T10:36:32.000Z","comments":true,"path":"post/23473589.html","link":"","permalink":"https://www.missf.top/post/23473589.html","excerpt":"","text":"Ubuntu 18.04 系统初始化#!/bin/bash #Author: Created by MoWenJie #Function: Ubuntu 18.04 Server System initialization kernel_optimize() { cat >/etc/sysctl.conf&lt;&lt;EOF fs.file-max = 10000000 fs.nr_open = 10000000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_fin_timeout = 30 net.ipv4.tcp_keepalive_time = 1200 net.ipv4.ip_local_port_range = 10000 65000 net.ipv4.tcp_max_syn_backlog = 204800 net.ipv4.tcp_max_tw_buckets = 204800 net.ipv4.tcp_max_orphans = 204800 net.core.netdev_max_backlog = 204800 net.core.somaxconn = 65000 vm.swappiness = 0 net.ipv4.ip_nonlocal_bind = 1 net.ipv6.ip_nonlocal_bind = 1 net.unix.max_dgram_qlen = 128 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-arptables = 0 net.ipv4.conf.all.rp_filter = 0 net.ipv4.conf.default.rp_filter = 0 net.ipv4.conf.all.accept_source_route = 0 net.ipv4.conf.default.accept_source_route = 0 net.ipv4.neigh.default.gc_thresh1 = 512 net.ipv4.neigh.default.gc_thresh2 = 28672 net.ipv4.neigh.default.gc_thresh3 = 32768 net.ipv6.neigh.default.gc_thresh1 = 512 net.ipv6.neigh.default.gc_thresh2 = 28672 net.ipv6.neigh.default.gc_thresh3 = 32768 EOF sysctl --system } limits_process() { cat >/etc/security/limits.d/sys_init.conf&lt;&lt;EOF * soft nproc 2000000 * hard nproc 2000000 * soft nofile 2000000 * hard nofile 2000000 root soft nproc 2000000 root hard nproc 2000000 root soft nofile 2000000 root hard nofile 2000000 EOF ulimit -HSn 2000000 } time_sync() { crontab -l | grep \"ntpdate\" > /dev/null if [ $? -eq 0 ];then echo \"Time sync already exists\" else timedatectl set-local-rtc 1 timedatectl set-timezone Asia/Shanghai echo \"*/5 * * * * /usr/sbin/ntpdate ntp.aliyun.com &amp;>/dev/null\" | crontab echo \"Time sync Configuration successful\" fi } sshd_permitrootlogin() { grep \"#PermitRootLogin prohibit-password\" /etc/ssh/sshd_config if [ $? -eq 0 ];then sed -i 's!#PermitRootLogin prohibit-password!PermitRootLogin yes!g' /etc/ssh/sshd_config else echo \"root Remote Login open\" fi } install_soft(){ apt update &amp;&amp; apt upgrade apt install -y tcpdump bash-completion tcptraceroute bc git gcc make net-tools mtr traceroute psmisc tcptrack nload ntpdate vim lsof tree if [ $? != 0 ];then echo \"Apt installation error\" fi apt clean &amp;&amp; apt autoremove } command_line(){ echo 'PS1=\"\\[\\e[0m\\][\\[\\e[32;40m\\]\\u\\[\\e[33;40m\\]@\\[\\e[34;40m\\]\\h \\[\\e[36;40m\\]\\w\\[\\e[0m\\]]\\\\$ \"' >> /root/.bashrc &amp;&amp; bash } main() { kernel_optimize limits_process time_sync sshd_permitrootlogin install_soft command_line } main","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://www.missf.top/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://www.missf.top/tags/Ubuntu/"}]},{"title":"TCP SOCKET 的 backlog 参数用途","slug":"TCP SOCKET 的 backlog 参数用途","date":"2021-01-25T02:17:28.000Z","updated":"2021-03-12T08:30:52.000Z","comments":true,"path":"post/fe2ea6ca.html","link":"","permalink":"https://www.missf.top/post/fe2ea6ca.html","excerpt":"","text":"小记# tcp三次握手 tcp标志位有6种标示: SYN(synchronous建立联机) ACK(acknowledgement确认) PSH(push传送) FIN(finish结束) RST(reset重置) URG(urgent紧急) Sequence number(顺序号码) Acknowledge number(确认号码) 第一次握手: client ---------------> server # client发送后状态为:SYN_SEND server接受后状态为:SYN_RCVD server收到请求后会将这个请求放到syns queue队列中 client发送位码为 [SYN＝1,seq number= x(随机产生)] 的数据包到server,server由SYN=1得知client要求建立联机 第二次握手: server ---------------> client # client收到SYN+ACK将状态改为:ESTABLISHED(半连接状态) server收到client建立联机的请求后,向client发送[ack number=x + 1(client的seq number+1),SYN=1,ACK=1,seq number= y(随机产生)]的数据包 第三次握手: client ---------------> server # server收到ACK将状态修改为:ESTABLISHED 并把该请求从syns queue队列放到accept queue队列 client收到数据包后检查ack number是否正确(即client第一次发送的seq number+1),以及位码是否为ACK=1,如果正确,client会发送 [ack number=y+1(server的seq number+1),ACK=1]数据包给server,server接受之后确认ack number=y+1值与ACK=1,连接建立成功 # syns queue队列: 用于保存半连接状态的请求 队列大小通过/proc/sys/net/ipv4/tcp_max_syn_backlog指定,著名的SYN洪水攻击就是建立大量的半连接状态请求,然后丢弃,导致syns queue不能保存正常的请求,注意半连接队列长度不能超过全连接队列长度 # accept queue队列: 用于保存全连接状态的请求 队列大小通过/proc/sys/net/core/somaxconn指定,net.core.somaxconn 定义了系统级别的全连接队列最大长度,backlog 只是应用层传入的参数,不可能超过内核参数,所以 backlog 必须小于等于 net.core.somaxconn # backlog 的定义是已连接但未进行 accept 处理的 SOCKET 队列大小 backlog 是底层方法 int listen 的一个参数,Nginx/Tomcat 等这种 Web 服务器,都提供了 backlog 参数设置入口 只增大应用层 backlog 参数大小是没有意义的,因为可能内核参数关于连接队列设置的都很小,必须综合应用层和内核参数一起调整 system net.core.somaxconn = 65535 # 默认为128,定义系统中每一个端口最大的监听队列长度 net.core.netdev_max_backlog = 65535 # 默认为1000,请求被切换到CPU处理前被网卡缓存的速率包,根据网卡文档加大值可以提高性能 net.ipv4.tcp_max_syn_backlog = 65535 # 默认为1024,对于还未获得对方确认的连接请求,可保存在syns queue队列中的最大数目 前端 Nginx/Tomcat listen 80 backlog=65535; 后台 PHP listen.backlog = 65535 # 这里还有一个需要注意的点 如果机器的性能不高,我们依然增大 backlog 参数和内核连接队列,反而会适得其反 假设 PHP-fpm 的 QPS(每秒响应次数) 是 5000,那么处理完 65535 个请求大概需要 13 秒 但是前端 Nginx 和 PHP-fpm 的连接已经等待超时,当 PHP-fpm 处理完最后一个请求,再往这个 SOCKET ID 写数据时,却发现连接已经关闭,得到的是\"error: Broken Pipe\" 这也是为什么 2013 年 12 月 14 日发布的 PHP5.5.6 中 backlog 参数被修改为 65535,后来在 2014 年 7 月 22 日又修改为了 511","categories":[{"name":"Backlog","slug":"Backlog","permalink":"https://www.missf.top/categories/Backlog/"}],"tags":[{"name":"Backlog","slug":"Backlog","permalink":"https://www.missf.top/tags/Backlog/"},{"name":"高并发","slug":"高并发","permalink":"https://www.missf.top/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"}]},{"title":"Vim 学习笔记","slug":"Vim 学习笔记","date":"2021-01-20T05:47:57.000Z","updated":"2021-08-23T10:07:00.000Z","comments":true,"path":"post/bc97b757.html","link":"","permalink":"https://www.missf.top/post/bc97b757.html","excerpt":"","text":"学习缘由我也想成为使用 vim 写 python 代码的人，我觉得这样非常酷 历史vim 源于 vi 但不是 vi，vi 作为计算机的文本编辑器历史极为悠远，它是由美国计算机科学家比尔·乔伊编写并于 1976 年发布的。vim 诞生得要晚一些，它的第一个版本由布莱姆·米勒在 1991 年发布，这个兄弟也是一位声名显赫的程序员，80 年代买了一台 Amiga 电脑，打开电脑一看，居然没有他最常用的 vi 编辑器，对于米勒来说这是不可接受的。愤怒的米勒决定自己开发一个文本编辑器，完全复制 vi 的功能，并起名为 vi IMitation(模拟)。随着 vim 的不断发展，更多更好的功能被加了进来，正式名称改成了 vi IMproved(增强)，也就形成了现代的 vim，vim 的开发语言是 C 和 VimScript，目前最新的稳定版本是 8.2 理念vim 是一款完全面向程序员的软件，几乎没有使用 vim 编辑文字的普通用户，这可能是因为 vim 的学习曲线陡峭导致的。但是一旦熟练使用 vim 之后，就可以不依赖鼠标，双手尽可能停留在键盘中央的区域，这样使得我们编码、 插入、移动、定位、查找都不需要产生停顿和间隙，极大的提高了我们工作的效率 安装Red Hat 和 CentOS 系统默认的 vim 版本是 7.4，我们使用编译安装可以升级到最新版本 # 现在 vim 最新版本是 8.2 yum -y remove vim wget -O vim-8.2.2377.tar.gz https://github.com/vim/vim/archive/v8.2.2377.tar.gz tar xf vim-8.2.2377.tar.gz &amp;&amp; cd vim-8.2.2377 ./configure --prefix=/usr/local/vim/ --enable-pythoninterp=yes --with-python-config-dir=/usr/lib64/python2.7/config make -j4 &amp;&amp; make install ln -s /usr/local/vim/bin/vim /usr/bin/vim ln -s /usr/local/vim/bin/vimdiff /usr/bin/vimdiff # 如果 ./configure 过程中遇到 no terminal library found checking for tgetent()… configure: error: NOT FOUND! You need to install a terminal library; for example ncurses. Or specify the name of the library with –with-tlib. # Ubuntu 解决方法 sudo apt install libncurses5-dev # CentOS 解决方法 yum install ncurses-devel.x86_64 基本使用打开文件 vim filename # 如果文件名称存在,就直接打开文件 # 如果文件名称不存在,vim 会在退出保存时自动创建文件 # vim 命令后没有跟任何文件名称时,无法实现 \":wq\" 退出保存的,需要用 \":wq filename\" 定义文件名称 退出文件 \":q\" # 退出 \":q!\" # 强制不保存退出 \":w\" # 保存编辑后的内容(将内存缓冲区的数据写到文件中) \":w!\" # 强制保存编辑后的内容 \":wq\" # 保存并退出 \":wq!\" # 强制保存并退出 \":x\" # 保存并退出 # \":x\" 和 \":wq\" 的区别在于: \":wq\" 强制性写入文件并退出,即使文件没有被修改也强制写入并更新文件的修改时间 \":x\" 仅当文件被修改时才写入并更新文件修改时间,否则不会更新文件修改时间 # \":x\" 和 \":wq\" 在一般情况下没有什么区别,但是进行编程时,如果修改的是源代码文件,即使文件内容没有被修改,但是文件修改时间更新了,在重新编译项目时就得重新编译,产生不必要的系统资源 关于更多的 vim 基本使用会在后面一一讲解 模式vim 设计得最特别的地方就是它的模式，与其他大部分编辑器不同，进入 vim 后默认是正常模式，此时键入的字符并不会被插入到所编辑的文件之中。vim 的模式(mode)，是它的麻烦所在，但同时也是它的厉害所在，vim 有四种主要的模式。正常模式(normal 也称普通模式)，如果不加特殊的说明，一般提到的命令都直接在正常模式下输入，在其他的任何模式中，都可以通过 Esc 键返回到正常模式。插入模式(insert)，输入字符时使用，例如在正常模式下键入 i (insert) 或者 a (append) 即可进入插入模式。按 v 进入可视模式(visual)，用于选定文本块，vim 里还提供其他不同的选定方法，包括按行和按列块。命令行模式(command-line)，用于执行较长、较复杂的命令、在正常模式下键入冒号 : 即可进入该模式，使用斜杠 / 和问号 ? 开始搜索也算作命令行模式，命令行模式的命令需要输入回车键才会执行 配置和选项vim 是有配置文件的，我们可以根据使用习惯，配置属于自己的 vim。vim 的配置文件有三个路径，我们可以敲 vim –version 就能看到三行信息，配置文件是存在优先级的。一开始的配置文件是一个很简单的，甚至是一个空文件，随着我们的深入学习和使用，配置文件会变得越来越复杂 system vimrc file: \"/etc/vimrc\" # 系统配置 user vimrc file: \"$HOME/.vimrc\" # 用户配置 2nd user vimrc file: \"~/.vim/vimrc\" # 第二用户配置 # 优先级 三个文件只要存在一个,vim 就能正常运行 如果有用户配置,第二用户配置就无法生效 相同的配置项,用户配置会覆盖系统配置 系统配置里有而用户配置里没有的配置项,以系统配置为准 鼠标支持在 vim 里也可以使用鼠标，我们可以设置鼠标在 vim 的那个模式下生效。启用鼠标支持之后在不同的终端使用场景下，也有一定的区别，如果使用 xterm 兼容终端，在多窗口编辑的情况下可以使用鼠标进行窗口切换和拖拽窗口大小，如果使用 linux 兼容终端，那么鼠标支持是不生效的 # 鼠标模式 n 普通模式 v 可视模式 i 插入模式 c 命令行模式 h 在帮助文件里 a 以上所有模式 # 启用鼠标支持 set mouse=a # 关闭鼠标支持 set mouse-=a # 判断终端类型启用鼠标支持模式 if has('mouse') if has('gui_running') || (&amp;term =~ 'xterm' &amp;&amp; !has('mac')) set mouse=a else set mouse=nvi endif endif 备份和撤销文件set backup # 对一个文件修改之后生成对应的备份文件 set undofile # 对一个文件修改之后生成对应的撤销文件 set undodir=~/.vim/undodir # 指定撤销文件的存放目录 set backupdir=~/.vim/backupdir # 指定备份文件的存放目录 # 如果没有此目录则自动创建目录 if !isdirectory(&amp;undodir) call mkdir(&amp;undodir, 'p', 0700) endif if !isdirectory(&amp;backupdir) call mkdir(&amp;backupdir, 'p', 0700) endif set enc=utf-8 # enc 是显示文件时的编码(fenc 是当前文件编码,fencs 是打开文件时进行解码的) set nocompatible # 设置 vim 不需要和 vi 兼容 set backspace=indent,eol,start # backspace 键的工作模式,indent:允许删除自动缩进的空格 eol:能够将一行删除完后合并到上一行 start:删除此次进入插入模式前的输入 光标移动vim 里的基本光标移动是通过 h、j、k、l 四个键来实现的，之所以使用这四个键，是有历史原因的，当时的 vi 开发者的键盘上还没有我们现在独立的光标键，四个光标的符号直接标注在 h、j、k、l 四个字母按键上。因此，即使今天所有的键盘都有了光标移动键，很多资深的 vim 用户仍然使用这四个键来移动光标 vim 跳转到行首的命令是 0，跳转到行尾的命令是 $，还有使用 ^ 跳转到行首的第一个非空白字符 对于一次移动超过一个字符的情况，vim 支持使用 b/w 和 B/W，来进行以单词为单位的跳转，用来向后或向前跳转一个单词。大写个小写命令的区别在于，小写命令跟编程语言里的标识符规则相似，认为一个单词是由字母、数字、下划线组成的，而大写的命令则认为非空字符都是单词 根据单个字符来进行光标移动，分别是 f(find) 和 t(till)，fa 是移动光标到下一个 a 字符，ta 是移动光标到下一个字符 a 的前一个字符，大写的 F/T 代表反向 对于使用 vim 去阅读一些文档的时候，使用 ( 和 ) 分别是移动光标到上一句和下一句，使用 { 和 } 分别是移动光标到上一段和下一段 很多环境里，vim 支持使用 &lt;Home&gt; 和 &lt;End&gt; 跳转到文件的开头行和结尾行，如果不行，还可以使用 vi 兼容的 gg 和 G 跳转到开头行和结尾行(G 是跳转到结尾行的第一个字符) h:左 j:下 k:上 l:右 0:跳转到行首 $:跳转到行尾 ^:跳转到行首的第一个非空白字符 b/B:光标移至光标当前所在单词的词首,如果光标已经在单词的词首,则会跳至前一个单词的词首 e/E:光标移至光标当前所在单词的词尾,如果光标已经在单词的词尾,则会跳至后一个单词的词尾 w/W:向前跳转一个单词或字符串,光标停留在单词词首 A:光标移至行尾并进入编辑模式 I:光标移至行首并进入编辑模式 fa:移动光标到当前行下一个 a 字符 2fa:移动光标到当前行第二个 a 字符 ta:移动光标到当前行下一个字符 a 的前一个字符 2ta:移动光标到当前行第二个字符 a 的前一个字符 F/T:代表反向 (:移动光标到上一句 ):移动光标到下一句 {:移动光标到上一段 }:移动光标到下一段 %:匹配括号移动光标,包括 ( { [,需要将光标先移动到括号上(编辑 Nginx 的配置文件时非常方便) */#:匹配光标当前所在的单词的下一个和上一个 gg:移动光标到首行的第一个字符 G:移动光标到尾行的第一个字符 :n/ngg/nG:指定光标跳转到 n 行 &lt;PageUp>:向上翻页 &lt;PageDown>:向下翻页 n|:n代表数字,移动到指定的列 文本修改在 vim 的一般原则里，常用的功能，按键应尽可能少，因此很多相近的功能在 vim 里会有不同的按键，不仅如此，大写键也一般会重载一个相近但稍稍不同的含义 dd:删除整行 d0:光标位置(不包含)删除到行首的所有字符 d$/D:光标位置(包含)删除到行尾的所有字符 db:删除光标当前位置(不包含)到单词起始处的所有字符 de:删除光标当前位置(包含)到单词结尾处的所有字符 dw:删除光标当前位置(包含)到下个单词起始处的所有字符 cc/S:删除整行并进入插入模式 c$/C:光标位置删除到行尾并进入插入模式 s/cl:删除一个字符并进入插入模式 i:在当前光标字符前面进入插入模式 I:光标移动到行首非空白字符并进入插入模式 a:在当前光标字符后面进入插入模式 $a/A:光标移动到行尾并进入插入模式 o:在当前行的下方插入新行并进入插入模式 O:在当前行的上方插入新行并进入插入模式 ra:光标下的字符替换为a R:进入替换模式,每次按键替换一个字符(直到按下&lt;Esc>) u:撤销上一个修改动作(可多次撤销) U:撤销当前行上的所有修改 文本对象如果想要在 vim 里拥有高效编辑的能力， 必然要掌握超过单个字符编辑的能力，也就是说，要掌握词(word)、句子(sentense)、段落(paragraph)级别的编辑能力。在 Vim 里，这样以一定标准分隔符界定的概念叫做文本对象(text objects)。文本对象是一个很强大的功能，无论光标处于该文本对象的哪个字符中，我们都可以对整个文本对象进行操作，这也是为什么 vim 是世界上最快的编辑器的原因 # 文本对象常用的编辑命令 y:复制 d:删除 c:替换 v:选中 # 文本对象有以下几种(标签用 t 表示) w s p '' \"\" &lt;> [] () {} &lt;tag> # 文本对象的操作范围有两种 i:是inner的意思,操作时不包括单词边上的空格符或包围符号 a:是arround的意思,操作时包括单词边上的空格符或包围符号 # 文本对象操作列子 操作文本对象:&lt;h1>Sample Title&lt;/h1>,光标在Sample单词的S上 dw:&lt;h1>Title&lt;/h1> diw:&lt;h1> Title&lt;/h1> daw:&lt;h1>Title&lt;/h1> dit:&lt;h1>&lt;/h1> (t代表的是&lt;tag>文本对象) dat:Empty (这里因为文本对象的操作范围是a,所有连同包围符号也一并删除) # 操作文本对象:cdmuwfon.rg(\"stwq jkntrc,\" + \"opac liixisq\"),光标在stwq单词的s上(对于成对的标签符号操作时,光标可以在标签内的任何位置) di\":cdmuwfon.rg(\"\" + \"opac liixisq\") da\":cdmuwfon.rg(+ \"opac liixisq\") di(:cdmuwfon.rg() # (和)都可以 da(:cdmuwfon.rg ci\":cdmuwfon.rg(\"\" + \"opac liixisq\") # 并且进入了插入模式 ca\":cdmuwfon.rg(+ \"opac liixisq\") # 并且进入了插入模式 ci(:cdmuwfon.rg() # 并且进入了插入模式,这在编程中非常方便 ca(:cdmuwfon.rg # 并且进入了插入模式 vi(:进入视图模式并且选中\"stwq jkntrc,\" + \"opac liixisq\" # 可以按d进行删除 va(:进入视图模式并且选中(\"stwq jkntrc,\" + \"opac liixisq\") 更多的文本对象操作可以使用如下的示例进行各种各样的组合，让复杂的操作只需要几个简单的组合键就能完成，极大的提高了我们的工作效率 [y:复制 d:删除 c:替换 v:选中] [i和a 文本对象的操作范围] [w s p '' \"\" &lt;> [] () {} &lt;tag> 文本对象] 重复操作vim 里有非常多的命令组合，如果我们需要重复这样的命令，每次都要手敲一遍，这显示不是一件容易的事情。其实 vim 已经想到了这个问题，提前定义好了一些简单的重复键 ;:重复最近的字符查找操作(f t) ,:反向 n:重复最近的字符查找操作(/ ?) N:反向 .:重复执行最近的修改操作 目录结构vim 的工作环境是由运行支持文件来设定的，如果想要定制 vim ，就需要知道 vim 的目录结构 # 以 vim8.2 为例,标准的安装位置分别在 Unix:/usr/share/vim/vim82(如果是编译安装则取决于你的安装目录) Windows:C:\\Program Files(x86)\\Vim\\vim82 # 这个目录下面还有很多子目录,这些子目录下面就是分类放置的 vim 支持文件 syntax:vim的语法加亮文件 doc:vim的帮助文件 colors:vim的配色方案 plugin:vim的\"插件\",即用来增强vim功能的工具 以 syntax 目录为例，目录下有 628 个文件都是以 .vim 作为后缀，就代表 vim 对 628 不同的文件类型提供语法加亮支持，例如 java.vim 文件，就是对 java 类型的文件进行语法加亮，也可以用 :setfiletype java 这样的命令来设置文件的类型 plugin 目录下的系统内置插件不多 getscriptPlugin:获得最新的vim脚本的插件(现在都广泛使用Git) gzip:编辑.gz压缩文件(编辑后缀为.gz的文件时自动解压和压缩,用户感知不到这个文件是压缩的) logiPat:模式匹配的逻辑运算符(允许以逻辑运算,而非标准正则表达式的方式来写模式匹配表达式) manpager:使用vim来查看man帮助(强烈建议试一下,记得使用vim的跳转键 C-] 和 C-T) matchparen:对括号进行高亮匹配(现代编辑器基本都有类似的功能) netrwPlugin:从网络上编辑文件和浏览远程目录(支持多种常见协议,如ftp和scp,可直接打开目录来选择文件) rrhelper:用于支持 --remote-wait 编辑(vim的多服务器会用到这一功能) spellfile:在拼写文件缺失时自动下载(vim一般只安装了英文的拼写文件) tarPlugin:编辑压缩的tar文件(tar 不支持写入) tohtml:把语法加亮的结果转成HTML文件并保存 vimballPlugin:创建和解开.vba文件(过时) zipPlugin:编辑zip文件(zip 可支持写入) # 打开远程文件和浏览目录 format: protocol://[user@]hostname[:port]/[path] vim scp://root@k8s-node1/test # 打开root用户家目录下的test文件 vim scp://root@k8s-node1//etc/docker/ # 使用远程终端的绝对路径,要使用双斜杠 # 使用vim查看man帮助文档 export MANPAGER=\"vim -M +MANPAGER -\" 包管理器vim 的插件严格来说应该叫包，我们安装一个插件，就是在 .vim 目录下解压插件包，基本上是安装之后就不管了，即使这个插件有更新，我们也不能及时的更新到最新的版本。现在 git 的流行，让我们对版本的控制变得简单，而在有了包管理器之后，配合 git 的版本控制，能够让我们非常方便的安装插件和更新插件，已经一系列对插件的管理操作 安装 minpac 包管理器并通过包管理器安装插件 # 安装 minpac git clone https://github.com/k-takata/minpac.git ~/.vim/pack/minpac/opt/minpac # 初始化包管理器和指定需要安装的插件(写入到 vim 的配置文件) function! PackInit() abort packadd minpac call minpac#init() call minpac#add('k-takata/minpac', {'type': 'opt'}) \" Additional plugins here. call minpac#add('vim-jp/syntax-vim-ex') call minpac#add('tyru/open-browser.vim') call minpac#add('rkulla/pydiction') # python 补全插件 endfunction \" Plugin settings here. \" Define user commands for updating/cleaning the plugins. \" Each of them calls PackInit() to load minpac and register \" the information of plugins, then performs the task. command! PackUpdate call PackInit() | call minpac#update() # 自定义命令 command! PackClean call PackInit() | call minpac#clean() command! PackStatus packadd minpac | call minpac#status() # 保存 .vimrc 文件,重启 vim 之后我们就有了三个自定义的命令(命令模式下) PackUpdate PackClean PackStatus # 安装插件 在 .vimrc 文件写入插件的 GitHub 项目的用户名/项目名,通过 :PackUpdate 命令更新插件 插件格式为:call minpac#add('[package-author]/[package-name]') # 删除插件 同样需要编辑 .vimrc 文件,删除不需要的插件,通过 :PackClean 命令更新插件,插件就会被删除 # 查看插件状态 :PackStatus 插件安装成功的界面 复制粘贴一般而言，对于终端 vim 来说，它是没法分辨用户是输入操作还是粘贴操作的。因此在粘贴内容时，Vim 的很多功能，特别是智能缩进、制表符转换等功能(这些功能是用于输入操作的)，就会修改粘贴的内容，导致我们粘贴的内容显示的结果不对，或者出现乱码。要解决这个问题，我们就得让 vim 知道，我们的操作到底是输入操作还是粘贴操作，vim 有一个 paste 选项，就是用来切换输入和粘贴的状态的，如果现在处于 :set paste 状态，vim 就认为现在是粘贴操作，智能缩进、制表符转换等功能就不会修改粘贴的内容，不过每次都手动修改这个状态是非常麻烦的，下面有两个优化方法 # 方法一,通过自定义键来切换paste和nopaste的状态 set pastetoggle=&lt;F2> # 在插入模式下,按&lt;F2>会切换paste状态 nnoremap &lt;F2> :set invpaste paste?&lt;CR> # 在命令模式下,按&lt;F2>会切换paste状态 imap &lt;F2> &lt;C-O>:set invpaste paste?&lt;CR> # 以nopaste状态进入插入模式后,可以按一次&lt;F2>切换paste状态 # 方法二,进入插入模式的时候自动开启paste,退出插入模式自动关闭paste if has('autocmd') augroup vimrcExtension autocmd! autocmd InsertEnter * set paste autocmd InsertLeave * set nopaste if ! has('gui_running') set ttimeoutlen=10 autocmd InsertEnter * set ttimeoutlen=0 autocmd InsertEnter * set ttimeoutlen=1000 endif augroup end endif # 一直处于 paste 开启的状态下虽然不影响基本的功能,但是会影响其他插件的正常工作(例如 python 补全插件就无法使用了) 交换文件对一个单独的文件使用多个 vim 会话进行编辑，很容易出现冲突的情况，所以使用 vim 时肯定会遇到过 Swap file “.filename.swp” already exists! 这个错误提示，出现这个错误提示有两种原因 上次编辑这个文件时，发生了意外崩溃，导致文件没有存盘就退出了 有另一个会话正在使用 vim 编辑这个文件 当错误提示的 process ID 后面没有 (still running) 的字样，就是第一种情况，否则就是第二种情况。第一种情况下，vim 支持即使没有存盘的情况下仍然保存编辑状态，我们可以按 r 键来恢复上次没有存盘的内容，在文件恢复之后，vim 仍然不会删除崩溃时保留下来的那个交换文件，因此我们恢复文件内容之后，确定内容无误就可以保存文件。重新打开文件，按 d 键可以删除交换文件，也可以单独使用 rm -f 删除交换文件(交换文件一般是 .filename.swp 的格式)。第二种情况下，是有另一个会话正在使用 vim 编辑这个文件，这时候是没有 delete 交换文件这一选项的，这时候一般选择 q 或者 a 放弃编辑，如果只是要查看文件，那也可以选择 o 以只读文件打开，需要用到 e 强行进行编辑的情况很少 Swap file \".nginx.conf.swp\" already exists! [O]pen Read-Only, (E)dit anyway, (R)ecover, (Q)uit, (A)bort: 编辑多个文件vim 支持一次性打开多个文件，只需要在命令行上写出多个文件即可，还支持通配符的方式。例如我们可以使用 vim *.cpp 或者 vim *.yaml 去编辑多个文件，但是执行这个命令之后只会打开所有文件中的第一个文件，这是 vim 为了确保低配置环境也能正常工作而设计的，避免不必要的内存浪费，其实在执行上述命令的时候，vim 建立了一个文件列表，并且暂时只打开其中的第一个文件，接下来用户可以在不退出 vim 的情况下，查看文件列表，继续编辑下一个文件或者退出编辑 \":args\" # 显示编辑的所有文件列表,其中[]括起来的文件是你正在编辑的文件 \":args **/*.yaml/filename\" # 在进入 vim 之后,打开当前目录下的 *.yaml 文件和打开指定文件 \":next/:n\" # 编辑下一个文件,如果当前文件未存盘则会报错,命令后面加!则会放弃修改内容,可以设置 vim 在切换文件时自动存盘:set autowrite \":Next/:previous\" # 打开上一个文件 \":first/rewind\" # 回到文件列表的第一个文件 \":last\" # 打开文件列表的最后一个文件 \":n|normal ggp\" # 切换到下一个文件并在正常模式下执行 ggp 命令 缓冲区的管理和切换vim 里会对每一个已打开或要打开的文件创建一个缓冲区，这个缓冲区就是文件在 vim 中的映射，它是 vim 里的一个基本概念。缓冲区(buffer)就是一块内存区域，里面存储着正在编辑的文件，如果没有把缓冲区里的文件存盘，那么原始文件不会被更改。在多文件编辑的时候你也会有同样数量的缓冲区，不过缓冲区的数量常常会比编辑多文件时的文件列表数更高，因为你用 :e/:o 等命令另外打开的文件不会改变命令行参数(就是不加入到 :args 的文件列表)，但同样会增加缓冲区的数量。 此外 :args 代表参数列表 / 文件列表，真的只是文件的列表而已。比起文件列表，缓冲区中有更多信息，最基本的就是记忆了光标的位置。在 vim 里，除了切换到下一个文件这样的批处理操作外，操作缓冲区的命令比简单操作文件的命令更为方便 # 使用通配符命令编辑多个文件 vim *.yaml # :args 查看文件列表 [deployment.yaml] emptyDir.yaml hostpath-vol.yaml ingress.yaml nfs-vol.yaml service.yaml # :ls/:files/:buffers 查看缓冲区列表 1 %a \"deployment.yaml\" line 45 2 \"emptyDir.yaml\" line 0 3 \"hostpath-vol.yaml\" line 0 4 \"ingress.yaml\" line 0 5 \"nfs-vol.yaml\" line 0 6 \"service.yaml\" line 0 Press ENTER or type command to continue 可以看到，文件列表和缓冲区列表都展示了打开的所有文件，而且分别使用 [] 和 %a 标示了当前正在编辑的文件。不过缓存区列表比文件列表给出了更多的文件信息 # 缓冲区示例 1 %a \"deployment.yaml\" line 45 # 参数意义 1:代表缓冲区列表文件的编号 %a:缓冲区的状态 \"deployment.yaml\":文件名字 line 45:光标所在的行 缓冲区状态 %:当前缓冲区 a:活动缓存区,当前显示在屏幕上的 #:交换缓冲区(最近的缓冲区) =:只读缓冲区 +:已经更改的缓冲区 -:非活动的缓冲区 h:隐藏的缓冲区 打开缓冲区 :buffer number # 以缓冲区列表文件编号来打开缓冲区 :buffer filename # 以缓冲区列表文件名字来打开缓冲区 :sbuffer number/filename # 分割当前窗口开始编辑另一个缓冲区,如果没有指定 number/filename,则以当前缓冲区进行窗口分割 :ball # 为每一个缓冲区打开一个窗口 切换缓冲区 :bnext # 切换到下一个缓冲区 :bprevious/:bNext # 切换到上一个缓冲区 :blast # 切换到最后一个缓冲区 :bfirst # 切换到第一个缓冲区 :set hidden # 允许缓冲区在未保存的情况下进行切换(修改由vim进行保存) 删除缓冲区 :bdelete filename/3 # 根据文件名字或者编号来删除一个缓冲区 :1,3 bdelete # 根据指定范围来删除缓冲区 :bdelete! filename # 强制删除缓冲区 卸载缓冲区 :bunload filename # 从内存中卸载一个缓冲区,这个缓冲区打开的所有窗口都会关闭,如果缓冲区被改动过,那么该命令将失败 :bunload! # 强制卸载缓冲区,但所有的改动也将会丢失 多窗口编辑前面所讲的编辑多个文件，也只是在单个窗口进行的，这样的局限在于，我们既不能同时修改两个文件，也不能在同一窗口对比两个文件。我们想要自己同时查看、编辑多个文件，最基本的命令就是 :split (缩写 :sp)，这个命令后面如果有文件名，表示分割窗口并打开指定的文件，如果没有文件名，那就表示仅仅把当前窗口分割开，当前编辑的文件在两个窗口里都显示，:split 默认使用水平分割的方式。竖直分割的命令是 :vsplit (缩写 :vs)，竖直分割要求屏幕比较宽，但如果你想对比两个文件时，竖直分割就会更方便 &lt;Ctrl-w> s/:split/:sp # 水平分割当前窗口 &lt;Ctrl-w> v/:vsplit/:vs # 竖直分割当前窗口 :split/:sp filename # 水平分割窗口并打开指定的文件 :vsplit/:vs filename # 竖直分割窗口并打开指定的文件 &lt;Ctrl-w> (h j k l 方向键) # 可以在多窗口之间跳转 &lt;Ctrl-w> w # 跳转到下一个(往右和往下跳)窗口,W反向 &lt;Ctrl-w> n/:new # 打开一个新窗口 &lt;Ctrl-w> c/:close # 关闭当前窗口,如果当前窗口已经是最后一个则无效 &lt;Ctrl-w> q/:quit # 退出当前窗口,当最后一个窗口退出时则退出 vim &lt;Ctrl-w> o/:only # 只保留当前窗口,关闭其他所有窗口 &lt;Ctrl-w> = # 使得所有窗口大小相同 文件比较多窗口编辑中有一个非常有用的功能，那就是比较两个文件的内容。vim 对此有特殊的支持，使用 vimdiff 或 gvimdiff 命令，后面跟上需要比较的两个文件，就能打开两个窗口比较两个文件了。在比较时，vim 会折叠相同的代码行，并加亮两边文本不同的部分，窗口的滚动也是联动的 vimdiff emptyDir1.yaml emptyDir2.yaml # 比较两个文件的内容 比较文件的实际截图如下 NERDTree 插件NERDTree 是最为著名的一个文件浏览 / 管理插件，就是你知道文件大概在哪里，但不知道文件具体名字时的一个好选择。跟很多 vim 插件一 样，NERDTree 会利用多窗口的特性 安装 NERDTree 插件 在vimrc的\"Other plugins\"下面加入以下语句 call minpac#add('preservim/nerdtree') 执行:PackUpdate命令进行安装 安装成功之后，NERDTree 缺省就会抢占 netrw 使用的路径形式，我们在 vim 中可以使用 e . 来打开 NERDTree(. 代表当前路径) ，也可以在打开一个文件之后使用 :NERDTreeToggle 命令。在打开 NERDTree 窗口之后，使用还是相当直观的，并且按下 ? 就可以查看帮助信息 在文件或目录上敲回车或双击立即打开该文件或目录 在文件上使用 go 会预览该文件，也就是光标不会跳转到文件所在的窗口中 按 i 会打开文件到一个新的水平分割的窗口中，按 s 会打开文件到一个新的竖直分割的窗口中，按 t 会打开文件到一个新的标签页中 NERDTree 会自动过滤隐藏文件和目录，但如果你需要看到它们的话，也可以用 I 来开启和关闭隐藏文件的显示 按 m 会出现一个菜单，允许添加、删除、更名等操作 正则表达式通过前面的学习，可能你已经知道搜索命令 / 和替换命令 :s 的用法了。其实，我们输入的待查找内容是被 vim 当成正则表达式来看待的，这里我们来简单学习一下 vim 里的正则表达式，它跟其他常用的正则表达式还是有区别的。在一个搜索表达式里(或者称为模式 pattern)，. 、* 、^ 、$ 、~ 、[] 、\\ 是有特殊含义的字符 . 可以匹配除换行符外的任何字符: 如a.可以匹配\"aa\" \"ab\" \"ac\"等,但不能匹配\"a\" \"b\"或\"ba\",如果需要匹配换行符(跨行匹配)的话,则需要使用\\_. * 表示之前的匹配源(最普通的情况为单个字符,匹配源可以是一个字符串,但需要该字符串需要组成一个项,如:\\(ab\\)*)重复零次或多次: 如aa*可以匹配\"a\" \"aa\" \"aaa\" ^ 匹配一行的开头,如果出现在模式的开头的话;在其他位置代表字符本身 $ 匹配一行的结尾,如果出现在模式的结尾的话;在其他位置代表字符本身 ~ 匹配上一次替换的字符串,即如果上一次你把\"foo\"替换成了\"bar\",那~就匹配\"bar\" [...] 匹配方括号内的任一字符,方括号内如果第一个字符是^,表示对结果取反,除开头之外的-表示范围:如[A-Za-z]表示任意一个拉丁字母,[^-+*/] 表示除了\"-\" \"+\" \"*\" \"/\"外的任意字符 \\ 的含义取决于下一个字符,在大部分的情况下把某些含有特殊意义的字符进行转义,让它们代表字符本身(. * \\ ^ $ ~ [ ]) 除此之外的字符都是普通字符，没有特殊含义。不过，需要注意的是，如果使用 / 开始一个搜索命令，或者在替换命令(:s)中使用 / 作为模式的分隔符，那模式中的 / 必须写作 / 才行，否则 Vim 看到 / 就会以为模式结束了，导致错误发生。为了方便书写，我们可以用其他的符号作为模式的分隔符，例如想把”/image/“全部替换成”/images/“的话，我们可以把 :%s//image///images//g 写成 :%s#/image/#/images/#g，这只能适用于替换命令，而在使用搜索命令 / 时我们就没有办法了，只能把模式里的 / 写作 / 通过 \\ 开始的特殊表达式 \\? 表示之前的匹配源重复零次或一次:如 aa\\? 可以匹配\"a\" \"aa\"但不能完整匹配\"aaa\"(可以匹配其前两个字符、后两个或最后一个字符) \\+ 表示之前的匹配源重复一次或多次:如 aa\\+ 可以匹配\"aa\" \"aaa\"但不能匹配\"a\"或\"b\" \\{n,m} 表示之前的匹配源重复 n 到 m 遍之间,两个数字可以省略部分或全部:如 a\\{3}(可读作:3个\"a\")可以匹配\"aaa\" a\\{,3}(可读作:最多3个\"a\")可以匹配\"\" \"a\" \"aa\"和\"aaa\" 两个数字都省略时等价于* 也就是之前的匹配源可以重复零次或多次 \\(和\\) 括起一个模式,将其组成为单个匹配源:如 \\(foo\\)\\? 可以表示单词\"foo\"出现零次或一次 \\(和\\)还有一个附加作用,是捕获匹配的内容,按\\(出现的先后顺序,可以用 \\1 \\2到 \\9来引用,如果你不需要捕获匹配内容的话,用\\%( 和 \\) 的性能更高 \\&amp; 是分支内多个邻接(concat)的分隔符,概念上可以和与操作相比,表示每一项都需要匹配成功,然后取最后一项的结果返回:如 .*foo.*\\&amp;.*bar.* 匹配同时出现了\"foo\"和\"bar\"的完整行 \\| 是多个分支的分隔符,概念上可以和或操作相比,表示任意一项匹配成功即可:如foo\\|bar 可匹配\"foo\"或\"bar\"两单词之一 \\&lt; 匹配单词的开头 \\> 匹配单词的结尾 \\s 匹配空白字符 &lt;Space> 和 &lt;Tab> \\S 匹配非空白字符 \\d 匹配数字,相当于 [0-9] \\D 匹配非数字,相当于 [^0-9] \\x 匹配十六进制数字,相当于 [0-9A-Fa-f] \\X 匹配非十六进制数字,相当于 [^0-9A-Fa-f] \\w 匹配单词字符,相当于 [0-9A-Za-z_] \\W 匹配非单词字符,相当于 [^0-9A-Za-z_] \\h 匹配单词首字符,相当于 [A-Za-z_] \\H 匹配非单词首字符,相当于 ^[A-Za-z_] \\c 忽略大小写进行匹配 抽象地讨论正则表达式恐怕你也不容易记住，还是拿一些具体的例子来练习一下吧 搜索替换实例搜索表达式 /aae # 往后查找aae,没有使用界定符,会查找到aaes qaae等单词 /\\&lt;name\\> # 搜索单词name,使用\\&lt; \\>进行界定单词的开头和结尾,这样的话names是搜索不到的 /\\&lt;\\(red\\|blue\\)\\> # 搜索单词red或blue ?aae # 往前查找aae,没有使用界定符,会查找到aaes qaae等单词 * # 搜索光标下的单词(光标停留在name上,键入*会跳转到下一个name) n # 光标跳转到后一个 N # 光标跳转到前一个 替换表达式 Example:[range]s/{pattern}/{string}/[flags] # flags 有如下四个选项 c confirm 每次替换前询问 e error 不显示错误 g globle 不询问,整行替换,如果不加g选项,则只替换每行的第一个匹配到的字符串 i ignore 忽略大小写 这些选项可以合并使用,如cgi表示不区分大小写,整行替换,替换前询问 # 替换例子 :s!ma!am! # 把当前行的ma替换成am :s!ma!am!g # g标记表示替换行中的所有匹配点(不加g只会替换行中的一个匹配点) :%s!ma!am!g # 把全文的ma替换成am(1,$s!ma!am!g也是一样的效果) :1,10s!ma!am! # 表示把第1到第10行(包含1到10行)的ma替换成am 删除替换表达式 :%s!\\s\\+$!!g # 删除全文行尾的空白字符(&lt;Space> 和 &lt;Tab>) :%s!^\\s\\+!!g # 删除全文行首的空白字符(&lt;Space> 和 &lt;Tab>) :g/^\\s*$/d # 全局删除沒有內容的空行 :%s!^\\s*$\\n!!g # 把沒有內容的空行(空格 制表符 换行符)替换为空,等于删除空行的效果 文件类型和关联设定程序源代码通常由文件组成，每个文件都有一个关联的文件类型。这个文件类型决定了 vim 对其进行处理的一些基本设定，可能受影响的设定具体有以下方面 如何对文件进行高亮(不同的语言高亮不一样，例如 .c 和 .py 文件) 制表符 &lt;tab&gt; 的宽度(4个空格数或者8个空格) 是否在键入 &lt;tab&gt; 时扩展为空格字符 每次缩进的空格数(可以和制表符宽度不同) 采用何种自动缩进方法 其他可适用的选项 # 开启文件类型检测(写入vimrc配置文件) filetype plugin indent on # filetype on 打开文件类型检测功能,它相当于文件类型检测功能的开关 # filetype plugin on 允许vim加载文件类型插件,vim会根据检测到的文件类型,在runtimepath中搜索该类型的所有插件并执行 # filetype indent on 允许vim为不同类型的文件定义不同的缩进格式 # 查看文件类型检测是否开启 :filetype # 如果文件的类型未能被正确的检测出来,可以手动设置文件类型 :set filetype=c 插入模式自动补全自动补全是一个编辑器中很主流的功能，编辑器能够在用户输入一部分内容时就能猜到用户希望输入的是什么，并能够予以提示。自动补全可以节约我们输入的工作量，非常值得我们去学习，vim 内置就有自动补全的功能，补全的对象如下 整行补全 CTRL-X CTRL-L 根据当前文件里关键字补全 CTRL-N/CTRL-P 根据字典补全 CTRL-X CTRL-K 根据同义词字典补全 CTRL-X CTRL-T 根据头文件内关键字补全 CTRL-X CTRL-I 根据标签补全 CTRL-X CTRL-] 补全文件名或路径 CTRL-X CTRL-F 补全宏定义 CTRL-X CTRL-D 补全vim命令 CTRL-X CTRL-V 用户自定义补全方式 CTRL-X CTRL-U 拼写建议 CTRL-X CTRL-S 停止补全并且不应用匹配项 CTRL-E 在补全菜单应用对象且离开补全模式 CTRL-Y/&lt;Enter> CTRL-N 和 CTRL-P 分别是对当前文件里的关键字进行补全(往下查找和往上查找)，其他补全都需要先进入 CTRL-X 模式，再键入对应的命令进入对应的补全模式。使用 CTRL-N 和 CTRL-P 上下移动时，输入的文本也会随之变化。使用 &lt;Up&gt; 和 &lt;Down&gt; 上下移动时，输入的文字并不会变化。使用 &lt;PageUp&gt; 和 &lt;PageDown&gt; 键，可以在补全菜单中翻页。使用 Esc 键，将关闭弹出菜单，但会保留之前应用的对象。也就是说，如果你不希望应用任何对象完成补全时，应该使用 CTRL-E 键，而不是使用 Esc 键来取消补全操作 补全文件名字或者路径(基于文件系统) Python 开发环境","categories":[{"name":"Vim","slug":"Vim","permalink":"https://www.missf.top/categories/Vim/"}],"tags":[{"name":"Vim","slug":"Vim","permalink":"https://www.missf.top/tags/Vim/"},{"name":"编辑器","slug":"编辑器","permalink":"https://www.missf.top/tags/%E7%BC%96%E8%BE%91%E5%99%A8/"}]},{"title":"Wrk 性能测试工具","slug":"Wrk 性能测试工具","date":"2021-01-04T02:58:13.000Z","updated":"2021-01-20T05:51:10.000Z","comments":true,"path":"post/bcbad8d3.html","link":"","permalink":"https://www.missf.top/post/bcbad8d3.html","excerpt":"","text":"wrk 简介wrk 是一款针对 HTTP 协议的基准测试工具，它能够在单机多核 CPU 的条件下，使用系统自带的高性能 I/O 机制，如 epoll，kqueue 等，通过多线程和事件模式，对目标机器产生大量的负载。wrk 是复用了 redis 的 ae 异步事件驱动框架，准确来说 ae 事件驱动框架并不是 redis 发明的，它来自于 Tcl 的解释器 jim，这个小巧高效的框架，因为被 redis 采用而被大家所熟知 wrk 安装# Ubuntu/Debian apt-get install build-essential libssl-dev git -y git clone https://github.com/wg/wrk.git wrk cd wrk &amp;&amp; make -j4 cp wrk /usr/local/bin # CentOS/RedHat yum groupinstall 'Development Tools' yum install -y openssl-devel git git clone https://github.com/wg/wrk.git wrk cd wrk &amp;&amp; make -j4 cp wrk /usr/local/bin 验证安装是否成功 wrk -v # 查看版本 wrk [epoll] Copyright (C) 2012 Will Glozer Usage: wrk &lt;options> &lt;url> Options: -c, --connections &lt;N> Connections to keep open 跟服务器建立并保持的TCP连接数量 -d, --duration &lt;T> Duration of test 压测时间 -t, --threads &lt;N> Number of threads to use 使用多少个线程进行压测 -s, --script &lt;S> Load Lua script file 指定Lua脚本路径 -H, --header &lt;H> Add header to request 为每一个HTTP请求添加HTTP头 --latency Print latency statistics 在压测结束后打印延迟统计信息 --timeout &lt;T> Socket/request timeout 超时时间 -v, --version Print version details 打印wrk的详细版本信息 Numeric arguments may include a SI unit (1k, 1M, 1G) 代表数字参数支持国际单位 Time arguments may include a time unit (2s, 2m, 2h) 代表时间参数支持时间单位 关于线程数：并不是设置得越大，压测效果越好，线程设置过大，反而会导致线程切换过于频繁，效果降低，一般来说，推荐设置成压测机器 CPU 核心数的 2 倍到 4 倍 wrk 测试wrk -t2 -c10 -d20s --latency http://10.244.169.184 Running 20s test @ http://10.244.169.184 2 threads and 10 connections Thread Stats Avg(平均值) Stdev(标准差) Max(最大值) +/- Stdev(正负一个标准差所占比例) Latency(延迟) 7.08ms 9.11ms 85.30ms 88.17% Req/Sec(每秒请求数) 1.08k 524.84 2.34k 59.13% Latency Distribution (延迟分布) 50% 3.34ms(50%的请求在3.34ms效应) 75% 7.07ms 90% 18.22ms 99% 45.30ms 42574 requests in 20.10s, 34.51MB read Requests/sec: 2117.96(QPS 即平均每秒处理请求数为 2117.96) Transfer/sec: 1.72MB(平均每秒流量) 这仅仅是一个 get 请求的测试。如果想进行 POST 请求，或者每一次请求的参数都不一样，用来模拟用户使用的实际场景，可以使用 Lua 脚本来进行一系列更复杂的测试","categories":[{"name":"性能测试","slug":"性能测试","permalink":"https://www.missf.top/categories/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"}],"tags":[{"name":"性能测试","slug":"性能测试","permalink":"https://www.missf.top/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"name":"Wrk","slug":"Wrk","permalink":"https://www.missf.top/tags/Wrk/"}]},{"title":"Kubeadm 快速搭建 k8s 集群","slug":"Kubeadm 快速搭建 k8s 集群","date":"2020-11-20T07:47:02.000Z","updated":"2021-03-18T02:27:50.000Z","comments":true,"path":"post/50806d3a.html","link":"","permalink":"https://www.missf.top/post/50806d3a.html","excerpt":"","text":"Kubeadm 简介kubeadm 是一个提供了 kubeadm init 和 kubeadm join 的工具，作为创建 Kubernetes 集群的 “快捷途径” 的最佳实践 kubeadm 通过执行必要的操作来启动和运行最小可用集群。按照设计，它只关注启动引导，而非配置机器。同样的，安装各种 “锦上添花” 的扩展，例如 Kubernetes Dashboard，监控方案，以及特定云平台的扩展，都不在讨论范围内 k8s 官方社区推出 Kubeadm 工具的原因，就是为了让 kubernetes 集群的部署变得快速简单 安装要求一台或多台机器，操作系统(CentOS 7、Ubuntu 16.04+、Red Hat Enterprise Linux (RHEL) 7 等) 每台机器 2 GB 或更多的 RAM 2 核 CPU 或更多 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以) 节点之中不可以有重复的主机名、MAC 地址或 product_uuid 禁用 swap 分区 准备环境节点规划 k8s-master 10.10.110.190 k8s-ndoe1 10.10.110.191 k8s-node2 10.10.110.192 架构图 配置节点 # 关闭防火墙 systemctl stop firewalld.service systemctl disable firewalld.service # 关闭seliinux sed -i 's#^SELINUX=enforcing#SELINUX=disabled#' /etc/selinux/config # 关闭swap分区 swapoff -a sed -i 's!^/dev/mapper/centos-swap!#&amp;!' /etc/fstab # 设置主机名 hostnamectl set-hostname [hostname] # 配置hosts解析 cat >> /etc/hosts &lt;&lt; EOF 10.10.110.190 k8s-master 10.10.110.191 k8s-node1 10.10.110.192 k8s-node2 EOF # 避免 iptables 被绕过而导致流量无法正确路由 cat > /etc/sysctl.d/k8s.conf &lt;&lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness = 0 EOF sysctl --system # 确保在此步骤之前已加载了 br_netfilter 模块,这可以通过运行 lsmod | grep br_netfilter 来查看,需要加载请执行 modprobe br_netfilter # 时间同步 yum install ntpdate -y ntpdate cn.pool.ntp.org 安装 docker kubeadm kubelet kubectl在所有节点安装 docker kubeadm kubelet，kubernetes 默认的 CRI 是 docker，因此先安装 docker 。kubeadm 是用来初始化集群的指令，kubelet 负责在集群中的每个节点上用来启动 pod 和容器，kubectl 是用来与集群通信的命令行工具。安装时需要确保kubeadm、kubelet、kubectl 和 kubeadm 安装的版本相匹配，否则存在发生版本偏差的风险 安装 docker # 安装依赖包 yum install -y yum-utils device-mapper-persistent-data lvm2 # 添加 Docker 阿里云镜像源 wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo # 安装 Docker CE yum install -y docker-ce # 启动 Docker 服务并设置开机启动 systemctl start docker systemctl enable docker # 配置阿里云镜像服务加速地址 tee /etc/docker/daemon.json &lt;&lt; EOF { \"registry-mirrors\": [\"https://265wemgl.mirror.aliyuncs.com\"] } EOF systemctl daemon-reload systemctl restart docker.service 安装 kubeadm kubelet kubectl # 添加 kubernetes 阿里云镜像源 cat > /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 指定版本安装 yum install -y kubelet-1.19.0 kubeadm-1.19.0 kubectl-1.19.0 systemctl enable kubelet 初始化 kubernetes master 节点在 master 节点执行 kubeadm init \\ --apiserver-advertise-address=10.10.110.190 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.19.0 \\ --service-cidr=10.10.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 \\ --ignore-preflight-errors=all –apiserver-advertise-address 服务器所公布的其正在监听的 IP 地址 –image-repository 默认拉取镜像地址为 k8s.gcr.io ，这里指定阿里云镜像仓库地址 –kubernetes-version 指定 k8s 安装版本 –service-cidr 集群内部虚拟网络，Pod 统一访问入口 –pod-network-cidr 指明 pod 网络可以使用的 IP 地址段 拷贝 kubectl 连接 k8s 所使用的认证文件到当前用户的默认路径 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 设置 k8s 命令自动补全 yum install -y epel-release bash-completion source /usr/share/bash-completion/bash_completion source &lt;(kubectl completion bash) echo \"source &lt;(kubectl completion bash)\" >> ~/.bashrc kubernetes 集群添加节点在 10.10.110.191/192 节点上执行，所有 node 节点都需要执行，执行 kubeadm init 输出的 kubeadm join 命令 kubeadm join 10.10.110.190:6443 --token j3rqzp.ku4k3ift3i6z1hnb \\ --discovery-token-ca-cert-hash sha256:c9460068e3b3bd4d3b111dd1581f50eaada11c2e7a47c958b3a7d158c6ae2170 这时候我们在 master 执行 kubectl get nodes ，发现 node 的状态其实是 NotReady 的 安装 Pod 网络插件我们必须部署一个基于容器网络接口 (CNI) 的 Pod 网络插件，这样 Pods 之间才可以相互通信。在安装网络之前，群集 DNS (CoreDNS)不会启动，这也是为什么 node 的状态其实是 NotReady 的原因。k8s 常用的网络插件包括 calico、flannel、Canal 和 Weave，这里我们使用 calico 来为 kubernetes 集群提供网络策略支持 部署 calico 网络插件 # 下载 calico 官方配置文件(国内网络可能会下载失败) wget https://docs.projectcalico.org/manifests/calico.yaml # 修改 calico 配置文件 - name: CALICO_IPV4POOL_CIDR value: \"10.244.0.0/16\" # 这个默认网段是192.168.0.0/16,修改成前面 kubeadm init 指定的 --pod-network-cidr 一样 # 应用配置文件 kubectl apply -f calico.yaml # 查看 pods 运行状态 kubectl get pods -n kube-system 测试 kubernetes 集群我们在搭建好 kubernetes 集群后，创建一个 pod 验证集群是否正常运行 kubectl create deployment nginx-deploy --image=nginx kubectl expose deployment nginx-deploy --port=8080 --type=NodePort --target-port=80 kubectl get pod,svc -o wide NodePort 类型的 service 会从 30000-32767 范围随机分配一个端口 --port 指的是 service CLUSTER-IP 的端口 --target-port 指的是 pod 的端口 [root@k8s-master ~]# kubectl get pod,svc -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-deploy-8588f9dfb-h28ck 1/1 Running 0 10m 10.244.169.135 k8s-node2 &lt;none> &lt;none> NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.0.0.1 &lt;none> 443/TCP 6d17h &lt;none> service/nginx-deploy NodePort 10.4.54.140 &lt;none> 8080:31234/TCP 14s app=nginx-deploy [root@k8s-master ~]# kubectl get ep NAME ENDPOINTS AGE kubernetes 10.10.110.190:6443 6d17h nginx-deploy 10.244.169.135:80 18s nodeIP:31234 >> 10.4.54.140:8080 >> 10.244.169.135:80 我们使用 NodePort 的方式将 service 暴露出去，如果 service 不指定类型那默认是 ClusterIP，只能集群内部访问 部署 Dashboardkubernetes dashboard 是一个用于 kubernetes 集群的通用、基于 web 的用户界面。它允许用户管理在集群中运行的应用程序并对其进行故障排除，以及管理集群本身 wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml # dashboard 默认的 service 是 ClusterIP 类型,我们需要修改为 NodePort 类型,才能让外部访问到我们的 dashboard --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 nodePort: 30023 selector: k8s-app: kubernetes-dashboard type: NodePort --- kubectl apply -f recommended.yaml dashboard 访问地址: https://nodeIP:30023 # 我们首先在 kubernetes-dashboard 命名空间中创建名为 admin-user 的 service account kubectl create serviceaccount admin-user -n kubernetes-dashboard kubectl get serviceaccounts -n kubernetes-dashboard # 给 admin-user 用户授权 kubectl create clusterrolebinding admin-user --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:admin-user # 获取 kubernetes-dashboard 命名空间下 admin-user 用户的 token kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}') 使用输出的 token 就可以登录到 dashboard 管理页面","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.missf.top/categories/k8s/"},{"name":"Kubeadm","slug":"k8s/Kubeadm","permalink":"https://www.missf.top/categories/k8s/Kubeadm/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.missf.top/tags/k8s/"},{"name":"Kubeadm","slug":"Kubeadm","permalink":"https://www.missf.top/tags/Kubeadm/"},{"name":"集群","slug":"集群","permalink":"https://www.missf.top/tags/%E9%9B%86%E7%BE%A4/"}]},{"title":"MariaDB 主从复制和 Maxscale 中间件实现读写分离及故障切换","slug":"MariaDB 主从复制和 Maxscale 中间件实现读写分离及故障切换","date":"2020-09-14T03:20:58.000Z","updated":"2020-10-12T06:57:46.000Z","comments":true,"path":"post/2c9da2f4.html","link":"","permalink":"https://www.missf.top/post/2c9da2f4.html","excerpt":"","text":"环境准备system version: CentOS Linux release 7.7.1908 mariadb version: 10.5.2 maxscale version: 2.5.3 GA client: 10.10.110.84 mariadb master: 10.10.110.80 mariadb slave1: 10.10.110.81 mariadb slave2: 10.10.110.82 maxscale proxy: 10.10.110.83 主从复制原理当 mariadb master 服务器上的数据发生改变时(增、删、改)，则将其改变写入 binlog 二进制日志中。slave 服务器会在一定时间间隔内对 master 二进制日志进行探测其是否发生改变，如果发生改变，则开启一个 I/O 线程请求 master 二进制事件，同时主节点为每个 I/O 线程启动一个 dump 线程，用于向其发送二进制事件，并保存至从库本地的中继日志中，从库将启动 SQL 线程从中继日志中读取二进制日志，在本地回放，使得从库数据和主库的数据保持一致，最后 IO 线程和 SQL 线程将进入睡眠状态，等待下一次被唤醒 主从复制的前提条件 master 一定要开启 binlog 二进制日志，并且授予 slave 远程连接的权限 主从复制至少需要两个 mysql 实例，可以分布在不同服务器，也可以在同一台服务器 master 实例和 slave 实例的 mysql 版本最好相同(如果不同，那么 master 实例版本需要低于 slave 实例) master 实例和 slave 实例之间时间同步 配置 MariaDB Master 节点# 安装MariaDB tee /etc/yum.repos.d/Mariadb.repo &lt;&lt; EOF # mariadb 10.5 CentOS repository list - created 2020-09-14 10:57 UTC # http://downloads.mariadb.org/mariadb/repositories/ [mariadb] name=mariadb baseurl=https://mirrors.aliyun.com/mariadb/yum/10.5/centos7-amd64/ gpgkey=https://mirrors.aliyun.com/mariadb/yum/RPM-GPG-KEY-MariaDB gpgcheck=1 EOF yum clean all &amp;&amp; yum makecache yum install -y MariaDB-server MariaDB-client # 配置文件 cat /etc/my.cnf.d/server.cnf ... [mysqld] log-bin=mariadb-bin server_id=180 port=53306 ... # 启动mariadb systemctl start mariadb # 添加用户slave授予远程连接的权限,供从节点复制binlog GRANT REPLICATION SLAVE ON *.* TO 'slave'@'10.10.110.%' IDENTIFIED BY '123456'; # 查看主库的binlog记录日志信息偏移量position mariadb [(none)]> show master status; +--------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +--------------------+----------+--------------+------------------+ | mariadb-bin.000001 | 529 | | | +--------------------+----------+--------------+------------------+ 1 row in set (0.000 sec) 配置 MariaDB Slave 节点mariadb 10.0.2开始，GTID 是默认打开的。因为 maxscale 故障切换功能需要 mariadb 开启基于 GTID 的主从复制，而且是以 master_use_gtid=current_pos 的方式。因为 master_use_gtid=slave_pos 的方式，master 将从最后一个 GTID 开始给 slave 复制 binlog，通过 @@gtid_slave_pos 这个变量来查看目前最后一个 GTID 的位置，如果A节点 (master) 故障了之后由B节点成为了 master，当 A 节点以 slave 的身份加入到集群时，由于 A 节点之前从未成为过 slave，那么 A 节点的 @@gtid_slave_pos 是空的。为了能让故障的 master 节点故障之后能够重新以 slave 的身份加入到集群，我们需要使用 master_use_gtid=current_pos 的 GTID 方式开启主从复制 # 指定主库信息(master信息会存到/var/lib/mysql/master.info文件) mariadb [(none)]> change master to master_host='10.10.110.80', -> master_user='slave', -> master_password='123456', -> master_port=53306, -> master_use_gtid=current_pos, -> master_connect_retry=30; Query OK, 0 rows affected (0.017 sec) # 启动slave线程,若要更改指定的主库信息,需先执行stop slave,修改完成后执行start slave mariadb [(none)]> start slave; Query OK, 0 rows affected (0.003 sec) # 查看slave状态(slave_IO_Running和slave_SQL_Running都为Yes状态) mariadb [(none)]> show slave status\\G *************************** 1. row *************************** slave_IO_State: Waiting for master to send event master_Host: 10.10.110.80 master_User: slave master_Port: 53306 Connect_Retry: 30 master_Log_File: mariadb-bin.000001 Read_master_Log_Pos: 529 Relay_Log_File: localhost-relay-bin.000002 Relay_Log_Pos: 830 Relay_master_Log_File: mariadb-bin.000001 slave_IO_Running: Yes slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_master_Log_Pos: 529 Relay_Log_Space: 1143 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 master_SSL_Allowed: No master_SSL_CA_File: master_SSL_CA_Path: master_SSL_Cert: master_SSL_Cipher: master_SSL_Key: Seconds_Behind_master: 0 master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: master_Server_Id: 80 master_SSL_Crl: master_SSL_Crlpath: Using_Gtid: Current_Pos Gtid_IO_Pos: 0-80-1 Replicate_Do_Domain_Ids: Replicate_Ignore_Domain_Ids: Parallel_Mode: optimistic SQL_Delay: 0 SQL_Remaining_Delay: NULL slave_SQL_Running_State: slave has read all relay log; waiting for more updates slave_DDL_Groups: 1 slave_Non_Transactional_Groups: 0 slave_Transactional_Groups: 0 1 row in set (0.001 sec) 验证主从复制在 mariadb master 上创建测试数据，然后在 mariadb slave 上查看数据是否已经同步过来 create database mariadb; use mariadb; create table mariadb(name varchar(25),city varchar(30),age int); insert into mariadb.mariadb values(\"mariadb\",\"china\",11); MariaDB 开启并行复制mariadb的复制通过三步完成: 1.从库的IO线程去主库上读取binlog日志变更，并把读取的事件按顺序存放到relay log 2.从库的SQL线程一次读取relay log中的一个事件 3.SQL线程依次执行relay log中的事件 mariadb 10之前的版本中，第三步是通过SQL线程来执行的，这意味着一次只能执行一个事件，复制本质上是单线程的。mariadb 10之后的版本中，第三步可以由一个单独的复制工作线程池执行，从而通过并行应用多个事件来提高复制性能 cat /etc/my.cnf.d/server.cnf ... [mysqld] slave-parallel-threads=8 # 在工作线程池中创建8个线程 ... 查看工作线程的数量 MariaDB [(none)]> SHOW PROCESSLIST; +----+-------------+-----------+------+--------------+------+--------------------------------------------------------+------------------+----------+ | Id | User | Host | db | Command | Time | State | Info | Progress | +----+-------------+-----------+------+--------------+------+--------------------------------------------------------+------------------+----------+ | 5 | system user | | NULL | Slave_IO | 27 | Waiting for master to send event | NULL | 0.000 | | 7 | system user | | NULL | Slave_worker | 27 | Waiting for work from SQL thread | NULL | 0.000 | | 8 | system user | | NULL | Slave_worker | 27 | Waiting for work from SQL thread | NULL | 0.000 | | 9 | system user | | NULL | Slave_worker | 27 | Waiting for work from SQL thread | NULL | 0.000 | | 10 | system user | | NULL | Slave_worker | 27 | Waiting for work from SQL thread | NULL | 0.000 | | 11 | system user | | NULL | Slave_worker | 27 | Waiting for work from SQL thread | NULL | 0.000 | | 12 | system user | | NULL | Slave_worker | 27 | Waiting for work from SQL thread | NULL | 0.000 | | 13 | system user | | NULL | Slave_worker | 27 | Waiting for work from SQL thread | NULL | 0.000 | | 14 | system user | | NULL | Slave_worker | 27 | Waiting for work from SQL thread | NULL | 0.000 | | 6 | system user | | NULL | Slave_SQL | 27 | Slave has read all relay log; waiting for more updates | NULL | 0.000 | | 15 | root | localhost | NULL | Query | 0 | starting | SHOW PROCESSLIST | 0.000 | +----+-------------+-----------+------+--------------+------+--------------------------------------------------------+------------------+----------+ 11 rows in set (0.000 sec) 判断主从复制延迟Seconds_Behind_Master 是通过 sql_thread 执行的事件的时间戳和 io_thread 复制好的事件的时间戳进行比较，而得到的一个差值 NULL：表示 io_thread 或是 sql_thread 有任何一个发生故障，也就是该线程的 Running 状态是 No 而非 Yes 0：该值为零，是我们极为渴望看到的情况，表示主从复制良好，可以认为 lag 不存在 正值：表示主从已经出现延时，数字越大表示从库落后主库越多 负值：几乎很少见，其实这是一个 BUG 值，该参数是不应该出现负值的 仅仅依靠 Seconds_Behind_Master 的值来监测主从同步数据是否延迟是绝对不可靠的，如果网络存在延迟，即使我们看到 Seconds_Behind_Master 的值为零，也会存在很大的偏差。更加严谨的判断主从延时的做法是： 对 master 和 slave 同时发起 SHOW BINARY LOGS 请求，判断二者 binlog 的差异 对 slave 发起 SHOW slave STATUS\\G 的请求，查看 Read_Master_Log_Pos 和 Exec_Master_Log_Pos 是否一致 读写分离和故障切换的实现数据写入操作在主库执行，数据读取操作在从库执行，在一定程度上减轻了数据库的压力，主从复制保证了数据的安全 Maxscale 概述maxscale 是由 mariadb 团队开发的一个数据库代理工具，将数据库语句转发到一个或多个数据库服务器，转发是使用基于对数据库语句的语义理解和后端数据库集群中服务器角色的规则来执行的，设计目的是为应用程序提供透明的负载平衡和高可用性功能。mariadb maxscale 具有可扩展和灵活的架构，通过插件组件支持不同的协议和路由方法 安装 Maxscalewget https://downloads.mariadb.com/MaxScale/2.5.3/rhel/7/x86_64/maxscale-2.5.3-2.rhel.7.x86_64.rpm yum -y install maxscale-2.5.3-2.rhel.7.x86_64.rpm MariaDB Master 主库上创建相关的账户在开始配置之前，需要在 mariadb master 中为 maxscale 创建两个用户，用于 maxscale 的监控模块和路由模块 monitor_user：该账号监控集群状态，如果发现某个从服务器复制线程停掉了，那么就不向其转发请求了 # 创建监控用户,用于[MariaDB-Monitor]配置 CREATE USER 'monitor_user'@'%' IDENTIFIED BY '123456'; GRANT REPLICATION CLIENT ON *.* TO 'monitor_user'@'%'; # 如果使用 MariaDB Monitor 的自动故障转移，用户将需要额外的授权 GRANT SUPER, RELOAD ON *.* TO 'monitor_user'@'%'; routing_user：该账号将不同的请求分发到不同的节点上，当客户端连接到 maxscale 这个节点上时，maxscale 节点会使用该账号去查后端数据库，检查客户端登陆的用户是否有权限或密码是否正确等等 # 创建routing user,用于[Read-Write-Service]配置 CREATE USER 'routing_user'@'%' IDENTIFIED BY '123456'; GRANT SELECT ON mysql.user TO 'routing_user'@'%'; GRANT SELECT ON mysql.db TO 'routing_user'@'%'; GRANT SELECT ON mysql.tables_priv TO 'routing_user'@'%'; GRANT SELECT ON mysql.columns_priv TO 'routing_user'@'%'; GRANT SELECT ON mysql.proxies_priv TO 'routing_user'@'%'; GRANT SELECT ON mysql.roles_mapping TO 'routing_user'@'%'; GRANT SHOW DATABASES ON *.* TO 'routing_user'@'%'; 配置加密密码我们创建的数据库用户信息需要填写到 maxscale 配置文件中，为了防止配置文件出现明文密码，我们可以使用秘钥为密码加密，然后将加密后的字符串填写在 maxscale 配置文件中 # 生成秘钥,密钥将保存到/var/lib/maxscale/.secrets maxkeys # 基于秘钥生成123456加密后的字符串(记录下来) maxpasswd /var/lib/maxscale/ 123456 Maxscale 配置文件[root@localhost ~]# grep -v \"^#\" /etc/maxscale.cnf [maxscale] threads=auto admin_host=0.0.0.0 admin_port=8080 admin_secure_gui=false # 关闭GUI安全验证,不然需要配置ssl [server1] # 不需要指定哪个是master和slave,maxscale会自动识别 type=server address=10.10.110.80 port=53306 protocol=MariaDBBackend [server2] type=server address=10.10.110.81 port=53306 protocol=MariaDBBackend [server3] type=server address=10.10.110.82 port=53306 protocol=MariaDBBackend [MariaDB-Monitor] type=monitor module=mariadbmon servers=server1,server2,server3 user=monitor_user password=EA25B20FBB2B3EF4562F9D585DE8826B64B328C08571D8F656424252F9560A62 monitor_interval=2000 [Read-Write-Service] type=service router=readwritesplit # 配置读写分离的路由 servers=server1,server2,server3 user=routing_user password=EA25B20FBB2B3EF4562F9D585DE8826B64B328C08571D8F656424252F9560A62 [Read-Write-Listener] type=listener service=Read-Write-Service # 监听读写分离的服务 protocol=MariaDBClient port=4006 # maxscale代理的端口 启动 Maxscale 服务systemctl start maxscale.service Maxctrl 管理工具的使用maxctrl 如果不指定 COMMAND 将会进入交互式模式，在交互式中可以直接输入 COMMAND，以表格格式显示基础信息 # 显示所有后端服务器 maxctrl -h 10.10.110.83:8080 list servers ┌─────────┬──────────────┬───────┬─────────────┬─────────────────┬─────────┐ │ Server │ Address │ Port │ Connections │ State │ GTID │ ├─────────┼──────────────┼───────┼─────────────┼─────────────────┼─────────┤ │ server1 │ 10.10.110.80 │ 53306 │ 0 │ Master, Running │ 0-80-15 │ ├─────────┼──────────────┼───────┼─────────────┼─────────────────┼─────────┤ │ server2 │ 10.10.110.81 │ 53306 │ 0 │ Slave, Running │ 0-80-15 │ ├─────────┼──────────────┼───────┼─────────────┼─────────────────┼─────────┤ │ server3 │ 10.10.110.82 │ 53306 │ 0 │ Slave, Running │ 0-80-15 │ └─────────┴──────────────┴───────┴─────────────┴─────────────────┴─────────┘ list services # 显示所有服务，例如读写分离服务等 list monitors # 显示所有监控信息 list listeners # 显示监听信息 # 更多的命令可以去官网自行了解... 登录 Maxscale 仪表板默认情况下用户名是 admin，密码是 mariadb 仪表板界面可以看到以下资源的概览信息：按 monitor 分组的所有服务器，当前会话和所有服务。这些资源的信息每10秒更新一次。监视器、服务器和服务资源都有自己的详细信息页面。可以通过单击仪表板页面上的资源名称来访问它，在详细信息页中，可以修改部分资源参数的值 Maxscale 测试读写分离验证读写分离的 “读” 操作是否在从库上 # 分别在两个从库上创建数据 create database slave; use slave; create table info(name varchar(25),ip int); insert into slave.info values(\"slave\",inet_aton('10.10.110.81')); create database slave; use slave; create table info(name varchar(25),ip int); insert into slave.info values(\"slave\",inet_aton('10.10.110.82')); # 在主库上创建测试用户 grant all on *.* to 'check'@'%' identified by '123456'; # 连接maxscale查询数据验证读写分离(读操作自动负载均衡) mysql -ucheck -p123456 -P4006 -h 10.10.110.83 MariaDB [(none)]> select name,inet_ntoa(ip) from slave.info; +-------+---------------+ | name | inet_ntoa(ip) | +-------+---------------+ | slave | 10.10.110.82 | +-------+---------------+ 1 row in set (0.002 sec) MariaDB [(none)]> select name,inet_ntoa(ip) from slave.info; +-------+---------------+ | name | inet_ntoa(ip) | +-------+---------------+ | slave | 10.10.110.81 | +-------+---------------+ 1 row in set (0.001 sec) 验证读写分离的 “写” 操作是否在主库上 # 连接maxscale往里写数据,看slave上数据有没有同步过来 create database test; use test; create table test(name varchar(25),city varchar(30),age int); insert into test.test values(\"mariadb\",\"china\",11); Maxscale 配置故障切换编辑 maxscale 的配置文件，配置故障切换参数 [root@localhost ~]# grep -v \"^#\" /etc/maxscale.cnf [maxscale] threads=auto admin_host=0.0.0.0 admin_port=8080 admin_secure_gui=false [server1] type=server address=10.10.110.80 port=53306 protocol=MariaDBBackend [server2] type=server address=10.10.110.81 port=53306 protocol=MariaDBBackend [server3] type=server address=10.10.110.82 port=53306 protocol=MariaDBBackend [MariaDB-Monitor] type=monitor module=mariadbmon servers=server1,server2,server3 user=monitor_user password=EA25B20FBB2B3EF4562F9D585DE8826B64B328C08571D8F656424252F9560A62 monitor_interval=2000 replication_user=slave # 复制用户 replication_password=EA25B20FBB2B3EF4562F9D585DE8826B64B328C08571D8F656424252F9560A62 auto_failover=true # 开启自动故障转移 auto_rejoin=true # 自动重新连接 failcount=3 # 故障次数 failover_timeout=90 # 故障转移超时 switchover_timeout=90 # 故障切换超时 verify_master_failure=true # 自动故障转移启用额外的主故障验证 master_failure_timeout=10 # [Read-Write-Service] type=service router=readwritesplit servers=server1,server2,server3 user=routing_user password=EA25B20FBB2B3EF4562F9D585DE8826B64B328C08571D8F656424252F9560A62 [Read-Write-Listener] type=listener service=Read-Write-Service protocol=MariaDBClient port=4006 手动关闭 master 节点，使用 maxctrl 查看 master 状态，可以看到 master 节点会自动切换。当 master节点修复之后会自动重新加入集群 [root@localhost ~]# maxctrl -h 10.10.110.83:8080 list servers ┌─────────┬──────────────┬───────┬─────────────┬────────────────┬─────────┐ │ Server │ Address │ Port │ Connections │ State │ GTID │ ├─────────┼──────────────┼───────┼─────────────┼────────────────┼─────────┤ │ server1 │ 10.10.110.80 │ 53306 │ 0 │ Down │ 0-80-26 │ ├─────────┼──────────────┼───────┼─────────────┼────────────────┼─────────┤ │ server2 │ 10.10.110.81 │ 53306 │ 0 │ Slave, Running │ 0-81-27 │ ├─────────┼──────────────┼───────┼─────────────┼────────────────┼─────────┤ │ server3 │ 10.10.110.82 │ 53306 │ 0 │ Slave, Running │ 0-82-27 │ └─────────┴──────────────┴───────┴─────────────┴────────────────┴─────────┘ [root@localhost ~]# maxctrl -h 10.10.110.83:8080 list servers ┌─────────┬──────────────┬───────┬─────────────┬─────────────────┬─────────┐ │ Server │ Address │ Port │ Connections │ State │ GTID │ ├─────────┼──────────────┼───────┼─────────────┼─────────────────┼─────────┤ │ server1 │ 10.10.110.80 │ 53306 │ 0 │ Down │ 0-80-26 │ ├─────────┼──────────────┼───────┼─────────────┼─────────────────┼─────────┤ │ server2 │ 10.10.110.81 │ 53306 │ 0 │ Master, Running │ 0-81-27 │ ├─────────┼──────────────┼───────┼─────────────┼─────────────────┼─────────┤ │ server3 │ 10.10.110.82 │ 53306 │ 0 │ Slave, Running │ 0-82-27 │ └─────────┴──────────────┴───────┴─────────────┴─────────────────┴─────────┘","categories":[{"name":"MariaDB","slug":"MariaDB","permalink":"https://www.missf.top/categories/MariaDB/"},{"name":"Maxscale","slug":"MariaDB/Maxscale","permalink":"https://www.missf.top/categories/MariaDB/Maxscale/"}],"tags":[{"name":"MariaDB","slug":"MariaDB","permalink":"https://www.missf.top/tags/MariaDB/"},{"name":"数据库","slug":"数据库","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"读写分离","slug":"读写分离","permalink":"https://www.missf.top/tags/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/"}]},{"title":"Elastic 收集 Java 日志","slug":"Elastic 收集 Java 日志","date":"2020-09-08T10:20:14.000Z","updated":"2021-01-20T10:13:36.000Z","comments":true,"path":"post/5a1ae96.html","link":"","permalink":"https://www.missf.top/post/5a1ae96.html","excerpt":"","text":"安装 TomcatTomcat 属于 Java 应用，这里收集 Tomcat 日志作为示例 # 下载软件包 wget -P /server/tools/https://mirror.bit.edu.cn/apache/tomcat/tomcat-8/v8.5.53/bin/apache-tomcat-8.5.53.tar.gz # 解压 tar xf apache-tomcat-8.5.53.tar.gz -C /usr/local/ &amp;&amp; mv /usr/local/apache-tomcat-8.5.53/ /usr/local/tomcat # 启动 tomcat /usr/local/tomcat/bin/startup.sh 编写 Filebeat pipelinefilebeat 获取所有不以 “[“ 开头的行，并将它们合并到上一行以 “[“ 开头的行之后 filebeat.inputs: - type: log enabled: true paths: - /usr/local/tomcat/logs/catalina.out tags: [\"catalina\"] fields: server: tomcat type: tomcat-catalina fields_under_root: true multiline: pattern: '^\\[' negate: true match: after #output.console: output.redis: hosts: [\"10.10.110.194:56379\"] password: \"123456\" key: \"tomcat\" db: 0 datatype: list 模拟 Tomcat 报错日志往 Tomcat 的日志写入错误信息，模拟报错信息 cat > /usr/local/tomcat/logs/catalina.out &lt;&lt; EOF Sep 09, 2020 5:50:33 PM org.apache.catalina.startup.Catalina stopServer SEVERE: Catalina.stop: org.xml.sax.SAXParseException; systemId: file:/usr/local/tomcat/conf/server.xml; lineNumber: 22; columnNumber: 45; Attribute name \"dda\" associated with an element type \"Server\" must be followed by the ' = ' character. at java.xml/com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1243) at java.xml/com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:635) at org.apache.tomcat.util.digester.Digester.parse(Digester.java:1495) at org.apache.catalina.startup.Catalina.stopServer(Catalina.java:485) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.catalina.startup.Bootstrap.stopServer(Bootstrap.java:389) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:479) EOF 编写 Logstash pipelineinput { redis { host => \"10.10.110.194\" port => 56379 password => \"123456\" db => \"0\" data_type => \"list\" key => \"tomcat\" } } output { if [type] == \"tomcat-catalina\" { if [tags][0] == \"catalina\" { elasticsearch { hosts => [\"http://10.10.110.191:9200\",\"http://10.10.110.192:9200\",\"http://10.10.110.193:9200\"] index => \"filebeat-tomcat-%{+YYYY.MM.dd}\" } stdout { codec=> rubydebug } } } } Kibana 展示数据这里展示数据是不显示完全的，我们可以指定字段查看更详细的信息 指定 message 字段，查看被合并成一行的 Tomcat 报错日志","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"}]},{"title":"Elastic 收集 Nginx 日志","slug":"Elastic 收集 Nginx 日志","date":"2020-08-25T06:27:22.000Z","updated":"2020-09-28T03:33:54.000Z","comments":true,"path":"post/baa98a96.html","link":"","permalink":"https://www.missf.top/post/baa98a96.html","excerpt":"","text":"Nginx 配置 Json 格式日志修改 Nginx 配置文件，定义输出 json 格式的日志，便于 filebeat 和 logstash 收集 http { log_format main '{\"@timestamp\": \"$time_iso8601\", ' '\"clientRealIp\": \"$remote_addr\", ' '\"scheme\": \"$scheme\", ' '\"method\": \"$request_method\", ' '\"host\": \"$host\", ' '\"url\": \"$request_uri\", ' '\"size\": $body_bytes_sent, ' '\"referrer\": \"$http_referer\", ' '\"agent\": \"$http_user_agent\", ' '\"upstream_addr\": \"$upstream_addr\", ' '\"request_time\": $request_time, ' '\"request_length\": $request_length, ' '\"upstream_connect_time\": \"$upstream_connect_time\", ' '\"upstream_response_time\": \"$upstream_response_time\", ' '\"upstream_status\": \"$upstream_status\", ' '\"status\": \"$status\"}'; } Filebeat 配置文件编写 filebeat 配置文件，收集 Nginx 的 access.log 和 error.log，并且添加自定义字段和标签存储到 redis cat /etc/filebeat/filebeat-nginx.yml filebeat.inputs: - type: log enabled: true json.keys_under_root: true paths: - /usr/local/nginx/logs/access.log tags: [\"access\"] fields: server: nginx type: nginx-access fields_under_root: true - type: log enabled: true json.keys_under_root: true paths: - /usr/local/nginx/logs/error.log tags: [\"error\"] fields: server: nginx type: nginx-error fields_under_root: true processors: - drop_fields: fields: [\"input_type\", \"ecs.version\", \"host.name\", \"agent\", \"log.offset\"] #output.console: output.redis: hosts: [\"10.10.110.194:56379\"] password: \"123456\" key: \"nginx\" db: 0 datatype: list 控制台调试 Filebeat 输出的日志数据通过 drop_fields 去控制我们想要输出的字段，得到精简的日志数据 { \"@timestamp\": \"2020-09-07T18:08:49.000Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.9.0\" }, \"server\": \"nginx\", \"ecs\": {}, \"host\": {}, \"log\": { \"file\": { \"path\": \"/usr/local/nginx/logs/access.log\" } }, \"json\": {}, \"input\": { \"type\": \"log\" }, \"type\": \"nginx-access\", \"message\": \"10.10.110.194 - - [08/Sep/2020:02:08:41 +0800] \\\"GET /848dd HTTP/1.1\\\" 404 153 \\\"-\\\" \\\"curl/7.29.0\\\"\", \"tags\": [\"access\"] } Logstash 读取 Redis 中的日志数据logstash 读取 redis 中的日志数据，并且在 Kibana展示 Nginx 日志 # logstash配置文件通过我们定义的fields字段和标签匹配数据,将不同的数据存储到不同的index input { redis { host => \"10.10.110.194\" port => 56379 password => \"123456\" db => \"0\" data_type => \"list\" key => \"nginx\" } } output { # 通过字段和标签判断日志数据,存储到不同的index if [type] == \"nginx-access\" { if [tags][0] == \"access\" { elasticsearch { hosts => [\"http://10.10.110.191:9200\",\"http://10.10.110.192:9200\",\"http://10.10.110.193:9200\"] index => \"filebeat-nginx-access%{+YYYY.MM.dd}\" } stdout { codec=> rubydebug } } } if [type] == \"nginx-error\" { if [tags][0] == \"error\" { elasticsearch { hosts => [\"http://10.10.110.191:9200\",\"http://10.10.110.192:9200\",\"http://10.10.110.193:9200\"] index => \"filebeat-nginx-error%{+YYYY.MM.dd}\" } stdout { codec=> rubydebug } } } } Kibana 展示 Nginx 日志我们可以在 kibana 上创建索引，查看 Nginx 日志，通过字段去统计和展示日志数据","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"}]},{"title":"Elastic 引入 Filebeat","slug":"Elastic 引入 Filebeat","date":"2020-08-20T03:58:57.000Z","updated":"2021-01-20T10:13:24.000Z","comments":true,"path":"post/9482a90c.html","link":"","permalink":"https://www.missf.top/post/9482a90c.html","excerpt":"","text":"引入 Filebeat 架构简介filebeat 代替 logstash 去收集日志数据，然后将收集到的日志数据存储到 redis 或者 kafka，再由 logstash 去消费数据。filebeat 是非常轻量级单用途的日志采集器，属于Beats 家族。早期的 elk 架构使用 logstash 收集、解析日志，但是 logstash 对内存、CPU、IO等资源消耗比较高(因为 logstash 是使用 java 语言编写的)，后来出现了使用golang 编写的 filebeat 日志收集器，可以不依赖任何环境安装即可使用，同时对资源的占用可以忽略不计，使用 filebeat 替代 logstash 去收集日志是非常好的方案 安装 Filebeat# 下载 filebeat wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.9.0-x86_64.rpm # 安装 yum install -y filebeat-7.9.0-x86_64.rpm 编写 Filebeat 配置文件filebeat 配置文件负责收集日志，然后将数据存到 redis cat /etc/filebeat/filebeat.yml filebeat.inputs: - type: log enabled: true paths: - /var/log/messages tags: [\"messages\",\"syslog\"] #include_lines: ['sometext'] Filebeat仅导出与列表中的正则表达式匹配的行 #exclude_lines: ['^DBG'] Filebeat会删除列表中与正则表达式匹配的所有行 fields: # 可以指定字段向输出添加附加信息 type: system # fields_under_root: true # 如果为true,则自定义字段将作为顶级字段而不是作为fields字段的子字典 - type: log enabled: true paths: - /var/log/audit/audit.log tags: [\"audit\",\"syslog\"] fields: type: system # fields_under_root: true output.console: # 将收集到的日志数据输出到控制台,可以查看fields定义的字段 output.redis: # filebeat将收集到的日志存储到redis hosts: [\"10.10.110.194:56379\"] password: \"123456\" key: \"filebeat\" db: 0 timeout: 5 查看 Filebeat 输出的 Json 数据我们在调试日志格式时使用命令去启动 filebeat，使用 systemctl 的方式去调试会出现很多转义符，不便于查看 /usr/bin/filebeat -c /etc/filebeat/filebeat.yml # 这里需要将控制台输出的数据json格式化 { \"@timestamp\": \"2020-09-07T16:17:42.615Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.9.0\" }, \"ecs\": { \"version\": \"1.5.0\" }, \"host\": { \"name\": \"localhost.localdomain\" }, \"agent\": { \"ephemeral_id\": \"660a2bfb-9a56-43a8-ae93-788060f5d243\", \"id\": \"6a8ff370-52b5-4f89-ad9c-b6feecf938a9\", \"name\": \"localhost.localdomain\", \"type\": \"filebeat\", \"version\": \"7.9.0\", \"hostname\": \"localhost.localdomain\" }, \"log\": { \"offset\": 997322, \"file\": { \"path\": \"/var/log/messages\" } }, \"message\": \"Sep 8 00:01:01 localhost systemd: Started Session 7 of user root.\", \"tags\": [\"messages\", \"syslog\"], \"fields\": { # 这里由于filebeat配置文件中没有开启fields_under_root: true这个选项,所以我们定义的字段会在fields里面 \"type\": \"system\" }, \"input\": { \"type\": \"log\" } } 定义 Filebeat 输出的 Json 数据我们除了可以自己自定义字段，还可以删除一些 filebeat 默认输出的字段，让日志数据更加易于查看 # 定义filebeat配置文件,过滤不需要的json数据 filebeat.inputs: - type: log enabled: true # json.keys_under_root: true 开始json解析,不是json格式的日志不要开启此选项 paths: - /var/log/messages tags: [\"messages\",\"syslog\"] fields: type: system fields_under_root: true - type: log enabled: true paths: - /var/log/audit/audit.log tags: [\"audit\",\"syslog\"] fields: type: system fields_under_root: true processors: - drop_fields: fields: [\"input_type\", \"ecs.version\", \"host.name\", \"agent\", \"log.offset\"] # 将这些字段丢弃 output.console: 查看自定义之后的 json 数据 { \"@timestamp\": \"2020-09-07T17:37:59.500Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.9.0\" }, \"tags\": [\"messages\", \"syslog\"], \"input\": { \"type\": \"log\" }, \"type\": \"system\", # fields_under_root: true 将作为顶级字段 \"ecs\": {}, \"host\": {}, \"log\": { \"file\": { \"path\": \"/var/log/messages\" } }, \"message\": \"Sep 8 01:35:28 localhost systemd-logind: Removed session 4.\" } Logstash 消费 Redis 中的数据filebeat 将日志数据存储到 redis 之后，logstash 从 redis 读取日志数据就是非常简单的事情了 cat /etc/logstash/conf.d/sys-from-redis.conf input { redis { host => \"10.10.110.194\" port => 56379 password => \"123456\" db => \"0\" data_type => \"list\" batch_count => \"1\" key => \"filebeat\" } } filter { } output { if [type] == \"system\" { # 这里的匹配由filebeat输出的json数据格式来定义 if [tags][0] == \"messages\" { elasticsearch { hosts => [\"http://10.10.110.191:9200\",\"http://10.10.110.192:9200\",\"http://10.10.110.193:9200\"] index => \"filebeat-fromredis-messages-%{+YYYY.MM.dd}\" } stdout { codec=> rubydebug } } else if [tags][0] == \"audit\" { elasticsearch { hosts => [\"http://10.10.110.191:9200\",\"http://10.10.110.192:9200\",\"http://10.10.110.193:9200\"] index => \"filebeat-fromredis-audit-%{+YYYY.MM.dd}\" } stdout { codec=> rubydebug } } } } Elasticsearch 查看数据索引的命名根据我们在 logstash 处理数据时的定义格式","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"}]},{"title":"Elastic 引入 Redis","slug":"Elastic 引入 Redis","date":"2020-08-17T10:48:56.000Z","updated":"2021-01-20T10:13:10.000Z","comments":true,"path":"post/23bc2fbc.html","link":"","permalink":"https://www.missf.top/post/23bc2fbc.html","excerpt":"","text":"引入 Redis 架构简介logstash 分为 shipper(负责收集日志数据)和 indexer(负责对日志做过滤存储到ES)两个角色。当日志量达到一个量级之后，我们就不能继续使用 logstash 去收集和处理数据，由于 ES 的 HTTP API 处理能力有限，在日志写入频繁的情况下可能会超时、丢失，所以用队列来做缓冲在两个 logstash 角色之间可以引入 redis 或者 kafka。使用消息队列的方式可减少 ES 压力，队列起到缓冲作用，也可以一定程度保护数据不丢失。同时我们还能将所有收集到的日志统一在 logstash indexer 进行处理 环境准备logstash 10.10.110.195 # logstash shipper 生产数据,将获取到的数据存到 redis logstash + redis 10.10.110.194 # logstash indexer 消费 redis 中的日志数据 生产日志数据编写 logstash pipeline 配置文件，将收集到的日志数据存储到 redis input { file { path => [\"/var/log/messages\"] type => \"syslog\" tags => [\"messages\",\"log\"] start_position => \"beginning\" } file { path => [\"/var/log/audit/audit.log\"] type => \"syslog\" tags => [\"audit\",\"log\"] start_position => \"beginning\" } } filter { } output { redis { host => [\"10.10.110.194:56379\"] password => \"123456\" db => \"0\" data_type => \"list\" key => \"logstash\" } } 启动 logstash 进行收集日志存储到 redis /usr/share/logstash/bin/logstash -rf /etc/logstash/conf.d/syslog-toredis.conf Redis查看日志数据logstash 在收集到日志数据并且添加上标签和类型然后存储到 redis，我们可以返回列表的长度来得知日志数据是否被存储到 redis 消费日志数据编写 logstash pipeline 配置文件，将 redis 中的日志数据存储到 ES input { redis { host => \"10.10.110.194\" port => 56379 password => \"123456\" db => \"0\" data_type => \"list\" key => \"logstash\" } } filter { } output { if [type] == \"syslog\" { if [tags][0] == \"messages\" { elasticsearch { hosts => [\"http://10.10.110.191:9200\",\"http://10.10.110.192:9200\",\"http://10.10.110.193:9200\"] index => \"syslog-fromredis-messages-%{+YYYY.MM.dd}\" } stdout { codec=> rubydebug } } else if [tags][0] == \"audit\" { elasticsearch { hosts => [\"http://10.10.110.191:9200\",\"http://10.10.110.192:9200\",\"http://10.10.110.193:9200\"] index => \"syslog-fromredis-audit-%{+YYYY.MM.dd}\" } stdout { codec=> rubydebug } } } } Redis 查看消费的数据日志数据被消费完之后就代表已经写入到 ES # redis 中的 key 会全部存到 ES 中(日志数据被消费完) 127.0.0.1:56379> llen logstash (integer) 7041 127.0.0.1:56379> llen logstash (integer) 5791 127.0.0.1:56379> llen logstash (integer) 4541 127.0.0.1:56379> llen logstash (integer) 3041 127.0.0.1:56379> llen logstash (integer) 1666 127.0.0.1:56379> llen logstash (integer) 0 127.0.0.1:56379>","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"}]},{"title":"Elastic Kibana 展示系统日志","slug":"Elastic Kibana 展示系统日志","date":"2020-08-16T10:39:44.000Z","updated":"2021-01-20T10:11:06.000Z","comments":true,"path":"post/c802a07c.html","link":"","permalink":"https://www.missf.top/post/c802a07c.html","excerpt":"","text":"编写 logstash pipeline 配置文件定义日志收集、过滤、存储的方式 input { file { path => [\"/var/log/messages\"] type => \"syslog\" tags => [\"messages\",\"log\"] start_position => \"beginning\" # 从文件开头读取 } file { path => [\"/var/log/audit/audit.log\"] type => \"syslog\" tags => [\"audit\",\"log\"] start_position => \"beginning\" } } filter { } output { if [type] == \"syslog\" { if [tags][0] == \"messages\" { elasticsearch { hosts => [\"http://10.10.110.191:9200\",\"http://10.10.110.192:9200\",\"http://10.10.110.193:9200\"] # 存储带ES index => \"syslog-messages-%{+YYYY.MM.dd}\" # index的命名格式 } stdout { codec=> rubydebug } } else if [tags][0] == \"audit\" { elasticsearch { hosts => [\"http://10.10.110.191:9200\",\"http://10.10.110.192:9200\",\"http://10.10.110.193:9200\"] index => \"syslog-audit-%{+YYYY.MM.dd}\" } stdout { codec=> rubydebug } } } } Logstash 收集日志存储到 ES# 启动 logstash,systemctl 启动方式可以指定配置文件 /usr/share/logstash/bin/logstash -rf /etc/logstash/conf.d/syslog.conf # logstash 常用参数 -n 指定logstash实例的名称,默认为当前主机名 -f 指定启动配置文件 -e 指定直接执行的配置文件内容,可以不指定-f参数了 -r 检测配置文件变化,自动重新加载 -t 检查配置的语法是否正确并退出 Elasticsearch 查看数据索引的命名格式按日期去分割 将 ES 的日志索引到 KibanaKibana 的配置文件指定 ES 的地址，使用正则匹配去创建索引 配置时间过滤器字段 Kibana 展示日志数据可以根据日志数据的字段去查看指定的信息","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"}]},{"title":"Elastic Kibana","slug":"Elastic Kibana","date":"2020-08-14T07:56:54.000Z","updated":"2021-01-20T10:10:44.000Z","comments":true,"path":"post/e26112db.html","link":"","permalink":"https://www.missf.top/post/e26112db.html","excerpt":"","text":"Kibana 简述Kibana 是一个针对 Elasticsearch 的开源分析及可视化平台，用来搜索、查看交互存储在 Elasticsearch 索引中的数据。使用 Kibana，可以通过各种图表进行高级数据分析及展示。Kibana 让海量数据更容易理解，它操作简单，基于浏览器的用户界面可以快速创建仪表板(dashboard)实时显示 Elasticsearch 查询动态。设置 Kibana 非常简单，无需编码或者额外的基础架构，就可以完成 Kibana 安装并启动 Elasticsearch 索引监测 Kibana 安装配置# 下载 Kibana wget https://artifacts.elastic.co/downloads/kibana/kibana-7.8.1-x86_64.rpm # 安装 shasum -a 512 kibana-7.8.1-x86_64.rpm rpm --install kibana-7.8.1-x86_64.rpm # 修改 Kibana 配置文件 grep -v \"^#\" /etc/kibana/kibana.yml server.port: 5601 server.host: \"127.0.0.1\" elasticsearch.hosts: [\"http://10.10.110.191:9200\"] i18n.locale: \"zh-CN\" # 启动 Kibana systemctl start kibana.service 配置 Nginx 代理 Kibana配置 Nginx 反向代理实现鉴权 vim /usr/local/nginx/conf/nginx.conf server { listen 9090; server_name localhost; location / { auth_basic \"Restricted Access\"; auth_basic_user_file /usr/local/nginx/conf/passwd.db; # 账号密码文件 proxy_pass http://127.0.0.1:5601; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 配置验证登录账号密码 # 需要安装 httpd-tools 工具,文件里的密码是密文的 htpasswd -c /usr/local/nginx/conf/passwd.db admin # 连续输入两次密码 # 测试本机 kibana 能否连接,如果本机都不能连接,那么 Nginx 代理就没有意义 curl -L -u admin:12345678 http://127.0.0.1:5601 登录 Kibana登录 kibana 的地址 http://10.10.110.194:9090/ Nginx 账号密码 kibana web 页面","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"}]},{"title":"Elastic Logstash","slug":"Elastic Logstash","date":"2020-08-11T07:04:57.000Z","updated":"2020-09-28T03:50:26.000Z","comments":true,"path":"post/fe947601.html","link":"","permalink":"https://www.missf.top/post/fe947601.html","excerpt":"","text":"Logstash 概述logstash 是 elasticsearch 的数据管道，负责对数据源进行处理。工作原理分别是输入、过滤、输出。其中 input(负责从数据源采集数据)和 output(将数据传输到目的地)是必要的，filter(将数据修改为你指定的格式或内容)是非必要的。logstash 是插件式管理模式，在输入、过滤、输出以及编码过程中都可以使用插件进行定制，Logstash 社区有超过 200 种可用插件 Logstash 安装这里使用 yum 是因为二进制安装的 jdk，在 Logstash 启动时会报 could not find java # 安装jdk yum install -y java-11-openjdk java-11-openjdk-devel java-11-openjdk-headless # 下载logstash wget https://artifacts.elastic.co/downloads/logstash/logstash-7.8.1.rpm # 安装logstash yum install -y logstash-7.8.1.rpm # 修改启动分配内存 vim /etc/logstash/jvm.options -Xms512m -Xmx512m # 第一个logstash示例 cd logstash Installation directory bin/logstash -e 'input { stdin { } } output { stdout {} }' 执行结果如下 Logstash 配置详解Logstash 的配置有两个必须元素(input和output)和一个可选元素(filter) input { # 输入 stdin { ... # 标准输入 } } filter { # 过滤 ... # 对数据进行分割、截取等处理 } output { # 输出 stdout { ... # 标准输出 } } 输入 采集各种样式、大小和来源的数据，数据往往以各种各样的形式，或分散或集中地存在于不同的系统中 Logstash支持各种输入选择 ，可以在同一时间从众多常用来源捕捉事件 能够以连续的流式传输方式，轻松地从您的日志、指标、Web 应用、数据存储以及各种AWS服务采集数据 过滤 实时解析和转换数据，Logstash过滤器能够解析各个事件 识别已命名的字段以构建结构，并将它们转换成通用格式，以便更轻松更快速地分析和实现商业价值 输出 Logstash提供众多输出选择，你可以将数据发送到指定的地方，并且能够灵活地解锁众多下游用例 输入插件 Stdin 示例从标准输入读取数据输出到标准输出 input { stdin { } } filter { } output { stdout { codec => rubydebug } } 执行结果如下 mwj { \"host\" => \"localhost.localdomain\", \"@timestamp\" => 2020-08-12T18:36:32.683Z, \"message\" => \"mwj\", \"@version\" => \"1\" } test data { \"host\" => \"localhost.localdomain\", \"@timestamp\" => 2020-08-12T18:36:47.691Z, \"message\" => \"test data\", \"@version\" => \"1\" } 输入插件 File 示例从文件中读取数据，输出到标准输出 input { file { # 调用file这个插件,logstash社区有非常多的插件可以供我们使用 path =>\"/var/log/messages\" # 数据源来自这个文件的内容 tags =>\"messages\" # 打标签 type =>\"syslog\" } } filter { } output { stdout { codec => rubydebug } } 执行结果如下 { \"type\" => \"syslog\", \"@timestamp\" => 2020-08-13T11:41:45.031Z, \"@version\" => \"1\", \"path\" => \"/var/log/messages\", \"host\" => \"localhost.localdomain\", \"tags\" => [ [0] \"messages\" ], \"message\" => \"Aug 13 19:41:44 localhost yum[86466]: Installed: 1:nginx-mod-http-image-filter-1.16.1-1.el7.x86_64\" } { \"type\" => \"syslog\", \"@timestamp\" => 2020-08-13T11:41:45.032Z, \"@version\" => \"1\", \"path\" => \"/var/log/messages\", \"host\" => \"localhost.localdomain\", \"tags\" => [ [0] \"messages\" ], \"message\" => \"Aug 13 19:41:44 localhost yum[86466]: Installed: 1:nginx-all-modules-1.16.1-1.el7.noarch\" } 输入插件 TCP 示例logstash 从本机端口读取数据，其他机器通过 nc 工具发送数据到 logstash 指定的端口 input { tcp { port =>12345 # 监听12345端口 type =>\"nc\" # 通过nc工具使用tcp/udp连接去发送网络数据给logstash } } filter { } output { stdout { codec => rubydebug } } 执行结果如下 { \"@version\" => \"1\", \"type\" => \"nc\", \"@timestamp\" => 2020-08-13T06:28:13.448Z, \"host\" => \"10.10.110.191\", \"port\" => 35228, \"message\" => \"we\" # logstash接收到其他机器nc工具发送过来的信息(nc 10.10.110.194 12345) } { \"@version\" => \"1\", \"type\" => \"nc\", \"@timestamp\" => 2020-08-13T06:28:40.148Z, \"host\" => \"10.10.110.191\", \"port\" => 35228, \"message\" => \"test\" } 编解码插件 Json 示例只有输入 json 格式的数据才会被成功编解码，不是 json 格式的数据 logstash 不处理 input { stdin { codec => json { charset => [\"UTF-8\"] } } } filter { } output { stdout { codec => rubydebug } } 执行结果如下 {\"name\":\"孙七\",\"age\": 24,\"mail\": \"555@qq.com\",\"hobby\":\"听音乐、看电影\"} { \"hobby\" => \"听音乐、看电影\", \"name\" => \"孙七\", \"mail\" => \"555@qq.com\", \"age\" => 24, \"@version\" => \"1\", \"host\" => \"localhost.localdomain\", \"@timestamp\" => 2020-09-01T14:27:24.066Z } 编解码插件 Multline 示例multline 会将不是以字母开头的行合并到上一行(next是合并到下一行)，下面模拟 java 日志报错 input { stdin { codec => multiline { pattern => \"^\\s\" what => \"previous\" } } } filter { } output { stdout { codec => rubydebug } } 执行结果如下 [INFO] 2020-08-13 15:56:53,195 --AsyncResolver-bootstrap-executor-0-- # 模拟java报错信息 at com.curre at org.sprin at org.sprin { \"@timestamp\" => 2020-09-01T14:36:50.642Z, \"tags\" => [ [0] \"multiline\" ], \"@version\" => \"1\", \"message\" => \"[INFO] 2020-08-13 15:56:53,195 --AsyncResolver-bootstrap-executor-0--\\n at com.curre\\n at org.sprin\\n at org.sprin\", \"host\" => \"localhost.localdomain\" } 过滤插件 Json 示例将 json 数据做过滤放在 content 字段里面 input { stdin { } } filter { json { source => \"message\" target => \"content\" } } output { stdout { codec => rubydebug } } 执行结果如下 {\"request\":\"get\", \"status\":\"404\", \"bytes\":\"563\"} # 数据源 { \"host\" => \"localhost.localdomain\", \"@timestamp\" => 2020-08-13T09:28:26.702Z, \"content\" => { \"request\" => \"get\", \"bytes\" => \"563\", \"status\" => \"404\" }, \"message\" => \"{\\\"request\\\":\\\"get\\\", \\\"status\\\":\\\"404\\\", \\\"bytes\\\":\\\"563\\\"}\", \"@version\" => \"1\" } 过滤插件 Kv 示例以 &amp; 和 ? 作为分隔符，得到 key=value 形式的数据 input { stdin { } } filter { kv { field_split => \"&amp;?\" # 以&amp;和?作为分隔符,得到key=value的形式 field_split_pattern => \":+\" # 以一个或者多个:作为分隔符 } } output { stdout { codec => rubydebug } } 执行结果如下 pin=12345~0&amp;d=123&amp;e=foo@bar.com&amp;oq=bo?oi=bo&amp;ss=12345: # 数据源 { \"d\" => \"123\", \"pin\" => \"12345~0\", \"ss\" => \"12345:\", \"oq\" => \"bo\", \"oi\" => \"bo\", \"e\" => \"foo@bar.com\", \"@timestamp\" => 2020-08-13T09:31:41.881Z, \"host\" => \"localhost.localdomain\", \"message\" => \"pin=12345~0&amp;d=123&amp;e=foo@bar.com&amp;oq=bo?oi=bo&amp;ss=12345:\", \"@version\" => \"1\" } 输出插件 ES 示例logstash 将日志输出到 ES 节点，存储到 missf 这个 index 并且以时间去命名 output { elasticsearch { hosts => \"localhost:9200\" index => \"missf-%{+YYYY.MM.dd}\" } }","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"}]},{"title":"Elastic Elasticsearch","slug":"Elastic Elasticsearch","date":"2020-08-05T10:26:09.000Z","updated":"2021-01-20T10:12:54.000Z","comments":true,"path":"post/1abc58c4.html","link":"","permalink":"https://www.missf.top/post/1abc58c4.html","excerpt":"","text":"Elasticsearch 简介Elasticsearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。Elasticsearch 是用 Java 语言开发，并作为 Apache 许可条款下的开放源代码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索、稳定、可靠、快速、使用方便 我们建立一个网站或应用程序，并要添加搜索功能，但是想要完成搜索工作的创建是非常困难的。我们希望搜索解决方案要运行速度快，我们希望能有一个零配置和一个完全免费的搜索模式，我们希望能够简单地使用 JSON 通过 HTTP 来索引数据，我们希望搜索服务器始终可用，我们希望能够从一台开始并扩展到数百台，我们要实时搜索，我们要简单的多用户，我们希望建立一个云的解决方案。因此我们利用 Elasticsearch 来解决所有这些问题及可能出现的更多其它问题 Elasticsearch 集群部署Elasticsearch 的发展是非常快速的，所以在 ES5.0 之前，ELK 的各个版本都不统一，出现了版本号混乱的状态，所以从 5.0 开始，所有 Elastic Stack 中的项目全部统一版本号。目前最新版本是 7.8.1 # 环境准备 ES1 10.10.110.191 ES2 10.10.110.192 ES3 10.10.110.193 # 下载 elasticsearch 和校验文件 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.1-x86_64.rpm wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.1-x86_64.rpm.sha512 # 安装 elasticsearch shasum -a 512 -c elasticsearch-7.8.1-x86_64.rpm.sha512 yum install -y elasticsearch-7.8.1-x86_64.rpm # 修改 jvm 启动参数,根据自己机器决定 vim /etc/elasticsearch/jvm.options -Xms512m # 确保 Xmx 和 Xms 的大小是相同的，其目的是为了能够在 java 垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源 -Xmx512m # 修改进程在 VMAs(虚拟内存区域)创建内存映射最大数量 echo \"vm.max_map_count=655360\" >> /etc/sysctl.conf &amp;&amp; sysctl -p # 修改 elasticsearch 配置文件 grep -v '^#' /etc/elasticsearch/elasticsearch.yml cluster.name: elk-cluster # 集群名称,所有节点一样 node.name: node-1 # 不同节点,分别用 node-1/node-2/node-3... path.data: /var/lib/elasticsearch # 数据目录,如果加入集群失败可以清空数据目录再重启服务 path.logs: /var/log/elasticsearch # 日志目录 network.host: 10.10.110.191 # 不同节点,分别用10.10.110...... http.port: 9200 # 监听端口 discovery.seed_hosts: [\"10.10.110.191\", \"10.10.110.192\", \"10.10.110.193\"] # 集群发现,可以写成 10.10.110.191:9200 cluster.initial_master_nodes: [\"node-1\", \"node-2\", \"node-3\"] # 指定可以成为 master 的节点,此参数只有在初始化集群时生效 # 启动 elasticsearch 服务 systemctl start elasticsearch.service Elasticsearch 集群常用查询查看集群状态 curl -X GET http://10.10.110.191:9200/_cluster/health?pretty # 响应 { \"cluster_name\" : \"elk-cluster\", \"status\" : \"green\", # 集群状态红绿灯,绿:健康,黄:亚健康,红:病态 \"timed_out\" : false, \"number_of_nodes\" : 3, \"number_of_data_nodes\" : 3, \"active_primary_shards\" : 0, \"active_shards\" : 0, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } 查看节点状态 curl -X GET 'http://10.10.110.191:9200/_cat/nodes?v' ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name 10.10.110.191 62 93 0 0.00 0.01 0.05 dilmrt - node-1 10.10.110.193 62 74 0 0.00 0.01 0.05 dilmrt * node-3 # *代表当前节点是master 10.10.110.192 70 75 0 0.00 0.01 0.05 dilmrt - node-2 查询节点所有索引 curl -X GET 'http://10.10.110.191:9200/_cat/indices?v' health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open data njHuT0XvSOa2NHPJM3Aj-g 1 1 3 0 19.5kb 9.7kb 查询一个索引所有数据 curl -X GET 'http://10.10.110.191:9200/data/_search/?pretty' Elasticsearch-head 安装由于 ES 官方并没有为 ES 提供界面管理工具，仅仅是提供了后台的服务。elasticsearch-head 是一个为 ES 开发的一个页面客户端工具，其源码托管于 GitHub，地址为：https://github.com/mobz/elasticsearch-head elasticsearch-head 提供了四种安装方式： 源码安装通过npm run start启动(不推荐) 通过docker安装(推荐) 通过chrome插件安装(推荐) 通过ES的plugin方式安装(不推荐) 通过 Docker 安装 # 拉取镜像 docker pull mobz/elasticsearch-head:5 # 启动容器 docker run -itd --name \"elasticsearch-head\" -p 9100:9100 -v elasticsearch_head:/usr/src/app --restart always mobz/elasticsearch-head:5 # 由于前后端分离开发,所以会存在跨域问题,需要在服务端做 CORS 的配置 vim /etc/elasticsearch/elasticsearch.yml http.cors.enabled: true http.cors.allow-origin: \"*\" # Web 访问 http://10.10.110.191:9100/ Elasticsearch Head 数据浏览不显示数据，使用浏览器按 F12 查看发现 406 Not Acceptable 错误，出现这个错误是因为后台返回的数据是 json 格式前台无法解析，解决方法如下： # 找到 docker 数据卷在宿主机上的目录 docker volume inspect elasticsearch_head # 修改数据卷目录下 _site/vendor.js 文件 contentType: \"application/x-www-form-urlencoded\" 修改为 contentType: \"application/json;charset=UTF-8\" var inspectData = s.contentType === \"application/x-www-form-urlencoded\" &amp;&amp; 修改为 var inspectData = s.contentType === \"application/json;charset=UTF-8\" &amp;&amp; Elasticsearch 基本概念索引(index)是 Elasticsearch 存放数据的地方，可以理解为关系型数据库的数据库。我们的数据被存储和索引在分片(shards)中，索引只是一个把一个或多个分片分组在一起的逻辑空间。索引的结构是为快速有效的全文索引准备的，索引名称必须是小写，并且不能用下划线开头 类型(type)用于区分同一个索引下不同的数据类型，相当于关系型数据库中的表。在 Elasticsearch 中，我们使用相同类型的文档表示相同的 “事物”，因为他们的数据结构也是相同的。每个类型都有自己的映射(mapping)或者结构定义，就像传统数据库表中的列一样。所有类型下的文档被存储在同一个索引下，但是类型的映射会告诉 Elasticsearch 不同的文档如何被索引(ES6.0之后一个索引只能存在一种类型) 文档(document)是 ElasticSearch 中存储的实体数据，一个文档相当于数据库表中的一行记录。在 Elasticsearch 中，文档这个术语有着特殊含义。它特指最顶层结构或者根对象(root object)序列化成的 JSON 数据(以唯一ID标识并存储于Elasticsearch中) 关系型数据库与 Elasticsearch 的概念类比如下 Relational DB Databases Tables Rows Columns Elasticsearch Indices Types Documents Fields RESTful API在 Elasticsearch 中，提供了功能丰富的 RESTful API 的操作，包括基本的 CRUD、创建索引、删除索引等操作。RESTful 是统一规范的 http 接口，任何语言都可以使用。我们可以直接使用 web 客户端(postman)来测试，甚至还可以使用 Linux 上的 curl 工具测试，不需要自己写程序来调用 Elasticsearch 服务 # Elasticsearch RESTful 操作数据的风格 curl -X &lt;verb> '&lt;protocol>://&lt;host>:&lt;port>/&lt;path>?&lt;query_string> -d &lt;body>' verb：HTTP 方法，如 GET、POST、PUT、HEAD、DELETE host：ES 集群中的任意节点主机名 port：ES HTTP 服务端口默认 9200 path：索引路径 query_string：可选的查询请求参数，例如 ?pretty 参数将格式化输出 JSON 数据 -d：一个 GET 的 JSON 格式请求主体 body：自己写的 JSON 格式的请求主体 创建索引在 Lucene 中创建索引是需要定义字段名称以及字段的类型，在 Elasticsearch 中提供了非结构化的索引，就是不需要创建索引结构，即可写入数据到索引中，实际上在 Elasticsearch 底层会进行结构化操作，此操作对用户是透明的 # 创建一个 data 的空索引 curl -X PUT '10.10.110.191:9200/data' # 删除索引 curl -X DELETE '10.10.110.191:9200/data' 插入数据URL 规则：POST /索引/类型/id # 往data这个索引下的user类型中插入一条ID为1的数据,?pretty是以json格式返回数据 curl -X POST '10.10.110.191:9200/data/user/1?pretty' -H \"Content-Type:application/json\" -d ' { \"name\": \"mowenjie\", \"job\": \"DevOps\", \"base\": \"sz\" }' # 响应 { \"_index\" : \"data\", \"_type\" : \"user\", \"_id\" : \"1\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 1, \"failed\" : 0 }, \"_seq_no\" : 0, \"_primary_term\" : 1 } # 不指定ID插入数据会自动生成ID curl -X POST '10.10.110.191:9200/data/user/?pretty' -H \"Content-Type:application/json\" -d ' { \"name\": \"missf\", \"job\": \"linux\", \"base\": \"sz\" }' # 响应 { \"_index\" : \"data\", \"_type\" : \"user\", \"_id\" : \"-J1PMXQBkHjO2vDovLJx\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 1, \"_primary_term\" : 1 } 更新数据在 Elasticsearch 中可以通过覆盖的方式对数据进行更新 # 对ID为1的这条数据进行更新 curl -X PUT '10.10.110.191:9200/data/user/1?pretty' -H \"Content-Type:application/json\" -d ' { \"name\": \"fan\", \"job\": \"java\", \"base\": \"bj\" }' # 查询更新结果 curl -X GET '10.10.110.191:9200/data/user/1?pretty' { \"_index\" : \"data\", \"_type\" : \"user\", \"_id\" : \"1\", \"_version\" : 2, # 更新之后的数据版本进行了+1 \"_seq_no\" : 2, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"name\" : \"fan\", \"job\" : \"java\", \"base\" : \"bj\" } } # 上面是更新整条数据,下面是局部更新一条数据的某些字段,需要使用_update标识 curl -X POST '10.10.110.191:9200/data/user/1/_update?pretty' -H \"Content-Type:application/json\" -d ' { \"doc\":{ \"name\": \"aaa\" } }' 删除数据在 Elasticsearch 中，删除文档数据只需要发起 DELETE 请求即可 # 删除ID为1的这条数据 curl -X DELETE 'http://10.10.110.191:9200/data/user/1?pretty' # 响应,看到返回\"result\" : \"deleted\"就表示删除成功,如果删除一条不存在的数据会返回404 { \"_index\" : \"data\", \"_type\" : \"user\", \"_id\" : \"1\", \"_version\" : 2, \"result\" : \"deleted\", \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"_seq_no\" : 1, \"_primary_term\" : 1 } 删除一个文档也不会立即从磁盘上移除，它只是被标记成已删除。Elasticsearch 将会在你之后添加更多索引的时候才在后台进行删除内容的清理 搜索数据根据 id 搜索数据 curl -X GET '10.10.110.191:9200/data/user/003?pretty' 搜索全部数据 curl -X GET '10.10.110.191:9200/data/user/_search?pretty' # 响应默认只返回10条数据 关键字搜素数据 # 查询 base 等于 sz 的用户数据 curl -X GET '10.10.110.191:9200/data/user/_search?q=base:sz' DSL 搜索Elasticsearch 提供基于 JSON 的完整查询语言 DSL(Query DSL) 来定义查询，它允许你构建更加复杂、强大的查询 # 查询base等于sz的用户 curl -X POST '10.10.110.191:9200/data/user/_search?pretty' -H \"Content-Type:application/json\" -d ' { # 请求体 \"query\" : { \"match\" : { \"base\" : \"sz\" } } } ' # 查询age大于16且job等于Linux的用户 curl -X POST '10.10.110.191:9200/data/user/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"query\": { \"bool\": { \"filter\": { \"range\": { \"age\": { \"gt\": 16 } } }, \"must\": { \"match\": { \"job\": \"Linux\" } } } } } ' # 全文搜索 curl -X POST '10.10.110.191:9200/data/user/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"query\": { \"match\": { \"name\": \"miss lisi\" } } } ' 高亮显示查询得到需要高亮的数据，再使用 highlight 将需要高亮的字段写在 fields 里面 curl -X POST '10.10.110.191:9200/data/user/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"query\": { \"match\": { \"name\": \"miss lisi\" } }, \"highlight\": { \"fields\": { \"name\": {} } } } ' 聚合在 Elasticsearch 中支持聚合操作，类似 SQL 中的 group by 操作 # 根据字段值分组聚合 curl -X POST '10.10.110.191:9200/data/user/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"aggs\": { \"age_terms\": { \"terms\": { \"field\": \"age\" } } } } ' # 响应,age字段值为16的有1条数据,age字段值为25的有2条数据 \"aggregations\" : { \"age_terms\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : 16, \"doc_count\" : 1 }, { \"key\" : 25, \"doc_count\" : 2 }, { \"key\" : 36, \"doc_count\" : 1 } ] } } 文档一个文档不只有数据，它还包含了元数据(metadata)——关于文档的信息。三个必须的元数据节点是 节点 说明 _index 文档存储的地方 _type 文档代表的对象的类 _id 文档的唯一标识 索引(index)类似于关系型数据库里的 “数据库” ——它是我们存储和索引关联数据的地方 _type(类型)，在关系型数据库中，我们经常将相同类的对象存储在一个表里，因为它们有着相同的结构。同理，在 Elasticsearch 中我们使用相同类型(type)的文档表示相同的 “事物”，因为他们的数据结构也是相同的 id 仅仅是一个字符串，它与 _index 和 _type 组合时，就可以在 Elasticsearch 中唯一标识一个文档。当创建一个文档时你可以自定义 _id ，也可以让 Elasticsearch 帮你自动生成 响应查询指定响应字段 # 只响应 _source 下的 name,job 字段 curl -X GET '10.10.110.191:9200/data/user/001/?_source=name,job' 不返回元数据，仅仅返回原始数据 curl -X GET '10.10.110.191:9200/data/user/001/_source' 判断文档存在如果我们只需要判断文档是否存在，而不查询文档内容 # 如果文档存在,Elasticsearch 将返回 HTTP/1.1 200 OK,如果不存在就返回 HTTP/1.1 404 Not Found curl -i -X HEAD 'http://10.10.110.191:9200/data/user/001' 当然，这只表示你在查询的那一刻文档不存在，但并不表示几毫秒后依旧不存在，另一个进程在这期间可能创建新文档 批量操作有些情况下可以通过批量操作以减少网络请求，如：批量查询、批量插入数据 # 批量查询 curl -X POST '10.10.110.191:9200/data/user/_mget?pretty' -H \"Content-Type:application/json\" -d ' { \"ids\": [\"001\", \"002\"] }' # 响应 { \"docs\" : [ { \"_index\" : \"data\", \"_type\" : \"user\", \"_id\" : \"001\", \"_version\" : 1, \"_seq_no\" : 0, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"name\" : \"lisi\", \"job\" : \"Python\", \"base\" : \"sh\", \"age\" : 16 } }, { \"_index\" : \"data\", \"_type\" : \"user\", \"_id\" : \"002\", \"_version\" : 1, \"_seq_no\" : 1, \"_primary_term\" : 1, \"found\" : true, \"_source\" : { \"name\" : \"mowenjie\", \"job\" : \"Linux\", \"base\" : \"sz\", \"age\" : 36 } } ] } 分页和 SQL 使用 LIMIT 关键字返回只有一页的结果一样，Elasticsearch 接受 from 和 size 参数 size: 结果数,默认10 from: 从第n条数据之后开始,默认0 查询一个区间的数据 # 导入官方测试数据 curl -H \"Content-Type: application/x-ndjson\" -XPOST \"10.10.110.191:9200/bank/account/_bulk?pretty\" --data-binary @accounts.json # 将数据的account_number字段进行排序之后再取数据 curl -X GET \"http://10.10.110.191:9200/bank/account/_search?pretty\" -H \"Content-Type:application/json\" -d ' { \"query\": {\"match_all\": {} }, \"sort\": [{\"account_number\": \"asc\"}], \"from\": 10, # 从第10条数据之后开始 \"size\": 30 # 一共返回30条数据,就是account_number为10-39的数据 }' # 取1000到2000这个区间的随机数据 curl -X GET \"http://10.10.110.191:9200/bank/account/_search?pretty\" -H \"Content-Type:application/json\" -d' { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"range\": { \"balance\": { \"gte\": 1000, \"lte\": 2000 } } } } } }' 映射前面我们创建的索引以及插入数据，都是由 Elasticsearch 进行自动判断类型，有些时候我们是需要进行明确字段类型的，否则自动判断的类型和实际需求是不相符的。每个字段都有一个数据类型，可以是一个简单的类型：text、keyword、date、long、double、boolean、ip，或者一个支持 JSON 层次结构的类型：例如 object、nested，或者是一种特殊的类型：geo_point、geo_shape、completion 创建明确类型的索引 curl -X PUT '10.10.110.191:9200/itcast' -H \"Content-Type:application/json\" -d ' { \"settings\": { \"index\": { \"number_of_shards\": \"2\", \"number_of_replicas\": \"0\" } }, \"mappings\": { \"properties\": { \"name\": { \"type\": \"text\" }, \"age\": { \"type\": \"integer\" }, \"mail\": { \"type\": \"keyword\" }, \"hobby\": { \"type\": \"text\" } } } }' 查看索引映射 curl -X GET '10.10.110.191:9200/itcast/_mapping' # 响应 { \"itcast\" : { \"mappings\" : { \"properties\" : { \"age\" : { \"type\" : \"integer\" }, \"hobby\" : { \"type\" : \"text\" }, \"mail\" : { \"type\" : \"keyword\" }, \"name\" : { \"type\" : \"text\" } } } } } 批量插入数据 # 如果插入的数据类型与我们字段定义的类型不同,那么就无法插入 curl -X POST '10.10.110.191:9200/itcast/_bulk' -H \"Content-Type:application/json\" --data-binary @itcast.json {\"index\":{\"_index\":\"itcast\",\"_type\":\"_doc\",\"_id\":\"1\"}} {\"name\":\"张三\",\"age\": 20,\"mail\": \"111@qq.com\",\"hobby\":\"羽毛球、乒乓球、足球\"} {\"index\":{\"_index\":\"itcast\",\"_type\":\"_doc\",\"_id\":\"2\"}} {\"name\":\"李四\",\"age\": 21,\"mail\": \"222@qq.com\",\"hobby\":\"羽毛球、乒乓球、足球、篮球\"} {\"index\":{\"_index\":\"itcast\",\"_type\":\"_doc\",\"_id\":\"3\"}} {\"name\":\"王五\",\"age\": 22,\"mail\": \"333@qq.com\",\"hobby\":\"羽毛球、篮球、游泳、听音乐\"} {\"index\":{\"_index\":\"itcast\",\"_type\":\"_doc\",\"_id\":\"4\"}} {\"name\":\"赵六\",\"age\": 23,\"mail\": \"444@qq.com\",\"hobby\":\"跑步、游泳\"} {\"index\":{\"_index\":\"itcast\",\"_type\":\"_doc\",\"_id\":\"5\"}} {\"name\":\"孙七\",\"age\": 24,\"mail\": \"555@qq.com\",\"hobby\":\"听音乐、看电影\"} 查询插入的数据 curl -X POST '10.10.110.191:9200/itcast/_doc/_search?pretty' 结构化查询term 主要用于精确匹配某些值，比如数字、日期、布尔值或 not_analyzed 的字符串(未经分析的文本数据类型) curl -X POST '10.10.110.191:9200/itcast/_doc/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"query\" : { \"term\" : { \"age\" : 20 } } }' range 过滤允许我们按照指定范围查询一批数据 # 查询age大于等于20小于等于22范围的数据(gt:大于,gte:大于等于,lt:小于,lte:小于等于) curl -X POST '10.10.110.191:9200/bank/account/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"query\" : { \"range\": { \"age\": { \"gte\": 20, \"lte\": 22 } } } }' exists 查询可以用于查找文档中是否包含指定字段或没有某个字段，类似于 SQL 语句中的 IS_NULL 条件 # 查询原始数据中含有address字段的文档 curl -X POST '10.10.110.191:9200/bank/account/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"query\" : { \"exists\": { \"field\": \"address\" } } }' match 是一个模糊查询，需要指定字段名，但是会进行分词(中英文分词不一样) # 查询hobby字段是乒乓球的记录,在查询之前会进行分词(只要记录包含[乒/乓/球]都会被匹配成功) curl -X POST '10.10.110.191:9200/itcast/_doc/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"query\": { \"match\": { \"hobby\": \"乒乓球\" } } }' bool 查询可以用来合并多个条件查询结果的布尔逻辑，它包含以下操作符： must: 多个查询条件的完全匹配，相当于and must_not: 多个查询条件的相反匹配，相当于not should: 至少有一个查询条件匹配，相当于or filter: 必须匹配，但它不会对匹配的数据进行评分 # 只要包含\"乒乓 游泳\"的数据都会被匹配 curl -X POST '10.10.110.191:9200/itcast/_doc/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"hobby\": \"乒乓 游泳\" } } ] } } }' # hobby包含乒乓但是age不等于21的数据 curl -X POST '10.10.110.191:9200/itcast/_doc/_search?pretty' -H \"Content-Type:application/json\" -d ' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"hobby\": \"乒乓\" } } ], \"must_not\": [ { \"match\": { \"age\": \"21\" } } ] } } }' 中文分词中文分词的难点在于在汉语中没有明显的词汇分界点，如在英语中空格可以作为分隔符，如果分隔不正确就会造成歧义。常用中文分词器有 IK、jieba、THULAC 等，推荐使用 IK 分词器 IK Analyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。从2006年12月推出1.0版开始，IKAnalyzer已经推出了3个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。新版本的IK Analyzer 3.0则发展为面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。采用了特有的”正向迭代最细粒度切分算法”，具有80万字/秒的高速处理能力 采用了多子处理器分析模式，支持：英文字母（IP地址、Email、URL）、数字（日期，常用中文数量词，罗马数字，科学计数法），中文词汇（姓名、地名处理）等分词处理。 优化的词典存储，更小的内存占用 安装 ik 中文分词器 # 下载对应 es 版本的 ik 分词器 https://github.com/medcl/elasticsearch-analysis-ik # 创建目录 cd your-es-root/plugins/ &amp;&amp; mkdir ik # 解压 unzip plugin to folder your-es-root/plugins/ik # 重启 es(集群环境每一台都要配置) 分词测试 curl -X POST '10.10.110.191:9200/_analyze?pretty' -H \"Content-Type:application/json\" -d ' { \"analyzer\": \"ik_max_word\", \"text\": \"我是中国人\" }' 结果 { \"tokens\" : [ { \"token\" : \"我\", \"start_offset\" : 0, \"end_offset\" : 1, \"type\" : \"CN_CHAR\", \"position\" : 0 }, { \"token\" : \"是\", \"start_offset\" : 1, \"end_offset\" : 2, \"type\" : \"CN_CHAR\", \"position\" : 1 }, { \"token\" : \"中国人\", \"start_offset\" : 2, \"end_offset\" : 5, \"type\" : \"CN_WORD\", \"position\" : 2 }, { \"token\" : \"中国\", \"start_offset\" : 2, \"end_offset\" : 4, \"type\" : \"CN_WORD\", \"position\" : 3 }, { \"token\" : \"国人\", \"start_offset\" : 3, \"end_offset\" : 5, \"type\" : \"CN_WORD\", \"position\" : 4 } ] }","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"}]},{"title":"Elastic Stack","slug":"Elastic Stack","date":"2020-08-05T06:19:27.000Z","updated":"2020-09-28T03:50:08.000Z","comments":true,"path":"post/cb83e724.html","link":"","permalink":"https://www.missf.top/post/cb83e724.html","excerpt":"","text":"Elastic Stack 简介ELK 日志收集分析平台相信所有的运维工程师都听说过，实际上 ELK 不是一门技术，而是三个软件的简称。它们分别是由 Elasticsearch、Logstash、Kibana 组成，在ELK发展的过程中，又有新成员 Beats 的加入，所以就形成了 Elastic Stack Elastic Stack 的组成 ElasticsearchElasticsearch 基于 java 语言开发，是个开源分布式搜索引擎，它的特点有:分布式、零配置、自动发现、索引自动分片、索引副本机制、RESTful 风格接口、多数据源、自动搜索负载等 LogstashLogstash 基于 java 语言开发，是一个开源的用于收集，分析和存储日志的工具 KibanaKibana 基于 nodejs，也是一个开源和免费的工具，Kibana 可以为 Logstash 和 ElasticSearch 提供的日志分析的友好 Web 界面，可以汇总、分析和搜索重要数据日志 BeatsBeats 是 elastic 公司开源的一款采集系统监控数据的代理 agent，是在被监控服务器上以客户端形式运行的数据收集器的统称，可以直接把数据发送给 Elasticsearch 或者通过 Logstash 发送给 Elasticsearch，然后进行后续的数据分析活动 Beats由如下组成： Packetbeat：一个网络数据包分析器，用于监控、收集网络流量信息，Packetbeat 嗅探服务器之间的流量，解析应用层协议，并关联到消息的处理，其支持ICMP(v4 and v6)、DNS、HTTP、Mysql、PostgreSQL、Redis、MongoDB、Memcache 等协议 Filebeat：用于监控、收集服务器日志文件，其已取代 logstash forwarder Metricbeat：可定期获取外部系统的监控指标信息，其可以监控、收集 Apache、HAProxy、MongoDB、MySQL、Nginx、PostgreSQL、Redis、System、Zookeeper 等服务 Winlogbeat：用于监控、收集 Windows 系统的日志信息","categories":[{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"}]},{"title":"Prometheus pushgateway工具","slug":"Prometheus pushgateway 工具","date":"2020-08-04T10:05:33.000Z","updated":"2020-10-10T08:29:28.000Z","comments":true,"path":"post/5cf39589.html","link":"","permalink":"https://www.missf.top/post/5cf39589.html","excerpt":"","text":"PushGateway 部署prometheus 基于 http 的 pull 方式去采集时间序列数据，但是由于业务需求，prometheus 和 exporter 可能不在一个子网或者防火墙原因，导致 prometheus 无法直接拉取各个 target 数据，或者需要将不同的数据进行汇总，这时候就可以使用 prometheus 的自带组件 pushgateway 进行数据的汇总，将默认的 pull 方式改为 push 方式进行数据的采集 # 下载pushgateway wget https://github.com/prometheus/pushgateway/releases/download/v1.2.0/pushgateway-1.2.0.linux-amd64.tar.gz # 解压 tar xf pushgateway-1.2.0.linux-amd64.tar.gz &amp;&amp; mv pushgateway-1.2.0.linux-amd64 /usr/local/pushgateway # 创建pushgateway启动文件 vim /usr/lib/systemd/system/pushgateway.service [Unit] Documentation=pushgateway exporter After=local-fs.target network-online.target network.target Wants=local-fs.target network-online.target network.target [Service] Restart=on-failure ExecStart=/usr/local/pushgateway/pushgateway # 需要修改监听端口可以自行添加参数 [Install] WantedBy=multi-user.target # 启动pushgateway systemctl start pushgateway.service Prometheus 添加 PushGateway在我们的 prometheus 配置文件添加 pushgateway 的地址 vim /usr/local/prometheus/prometheus.yml scrape_configs: - job_name: 'pushgateway' static_configs: - targets: ['49.233.200.185:9091'] # 这个是安装了pushgateway的服务器地址 labels: instance: pushgateway 重启 prometheus 服务 systemctl restart prometheus.service pushgateway 其实是一个中转站，我们可以使用任何高级语言发送 post 请求到 pushgateway，然后对数据进行增加删除等操作，pushgateway 再把数据实时推送到 prometheus 推送数据到 PushGatewayecho \"missf 123456\" | curl --data-binary @- http://49.233.200.185:9091/metrics/job/prometheus # 数据:missf,值:123456 # --data-binary 表示发送二进制数据(post方式) # http://49.233.200.185:9091 pushgateway的地址 查看 pushgateway 推送到 prometheus上的数据，这可以看到有 exported_job=”prometheus” 和 job=”prometheus” 两个指标，我们推送数据时指定的 job 是 prometheus，为什么这里的 job 会显示 pushgateway 呢？这里需要修改一个 honor_labels 的参数 修改 prometheus 的配置文件，开启 honor_labels 参数(默认为false) scrape_configs: - job_name: 'pushgateway' honor_labels: true static_configs: - targets: ['49.233.200.185:9091'] labels: instance: pushgateway 重启 prometheus 再次推送数据到 pushgateway，然后查看 prometheus 上的数据 echo \"mwj 123456\" | curl --data-binary @- http://49.233.200.185:9091/metrics/job/prometheus/instance/missf 这里说明一下 honor_labels 的作用：因为 prometheus 配置 pushgateway 的时候，也会指定 job 和 instance，但是它只表示 pushgateway 实例本身，不能真正表达收集数据的含义。所以配置 pushgateway 需要添加 honor_labels：true 参数，避免收集到的数据本身的 job 和 instance 被覆盖。具体参考官网 在 PushGateway 删除数据curl -X DELETE http://49.233.200.185:9091/metrics/job/prometheus/instance/missf","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/categories/Prometheus/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/tags/Prometheus/"},{"name":"监控","slug":"监控","permalink":"https://www.missf.top/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"Prometheus 业务监控","slug":"Prometheus 业务监控","date":"2020-08-03T02:26:26.000Z","updated":"2020-09-28T03:57:20.000Z","comments":true,"path":"post/f261c617.html","link":"","permalink":"https://www.missf.top/post/f261c617.html","excerpt":"","text":"Blackbox_exporter 部署Blackbox_exporter 是 prometheus 官方提供的 exporter 之一，可以提供 http、dns、tcp、icmp 的监控数据采集 # 下载 wget https://github.com/prometheus/blackbox_exporter/releases/download/v0.17.0/blackbox_exporter-0.17.0.linux-amd64.tar.gz # 解压 tar xf blackbox_exporter-0.17.0.linux-amd64.tar.gz &amp;&amp; mv blackbox_exporter-0.17.0.linux-amd64 /usr/local/blackbox # 创建blackbox启动文件 vim /usr/lib/systemd/system/blackbox.service [Unit] Documentation=Blackbox exporter After=local-fs.target network-online.target network.target Wants=local-fs.target network-online.target network.target [Service] Restart=on-failure ExecStart=/usr/local/blackbox/blackbox_exporter --config.file=/usr/local/blackbox/blackbox.yml [Install] WantedBy=multi-user.target # 启动blackbox systemctl daemon-reload systemctl restart blackbox.service 配置 TCP 端口检测及告警传统的端口检测方式，调用命令的方式去实现 ncat -vz 47.100.107.121 80 # 返回seconds而不是timeout那么端口就是通的 telnet ...... zabbix监控端口可以通过模板或者自定义key写脚本实现 修改 prometheus 配置文件，配置 TCP 端口检测 vim /usr/local/prometheus/prometheus.yml scrape_configs: - job_name: 'nginx_port_check' metrics_path: /probe params: module: [tcp_connect] file_sd_configs: - files: - check/port/nginx.json relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 47.100.107.121:9115 # 这个是blackbox所在主机以及端口 编写监控 TCP 端口的告警规则 vim /usr/local/prometheus/rules/nginx_port_check.yml groups: - name: nginx port check rules: - alert: nginx_port_check failed for: 5s expr: probe_success{job=\"nginx_port_check\"} == 0 labels: serverity: critical annotations: description: \"{{ $labels.group }} port connection fail,{{ $labels.group }} value is:{{ $value }}\" summary: \"{{ $labels.group }} port connection failed\" 修改 prometheus 配置文件需要重启 prometheus 服务 systemctl restart prometheus.service 关闭 Nginx 测试当 80 端口无法访问之后的告警结果 业务接口检测及告警基于现在 Java + Vue 前后端分离的开发模式下，我们很多时候需要去检测 Java 的接口是否正常。传统的手动检测可以使用 postman，或者写 shell 脚本也可以实现，但是prometheus 可以通过 blackbox 去更好的检测业务接口 修改 prometheus 配置文件，添加监控业务接口的 job scrape_configs: - job_name: 'get_mysite' scrape_interval: 5s metrics_path: /probe params: module: [http_2xx] file_sd_configs: - files: - check/url/*.json relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 47.100.107.121:9115 编写监控 url 链接的告警规则文件 vim /usr/local/prometheus/rules/get_mysite.yml groups: - name: get mysite check rules: - alert: get_mysite_check failed for: 5s expr: probe_success{group=\"get_mysite\",instance=\"https://www.missf.top\",job=\"get_mysite\"} == 0 labels: serverity: critical annotations: description: \"{{ $labels.group }} failed, {{ $labels.group }} value is:{{ $value }}\" summary: \"{{ $labels.group }} website not accessible\" 修改 prometheus 配置文件之后记得重启 prometheus 服务 systemctl restart prometheus.service 如果 https://www.missf.top 这个链接的 http 请求不是返回 2xx 的状态码就会告警 我们在监控业务接口时，只监控到接口的返回状态(2xx状态码)，假如我们想要监控业务接口的返回内容该如何实现呢？那就需要修改 blackbox 的配置文件 modules: http_2xx: prober: http # 下面这段是需要添加的内容 http: method: GET headers: Host: www.missf.top Accept-Language: en-US Origin: missf.top fail_if_body_matches_regexp: # 如果我get的url地址返回的正文中有\"apache\",那么就会失败,则probe_success值为0 - \"apache\" fail_if_body_not_matches_regexp: - \"nginx\" # 如果我get的url地址返回的正文中没有\"nginx\",那么就会失败,则probe_success值为0 http_post_2xx: prober: http http: method: POST tcp_connect: prober: tcp pop3s_banner: prober: tcp tcp: query_response: - expect: \"^+OK\" tls: true tls_config: insecure_skip_verify: false ssh_banner: prober: tcp tcp: query_response: - expect: \"^SSH-2.0-\" irc_banner: prober: tcp tcp: query_response: - send: \"NICK prober\" - send: \"USER prober prober prober :prober\" - expect: \"PING :([^ ]+)\" send: \"PONG ${1}\" - expect: \"^:[^ ]+ 001\" icmp: prober: icmp 修改了 blackbox 配置文件需要重启 blackbox 服务 systemctl restart blackbox.service 上面所配置的匹配返回内容是在http_2xx这个模块下添加的，我们需要修改prometheus配置文件对应的http_2xx模块的规则文件，配置我们监控业务接口的返回内容的url地址 vim /usr/local/prometheus/check/url/get_mysite.json [ { \"targets\": [ \"47.100.107.121\" # 这个url返回的是默认的Nginx页面,对应我上面的匹配规则(nginx/apache) ], \"labels\": { \"group\": \"get_mysite\" } } ] 查看 blackbox 的采集数据 probe_success 的值是根据我们在 blackbox 配置文件的正则去决定的 这时候我们 get_mysite.json 这个规则文件的 job 的 probe_success 值就是通过 get 获取一个 url 的返回值去确定的，我们这样就可以去监控接口的返回内容了 配置网络监控我们可以让服务器使用 icmp 协议去请求 www.baidu.com 或者是一个公网 IP，测试服务器的网络是否正常 修改 prometheus 配置文件，添加网络监控的 job scrape_configs: - job_name: 'icmp_check_network' scrape_interval: 5s metrics_path: /probe params: module: [icmp] file_sd_configs: - files: - check/icmp/*.json relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 47.100.107.121:9115 编写网络监控的规则文件 vim /usr/local/prometheus/rules/check_network.yml groups: - name: icmp check network rules: - alert: icmp check network failed for: 10s expr: probe_success{group=\"icmp_check_network\",instance=\"www.baidu.com\",job=\"icmp_check_network\"} == 0 labels: serverity: critical annotations: description: \"{{ $labels.group }} icmp connection failed, {{ $labels.group }} value is: {{ $value }}\" summary: \"{{ $labels.group }} connection failed, instance: {{ $labels.instance }}\" 修改 prometheus 配置文件之后记得重启 prometheus 服务 systemctl restart prometheus.service","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/categories/Prometheus/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/tags/Prometheus/"},{"name":"监控","slug":"监控","permalink":"https://www.missf.top/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"Prometheus 告警","slug":"Prometheus 告警","date":"2020-07-23T03:53:03.000Z","updated":"2020-09-28T04:00:20.000Z","comments":true,"path":"post/615f0093.html","link":"","permalink":"https://www.missf.top/post/615f0093.html","excerpt":"","text":"Alertmanager 概述prometheus 发出告警时分为两部分，首先 prometheus 按告警规则(rule_files配置)向 alertmanager 发送告警，即告警规则是在 prometheus 上定义的，然后由 alertmanager 去管理这些告警，包括去重(deduplicating)、分组(grouping)、静音(silencing)、抑制(inhibition)、聚合(aggregation)，最终通过丰富的告警通知渠道(电话、微信、短信、邮件)将告警通知路由给对应的联系人。prometheus 的大部分组件都是 go 语言开发的，zabbix 到4.4之后的客户端才是 go 编写 Alertmanager 二进制安装# 下载 wget https://github.com/prometheus/alertmanager/releases/download/v0.21.0/alertmanager-0.21.0.linux-amd64.tar.gz # 解压 tar xf alertmanager-0.21.0.linux-amd64.tar.gz &amp;&amp; mv alertmanager-0.21.0.linux-amd64 /usr/local/alertmanager # 创建alertmanager启动文件 vim /usr/lib/systemd/system/alertmanager.service [Unit] Documentation=alertmanager [Service] Restart=on-failure ExecStart=/usr/local/alertmanager/alertmanager --config.file=/usr/local/alertmanager/alertmanager.yml --storage.path=/usr/local/alertmanager/data [Install] WantedBy=multi-user.target # 启动 systemctl daemon-reload systemctl start alertmanager.service Alertmanager 配置文件详解vim /usr/local/alertmanager/alertmanager.yml global: resolve_timeout: 5m # 恢复的超时时间,这个跟告警恢复通知有关,此参数并不是说在这个时间没有收到告警就会恢复 route: group_by: ['alertname'] # 默认以告警名进行分组,就是rule文件的alert值进行分组 group_wait: 10s # 发送警报前，至少等待多少秒才会发送(为了收集同组更多的警报信息一起发送) group_interval: 10s # 如果警报1已经发送,这时又出现同组的警报2,由于组状态发生变化,警报会在group_interval这个时间内发送,不会被repeat_interval这个时间收敛 repeat_interval: 20m # 报警信息已发送，但事件并没有恢复,则等待多久时间再重新发送(生产环境一般设成20min或者30min) receiver: 'web.hook' # 发送警报的接收者名称,如果一个报警没有被一个route匹配,则发送给默认的接收器 receivers: # 发送告警信息给那个接收者 - name: 'web.hook' # 这个需要和上面定义的接收者名称一致 webhook_configs: - url: 'http://127.0.0.1:5001/' inhibit_rules: # 抑制规则,防止告警风暴 - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] 检查 Alertmanager 配置文件# 提示SUCCESS,则配置文件没有问题 ./amtool check-config alertmanager.yml # 修改配置文件之后重启alertmanager systemctl restart alertmanager.service 配置邮件告警修改 alertmanager 配置文件，填写邮箱的验证信息，定义路由的收件人，配置发送告警邮件到那个邮箱 cat /usr/local/alertmanager/alertmanager.yml global: resolve_timeout: 3m smtp_smarthost: 'smtp.qq.com:465' smtp_from: '1173354099@qq.com' smtp_auth_username: '1173354099@qq.com' smtp_auth_password: '' # 授权码 smtp_require_tls: false route: group_by: ['alertname'] group_wait: 10s group_interval: 10s repeat_interval: 20m receiver: 'devops.mail' receivers: - name: 'devops.mail' email_configs: - to: 'mf_2013@163.com' headers: { Subject: \"[WARN] Prometheus 报警邮件\" } send_resolved: true # 发送告警恢复通知 #inhibit_rules: # - source_match: # severity: 'critical' # target_match: # severity: 'warning' # equal: ['alertname', 'dev', 'instance'] 配置 prometheus 与 alertmanager 通信，设置规则文件的路径和正则匹配 # 修改prometheus配置文件 vim /usr/local/prometheus/prometheus.yml alerting: alertmanagers: - static_configs: - targets: - 127.0.0.1:9093 # 这里由于alertmanager是和prometheus部署在同一台机器上,所以写本机地址 rule_files: - \"rules/*.yml\" # rules这个目录是在prometheus上的,指当前配置文件的同级目录,这个目录需要自己创建 # 检查prometheus配置文件 ./promtool check config prometheus.yml systemctl restart prometheus.service 编写 rules 文件，根据 rules 文件中的表达式去告警，这个规则文件的路径是 prometheus 配置文件中定义的 # 监控节点的状态 cat /usr/local/prometheus/rules/node.yml groups: - name: node_alert rules: - alert: Node_InstanceDown expr: up == 0 # 表达式 for: 5s labels: serverity: error annotations: summary: \"Instance {{ $labels.instance }} down\" description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes\" 在 prometheus 的 web 控制台查看配置的规则 关闭 node_exporter.service 节点，查看告警邮件 配置微信告警修改 alertmanager 配置文件，定义路由规则 cat /usr/local/alertmanager/alertmanager.yml global: resolve_timeout: 3m smtp_smarthost: 'smtp.qq.com:465' smtp_from: '1173354099@qq.com' smtp_auth_username: '1173354099@qq.com' smtp_auth_password: '' # 授权码 smtp_require_tls: false templates: - /usr/local/alertmanager/template/wechat.temp route: group_by: ['alertname'] group_wait: 10s group_interval: 10s repeat_interval: 20m #receiver: 'devops.mail' receiver: 'devops.mailwechat' routes: # 为node_exporter、docker、mysqld_exporter定义匹配路由,每个路由有自己的分组在微信告警时信息就会单独发送 - receiver: 'devops.mailwechat' # 每个服务可以定义自己的接收者,这样在发送时就可以发送给不同的人,不同的服务对应不同的处理人员 group_wait: 10s group_by: ['node_exporter'] match_re: job: node_exporter - receiver: 'devops.mailwechat' group_wait: 10s group_by: ['docker'] match_re: job: docker - receiver: 'devops.mailwechat' group_wait: 10s group_by: ['mysqld_exporter'] match_re: job: mysqld_exporter receivers: - name: 'devops.mailwechat' # 将这个告警同时发送到邮件和微信 email_configs: - to: 'mf_2013@163.com' headers: { Subject: \"[WARN] Prometheus 报警邮件\" } send_resolved: true wechat_configs: - api_secret: '' agent_id: '' corp_id: '' to_party: '' send_resolved: true #inhibit_rules: # - source_match: # severity: 'critical' # target_match: # severity: 'warning' # equal: ['alertname', 'dev', 'instance'] 编写 rules 文件，为了每个服务单独报警，这里将 node_exporter、docker、mysqld_exporter 分开去写匹配规则 cat /usr/local/prometheus/rules/node.yml groups: - name: node_exporter rules: - alert: node_exporter_Down expr: up{job=\"node_exporter\"} == 0 for: 5s labels: serverity: error annotations: summary: \"Instance {{ $labels.instance }} down\" description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes\" - name: mysqld_exporter rules: - alert: mysqld_exporter_Down expr: up{job=\"mysqld_exporter\"} == 0 for: 5s labels: serverity: error annotations: summary: \"Instance {{ $labels.instance }} down\" description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes\" - name: docker rules: - alert: docker_Down expr: up{job=\"docker\"} == 0 for: 5s labels: serverity: error annotations: summary: \"Instance {{ $labels.instance }} down\" description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes\" 配置完成之后重启 alertmanager systemctl restart alertmanager.service 关闭 node_exporter 和 docker_cadvisor 服务，这时候会每个服务单独发送告警信息，由于将全部服务 group_by 在一个组里面，在发送恢复信息时会出现服务混乱的情况，所以我将每个服务做了路由，每一个服务都有自己的 group_by，这样在发送信息时才会单独去发送 配置钉钉告警先去创建一个钉钉机器人，具体过程这里就不详细说明了 prometheus 配置钉钉告警需要使用到 prometheus-webhook-dingtalk 插件，我们先使用二进制安装钉钉插件，dingtalk 服务默认启动的端口是 8060 prometheus-webhook-dingtalk插件下载地址 # 下载prometheus-webhook-dingtalk wget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v1.4.0/prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz # 解压prometheus-webhook-dingtalk tar xf prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz &amp;&amp; mv prometheus-webhook-dingtalk-1.4.0.linux-amd64 /usr/local/dingtalk # 编写dingtalk启动文件 vim /usr/lib/systemd/system/dingtalk.service [Unit] Description=prometheus-webhook-dingtalk After=network-online.target [Service] Restart=on-failure ExecStart=/usr/local/dingtalk/prometheus-webhook-dingtalk \\ --ding.profile=Prometheus告警=https://oapi.dingtalk.com/robot/send?access_token=xxxxxx [Install] WantedBy=multi-user.target # 启动dingtalk.service systemctl daemon-reload systemctl restart dingtalk.service # 查看dingtalk的webhook地址 journalctl -fu dingtalk.service Jul 29 18:38:01 iZuf6fpaicz5jt7kep555qZ prometheus-webhook-dingtalk[5504]: ts=2020-07-29T10:38:01.655Z caller=main.go:133 component=configuration msg=\"Webhook urls for prometheus alertmanager\" urls=http://localhost:8060/dingtalk/Prometheus告警/send 修改 prometheus 的 alertmanager 配置，更改告警的路由和接收者 route: receiver: 'devops_dingtalk' # 接收者必须和下面的一致 receivers: - name: 'devops_dingtalk' webhook_configs: - url: 'http://localhost:8060/dingtalk/Prometheus告警/send' # 这个URL是dingtalk的webhook地址 send_resolved: true 关闭 docker 收集器查看告警效果 告警状态prometheus 的告警状态有三种，我们可以在 prometheus 的控制台页面上查看告警的状态 inactive 没有触发任何阈值，这个是根据 scrape_interval 参数(采集数据周期)和 evaluation_interval 参数(对比规则周期)去决定的 pending 已触发阈值但未满足告警持续时间，告警进入 pending 状态之后，需要等待规则配置的 for 时间，如果在这个时间内触发阈值的表达式一直成立，才会进入 firing 状 态 firing 已触发阈值且满足告警持续时间，将告警从 prometheus 发送给 alertmanager，在 alertmanager 收到告警之后并不会立刻发送，还需要等待一个 group_wait 时间，直到某个计算周期表达式为假，告警状态变更为 inactive，发送一个 resolve 给 altermanger，说明此告警已解决 告警收敛alertmanager 在收到 prometheus 发送的告警之后，并不是把收到的信息简单的直接发送出去，而是通过一系列的收敛机制(分组、抑制、静默)去筛选出需要发送的信息，如果 alertmanager 收到信息就直接发送出去，会导致告警信息过多，运维人员会被告警信息淹没，错过重要的告警信息 分组 将类似性质的告警分类为单个通知，减少告警消息数量 将类似性质的告警进行聚合发送，帮助运维更好的排查问题 抑制 当告警发出后，停止重复发送由此告警而引起的其他告警，帮助运维第一时间掌握最核心的告警信息 inhibit_rules: - source_match: severity: 'critical' # 当发生critical级别的告警时,就会抑制下面warning级别的告警 target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] # 已发送的告警与新产生的告警中equal定义的标签完全相同,则启动抑制机制 静默 是一种简单的特定时间静音的提醒机制，在发布新版本时我们需要停掉某些进程，这时候告警肯定会触发的，由于这是我们已经预知的现象，我们可以打开 prometheus 主机的 9093 端口暂时将告警设置成静音 Prometheus一条告警是怎么触发的1.采集数据 scrape_interval: 15s 2.比对采集到的数据是否触发阈值 evaluation_interval: 15s 3.判断是否超出持续时间(在这个时间内一直处于触发阈值状态)for: 5s 4.告警到达alertmanager然后进行分组、抑制、静默 5.通过分组、抑制、静默一系列机制的信息将会被发送，但是会延迟发送group_wait: 10s 编写告警规则案例groups: - name: general.rules rules: - alert: node_FileSystemUsage # 监控磁盘使用率 expr: 100 - node_filesystem_free_bytes{mountpoint=\"/\",fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{mountpoint=\"/\",fstype=~\"ext4|xfs\"} * 100 > 80 for: 5s labels: serverity: warning annotations: summary: \"Instance {{ $labels.instance }} : {{ $labels.mountpoint }} Partition utilization is too high\" description: \"{{ $labels.instance }} : {{ $labels.mountpoint }} Partition utilization is greater than 80% (Currently: {{ $value }})\" - alert: node_MemoryUsage # 监控内存使用率 expr: 100 - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) / node_memory_MemTotal_bytes * 100 > 80 for: 5s labels: serverity: warning annotations: summary: \"Instance {{ $labels.instance }} High memory usage\" description: \"{{ $labels.instance }} Memory usage greater than 80% (Currently: {{ $value }})\" - alert: node_cpuUsage # 监控CPU使用率 expr: 100 - irate(node_cpu_seconds_total{mode=\"idle\",job=\"node_exporter\",instance=\"47.100.107.121:9100\"}[5m]) * 100 > 80 for: 5s labels: serverity: warning annotations: summary: \"Instance {{ $labels.instance }} High cpu usage\" description: \"{{ $labels.instance }} Memory usage greater than 60% (Currently: {{ $value }})\"","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/categories/Prometheus/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/tags/Prometheus/"},{"name":"监控","slug":"监控","permalink":"https://www.missf.top/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"Prometheus 监控案例","slug":"Prometheus 监控案例","date":"2020-07-16T07:43:16.000Z","updated":"2021-08-03T08:32:42.000Z","comments":true,"path":"post/ba827699.html","link":"","permalink":"https://www.missf.top/post/ba827699.html","excerpt":"","text":"监控 Linux 服务器部署 node_exporter prometheus 官方提供 Node_exporter 来让我们收集机器的系统数据，除 node_exporter 外，官方还提供 consul、memcached、haproxy、mysqld 等 exporter。exporter类似于 zabbix 写好的监控模板，但是这些 exporter 都是需要在被监控节点安装 # 下载node_exporter wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz # 解压 tar xf node_exporter-1.0.1.linux-amd64.tar.gz &amp;&amp; mv node_exporter-1.0.1.linux-amd64 /usr/local/node_exporter # 编写启动文件 vim /usr/lib/systemd/system/node_exporter.service [Unit] Description=prometheus [Service] Restart=on-failure ExecStart=/usr/local/node_exporter/node_exporter [Install] WantedBy=multi-user.target # 启动node_exporter systemctl daemon-reload systemctl start node_exporter.service # 访问node_exporter的数据接口 http://10.10.110.23:9100/metrics # 默认端口是9100,默认接口是metrics 配置监控 # 修改prometheus配置文件 vim /usr/local/prometheus/prometheus.yml - job_name: 'node_exporter' file_sd_configs: - files: ['/usr/local/prometheus/sd_config/node/*.yaml'] refresh_interval: 5s # 创建服务发现的文件 vim /usr/local/prometheus/sd_config/node/*.yaml - targets: - '10.10.110.23:9100' # 这个地址是被监控节点的IP地址 promSQL监控CPU、内存、硬盘CPU监控 # 计算CPU五分钟内平均的使用率表达式 100 - irate(node_cpu_seconds_total{mode=\"idle\",instance=\"10.10.110.23:9100\",job=\"node_exporter\"}[5m]) * 100 # node_cpu_seconds_total{mode=\"idle\",instance=\"10.10.110.23:9100\",job=\"node_exporter\"}[5m]:取CPU五分钟之内的空闲值 # irate函数:将会用于计算某个指标在一定时间间隔内的变化速率 # 将得到的空闲值乘以100再得到CPU百分比的空闲值,再以100减去CPU百分比的空闲值,就得到CPU五分钟内平均的使用率 内存监控 # 计算内存使用率表达式 100 - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) / node_memory_MemTotal_bytes * 100 # (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)是内存剩余的总量 在系统层面来考虑:buff和cache是已经被使用的内存 在程序层面来考虑:buff和cache是剩余的内存 # 内存剩余的总量除以内存总量得到内存剩余率,再以100减去内存剩余率得到内存使用率 硬盘监控 # 计算硬盘使用率表达式 100 - node_filesystem_free_bytes{mountpoint=\"/\",fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{mountpoint=\"/\",fstype=~\"ext4|xfs\"} * 100 # node_filesystem_free_bytes{mountpoint=\"/\",fstype=~\"ext4|xfs\"}是根分区硬盘剩余容量,只计算ext4|xfs类型的文件系统 # node_filesystem_size_bytes{mountpoint=\"/\",fstype=~\"ext4|xfs\"}是根分区的硬盘总量 # 根分区硬盘剩余容量除以根分区的硬盘总量得到根分区硬盘的剩余率,再以100减去硬盘的剩余率得到硬盘使用率 监控系统服务状态修改 node_exporter 的启动参数 vim /usr/lib/systemd/system/node_exporter.service [Unit] Description=prometheus [Service] Restart=on-failure ExecStart=/usr/local/node_exporter/node_exporter --collector.systemd --collector.systemd.unit-whitelist=(docker|sshd).service # 利用正则匹配监控systemd的docker|sshd这三个服务 [Install] WantedBy=multi-user.target 重启 node_exporter systemctl daemon-reload systemctl restart node_exporter.service 查看监控服务的数据指标 node_systemd_unit_state{name=\"docker.service\"} 在 activating、active、deactivating、failed、inactive 五个状态中 value 为 1 的状态，就是服务当前的状态 使用 Grafana 图表展示监控数据安装 Grafana # 下载软件包 wget https://dl.grafana.com/oss/release/grafana-7.1.0-1.x86_64.rpm # 安装 yum install grafana-7.1.0-1.x86_64.rpm -y # 启动 systemctl enable grafana-server.service systemctl start grafana-server.service # Grafana默认端口为3000,账号密码都为admin,初次登录会提示需要修改密码 Grafana 配置数据源 填写 prometheus 主机的地址，在配置数据源时我们还可以配置验证、定义 HTTP 头部、以及其他的一些信息 Grafana 导入仪表盘我们可以自己编写仪表盘，也可以使用官方网站上别人已经写好的仪表盘模板直接导入使用，这里我们没有必要自己去编写(重复造轮子而且还没有人家专业…)。我们先去Grafana Labs上找到监控 Linux 主机的仪表盘，然后将仪表盘的 ID 号导入到 Grafana 查看仪表盘Grafana 监控 Linux 主机的仪表盘数据是从 prometheus 的数据源获取的，就是被监控主机上的 node_exporter 获取到的数据 监控 Docker 服务器部署 cadvisor 想要监控 Docker 容器，需要在被监控主机安装 cadvisor 插件，暴露一个 HTTP 端口，为 prometheus 提供容器的监控数据 # 由于国内无法连接到gcr.io,这里使用张馆长仓库的镜像地址 docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:ro \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ --privileged \\ --device=/dev/kmsg \\ registry.aliyuncs.com/k8sxio/cadvisor:latest 配置 Prometheus 监控 cadvisor cadvisor 可以搜集一台机器上所有运行的容器信息，还提供基础查询界面和 http 接口，供其他组件如 prometheus 拉取数据 vim /usr/local/prometheus/prometheus.yml # 在prometheus配置文件加入监控主机的cadvisor端口(拉取容器数据) - job_name: 'docker' static_configs: - targets: ['10.10.110.23:8080'] systemctl daemon-reload systemctl restart prometheus.service Grafana 导入仪表盘 我们去 Grafana Labs 网站寻找一个监控 Docker 主机的仪表盘，在 Grafana 进行导入 查看Docker主机仪表盘 监控 MySQL 服务器监控 MySQL 主机和监控 Linux 主机一样，都是需要导出器去获取数据，这里我们去 prometheus 官网下载 mysqld_exporter，然后在 mysql 主机上安装(监控那台 mysql 主机就在那台主机安装mysqld_exporter) MySQL 主机安装 mysqld_exporter # 下载 wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.12.1/mysqld_exporter-0.12.1.linux-amd64.tar.gz # 解压 tar xf mysqld_exporter-0.12.1.linux-amd64.tar.gz &amp;&amp; mv mysqld_exporter-0.12.1.linux-amd64 /usr/local/mysqld_exporter # 创建启动文件 vim /usr/lib/systemd/system/mysqld_exporter.service [Unit] Documentation=https://prometheus.io/ [Service] Restart=on-failure Environment=DATA_SOURCE_NAME=exporter:Missf.top123@(localhost:3306)/ # 连接数据库的账号密码,也可以指定.my.cnf文件 ExecStart=/usr/local/mysqld_exporter/mysqld_exporter [Install] WantedBy=multi-user.target # 被监控数据库添加mysql用户及监控权限 CREATE USER 'exporter'@'localhost' IDENTIFIED BY 'Missf.top123' WITH MAX_USER_CONNECTIONS 3; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'exporter'@'localhost'; FLUSH PRIVILEGES; # 启动mysqld_exporter systemctl start mysqld_exporter # 获取监控数据 curl [IP]:9104/metrics 配置Prometheus监控mysqld_exporter # 修改配置文件 vim /usr/local/prometheus/prometheus.yml - job_name: 'mysqld_exporter' # 添加监控mysqld_exporter static_configs: - targets: ['47.100.107.121:9104'] # 重启 systemctl restart prometheus.service 导入 MySQL 仪表盘 导入 ID 为 7362 的 MySQL 仪表盘，查看 MySQL 的监控数据","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/categories/Prometheus/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/tags/Prometheus/"},{"name":"监控","slug":"监控","permalink":"https://www.missf.top/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"Prometheus 配置文件与核心功能","slug":"Prometheus 配置文件与核心功能","date":"2020-07-15T11:06:15.000Z","updated":"2020-09-28T04:04:18.000Z","comments":true,"path":"post/521f1005.html","link":"","permalink":"https://www.missf.top/post/521f1005.html","excerpt":"","text":"全局配置文件介绍global: # 全局默认的数据拉取间隔,默认每隔1m拉取一次监控数据 [ scrape_interval: &lt;duration> | default = 1m ] # 全局默认的单次数据拉取超时 [ scrape_timeout: &lt;duration> | default = 10s ] # 对告警规则做定期计算的间隔时间,每隔1m对比一次我采集到的数据跟我设置的告警规则,符合告警规则的事件就会被发送到alertmanager,由alertmanager做路由匹配然后进行告警处理 [ evaluation_interval: &lt;duration> | default = 1m ] # 监控告警的规则设置 rule_files: [ - &lt;filepath_glob> ... ] # 配置被监控指标 scrape_configs: [ - &lt;scrape_config> ... ] # 指定告警和告警管理器相关的设置 alerting: alert_relabel_configs: [ - &lt;relabel_config> ... ] alertmanagers: [ - &lt;alertmanager_config> ... ] scrape_configs配置数据源，拉取数据的对象称为 Targets，每个 Targets 用 job_name 命名，添加数据源又分为静态配置和服务发现 # 定义job名称,是一个拉取单元,每个job_name都会自动引入默认配置如: # scrape_interval 依赖全局配置 # scrape_timeout 依赖全局配置 # metrics_path 默认为'/metrics' # scheme 默认为'http' job_name: &lt;job_name> # 数据拉取间隔 [ scrape_interval: &lt;duration> | default = &lt;global_config.scrape_interval> ] # 数据拉取超时时间 [ scrape_timeout: &lt;duration> | default = &lt;global_config.scrape_timeout> ] # 拉取数据指标的地址 [ metrics_path: &lt;path> | default = /metrics ] 基于文件的服务发现基于文件的服务发现不需要依赖其他平台与第三方服务，用户只需将要更新的 target 信息以 yaml 或 json 文件格式添加到 target 文件中，prometheus 会定期的从指定文件中读取 target 信息并更新。给我们带来的好处就是不需要一个个 target 去添加，只需要一个 yaml 或者 json 文件，便于管理 编写配置文件 vim prometheus.yml # my global config 全局配置文件 global: scrape_interval: 5s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration 告警管理 alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\" # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=&lt;job_name>` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # scrape_interval: 5s # static_configs: # - targets: ['localhost:9090'] file_sd_configs: # 文件服务发现 - files: ['/usr/local/prometheus/sd_config/*.yaml'] # 指定服务发现的文件路径 refresh_interval: 5s # 每过5秒动态发现服务配置 创建目录及文件 vim /usr/local/prometheus/sd_config/test.yaml # 需要监控那一台主机就在那一台主机上创建 - targets: - '10.10.110.150:9090' # 这个是填写prometheus主机的地址,如果prometheus启动时监听的是8080端口,那么这里就需要和prometheus端口一致,不然获取不到数据 labels: group: prometheus 重载配置文件 ps -ef | grep prometheus root 1774 1 0 Jul15 ? 00:02:21 /usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.yml root 2741 1702 0 14:13 pts/1 00:00:00 grep --color=auto prometheus kill -hup 1774","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/categories/Prometheus/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/tags/Prometheus/"},{"name":"监控","slug":"监控","permalink":"https://www.missf.top/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"Prometheus 部署","slug":"Prometheus 部署","date":"2020-07-15T09:11:11.000Z","updated":"2020-09-28T04:04:48.000Z","comments":true,"path":"post/d26dfcbe.html","link":"","permalink":"https://www.missf.top/post/d26dfcbe.html","excerpt":"","text":"Prometheus 二进制部署# 下载二进制安装包 wget https://github.com/prometheus/prometheus/releases/download/v2.19.2/prometheus-2.19.2.linux-amd64.tar.gz # 解压 tar xf prometheus-2.19.2.linux-amd64.tar.gz &amp;&amp; mv prometheus-2.19.2.linux-amd64 /usr/local/prometheus # 创建启动文件 cp /usr/lib/systemd/system/sshd.service /usr/lib/systemd/system/prometheus.service # 编写启动文件 tee /usr/lib/systemd/system/prometheus.service &lt;&lt; EOF [Unit] Description=http://prometheus.io [Service] Restart=on-failure ExecStart=/usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.yml [Install] WantedBy=multi-user.target EOF # 启动prometheus systemctl daemon-reload systemctl restart prometheus.service 修改配置文件vim /usr/local/prometheus/prometheus # my global config global: scrape_interval: 5s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\" # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=&lt;job_name>` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['127.0.0.1:9090'] Docker 部署docker run -d --name \"prometheus\" -p 9090:9090 \\ --mount src=prometheus,dst=/etc/prometheus \\ --mount type=bind,src=/prometheus/prometheus.yml,dst=/etc/prometheus/prometheus.yml prom/prometheus 启动常用命令行参数./prometheus -h --config.file=\"prometheus.yml\" # 指定配置文件 --web.listen-address=\"0.0.0.0:9090\" # 指定端口 --log.level=info # 指定日志级别","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/categories/Prometheus/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/tags/Prometheus/"},{"name":"监控","slug":"监控","permalink":"https://www.missf.top/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"Prometheus 概述","slug":"Prometheus 概述","date":"2020-07-14T06:48:33.000Z","updated":"2020-09-28T04:06:48.000Z","comments":true,"path":"post/ef0b21f0.html","link":"","permalink":"https://www.missf.top/post/ef0b21f0.html","excerpt":"","text":"Prometheus 简介Prometheus (普罗米修斯)是一个最初在 SoundCloud 上构建的监控系统。自2012年成为社区开源项目，拥有非常活跃的开发人员和用户社区。为强调开源及独立维护，Prometheus 于2016年加入云原生云计算基金会(CNCF)，成为继 Kubernetes 之后的第二个托管项目 可能有些运维小伙伴不知道 Prometheus，但是你们一定用过 zabbix。现在由于 Docker 和 Kubernetes 的兴起，zabbix 渐渐的失去了监控的优势，现在 Prometheus 是用来监控容器的最好实现，只有用到 Docker 和 Kubernetes 就离不开Prometheus提供监控支持。以前刚接触 zabbix 时，配置的微信告警让我开心了一整天，那时候觉得 zabbix 是世界上最好的监控软件，但是现在却觉得 Prometheus 才是。可能人总是需要不断向前看、不断向前奔跑的吧 prometheus官网 Prometheus 特点 多维数据模型(由时序列数据metric和一组key/value组成) 使用多维度数据完成复杂的语言查询，为 prometheus 的后期发展奠定基础(PromSQL) 不依赖分布式存储，单个服务器节点可直接工作 通过 pushgateway 进行时序列数据推送(pushing) 可以通过服务发现或者静态配置去获取要采集的目标服务器 多种可视化图表及仪表盘支持 基于 HTTP 的 pull 方式采集时间序列数据 Prometheus 组成及架构Prometheus 根据配置定时去拉取各个节点的数据，默认使用的拉取方式是 pull，也可以使用 pushgateway 提供的 push 方式获取各个监控节点的数据。将获取到的数据存入TSDB(时序型数据库)，此时 prometheus 已经获取到监控数据，可以使用内置的 promSQL 进行查询。它的报警功能使用 alertmanager 提供，alertmanager 是prometheus的告警管理和发送报警的一个组件。prometheus 原生的图表结构过于简单，prometheus 的图表展示功能一般由 grafana 进行统一管理 Prometheus 数据模型Prometheus 将所有数据存储为时间序列，具有相同度量名称以及标签属于同一个指标。每个时间序列都由度量标准名称和一组键值对(也成为标签)唯一标识 # 时间序列格式示例 &lt;metric name>{&lt;label name>=&lt;label value>, ...} api_http_requests_total{method=\"POST\", handler=\"/messages\"} Prometheus 指标类型Counter：递增的计数器 Gauge：可以任意变化的数值 Histogram：对一段时间范围内数据进行采样，并对所有数值求和与统计数量 Summary：与Histogram类似 不同的指标类型用于渲染不同的图表 Prometheus 作业和实例实例：可以抓取的目标称为实例(Instances) 作业：具有相同目标的实例集合称为作业(Job) scrape_configs: - job_name: 'prometheus' # prometheus这个job作用于localhost:9090这个目标 static_configs: - targets: ['localhost:9090'] - job_name: 'node' # node这个job作用于192.168.1.10:9090这个目标 static_configs: - targets: ['192.168.1.10:9090']","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/categories/Prometheus/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/tags/Prometheus/"},{"name":"监控","slug":"监控","permalink":"https://www.missf.top/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"Prometheus 监控的意义","slug":"Prometheus 监控的意义","date":"2020-07-13T17:28:39.000Z","updated":"2020-09-28T04:07:34.000Z","comments":true,"path":"post/ded057ed.html","link":"","permalink":"https://www.missf.top/post/ded057ed.html","excerpt":"","text":"监控目的监控分为白盒监控和黑盒监控。白盒监控: 通过监控内部的运行状态及指标判断接下来可能会发生的问题，从而做出预判或应对的方法。黑盒监控: 监控系统或服务，在发生异常时做出相应的措施。prometheus 属于黑盒监控，是在服务发生异常时，我们通过告警信息得知，才去处理异常问题 监控的目的主要分为以下方面: 根据历史监控数据，对未来做出预测 发生异常时即使告警，或做出相应措施 根据监控报警及时定位问题根源，记录问题出现的证据(记录网络波动) 通过可视化图表展示，便于直观获取信息 领导查看数据图表(PV、UV、订单趋势图) 运维人员能够提前预知风险，避免故障的产生或者在故障发生时能够迅速处理 怎么监控使用传统监控工具，直接调用 Linux 系统命令去获取服务状态和信息 # free # vmstat # df # top # ss # iftop ... 使用监控系统去监控系统和服务，能够整体监控每一项数据 # zabbix # nagios # prometheus # open-falcon 监控流程监控的大概流程分为：数据采集、数据存储、数据分析、以及展示和告警 监控什么 监控类型 具体参数 硬件监控 硬件参数、温度、故障等 系统监控 CPU，内存，硬盘，网卡流量，TCP状态，进程数 应用监控 Nginx、Tomcat、PHP、MySQL、Redis等 日志监控 系统日志、服务日志、访问日志、错误日志 安全监控 WAF，敏感文件监控 API监控 可用性，接口请求，响应时间 业务监控 例如电商网站，每分钟产生多少订单、注册多少用户、多少活跃用户、推广活动效果 流量分析 根据流量获取用户相关信息，例如用户地理位置、某页面访问状况、页面停留时间等","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/categories/Prometheus/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/tags/Prometheus/"},{"name":"监控","slug":"监控","permalink":"https://www.missf.top/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"Docker Compose 单机编排利器","slug":"Docker Compose 单机编排利器","date":"2020-07-10T06:27:09.000Z","updated":"2020-09-28T03:28:40.000Z","comments":true,"path":"post/34699079.html","link":"","permalink":"https://www.missf.top/post/34699079.html","excerpt":"","text":"Docker Compose 概述Compose 是用于定义和运行多容器的工具，通过 Compose 可以使用 YAML 文件来配置容器。然后使用一个命令就可以从配置中创建并启动所有服务。其实在刚学习Docker 时我就想过，如果我是 LNMP 架构容器化项目，因为每次都要一个个容器的启动，是否有必要将启停多个容器的命令写成一个 shell 脚本呢。现在学到 Docker Compose，才知道根本没有这个必要，我们现在所有能想到的东西，其实早就有人帮我们实现了。这里不得不敬佩那些为开源项目做出贡献的伟大开发者们 使用 Compose 大概分为三个步骤： 定义 Dockerfile，以便可以在任意环境运行 定义应用程序启动配置文件 docker-compose.yml docker-compose 启动并管理整个应用程序生命周期 Linux 安装 Compose其实前面我们在学习 Harbor 时已经安装过 docker-compose，这是一个使用 python 开发的编排工具，国内下载可能会比较慢(你应该知道怎么做了吧…) curl -L \"https://github.com/docker/compose/releases/download/1.26.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/bin/docker-compose chmod +x /usr/bin/docker-compose docker-compose.yaml 配置文件参数build 使用 docker-compose 启动容器服务除了可以基于指定的镜像，还可以基于一份 Dockerfile，在使用 up 启动时执行构建镜像的任务，这个构建标签就是 build。Compose 将会利用它自动构建这个镜像，然后使用这个镜像启动服务容器 version:\"3.7\" services: web: context:./web dockerfile:Dockerfile context context 选项可以是 Dockerfile 的文件路径，也可以是到链接到 git 仓库的 url。当提供的值是相对路径时，它被解析为相对于撰写文件的路径，此目录也是发送到 Docker守护进程的 context build: context:./dir dockerfile 使用此 dockerfile 文件来构建，必须使用 context 指定构建路径 build: context:. # 知道dockerfile必须要有构建路径,.表示当前路径 dockerfile:Dockerfile-alternate image 指定 docker-compose 启动容器服务的镜像，可以是存储仓库、标签以及镜像 ID，如果镜像不存在，Compose 会自动拉去镜像 image: redis image: ubuntu:14.04 image: tutum/influxdb image: example-registry.com:4000/postgresql image: a4bc65fd command 覆盖容器启动后默认执行的命令 command:bundle exec thin -p 3000 command:[\"bundle\",\"exec\",\"thin\",\"-p\",\"3000\"] container_name 指定容器名称，由于容器名称是唯一的，如果指定自定义名称，则无法使用 scale container_name:my-web-container environment 添加环境变量，可以使用数组或字典。这个标签的作用是设置镜像变量，它可以保存变量到镜像里面，类似 ENV 指令一样会把变量一直保存在镜像、容器中 environment: RACK_ENV:development SHOW:'true' SESSION_SECRET: environment: -RACK_ENV=development -SHOW=true -SESSION_SECRET networks 加入指定网络 networks: - lnmp ports 映射端口 ports: -\"3000\" -\"3000-3005\" -\"8000:8000\" -\"9090-9091:8080-8081\" -\"49100:22\" -\"127.0.0.1:8001:8001\" -\"127.0.0.1:5000-5010:5000-5010\" # 指定IP+端口的话只会监听ipv4的地址 -\"6060:6060/udp\" expose 暴露端口，但不映射到宿主机，只被连接的服务访问。这个标签与 Dockerfile 中的 EXPOSE 指令一样，用于指定暴露的端口，实际上 docker-compose.yml 的端口映射还得 ports 这样的标签 extra_hosts 添加主机名的标签，就是往 /etc/hosts 文件中添加一些记录，与 Docker 客户端中的 –add-host 类似 extra_hosts: -\"www.missf.top:124.156.205.241\" -\"mf_missf.gitee.io:212.64.62.174\" volumes 挂载一个目录或者一个已存在的数据卷容器 volumes: - /opt/data:/var/lib/mysql # 挂载宿主机的/opt/data目录到容器的/var/lib/mysql - datavolume:/var/lib/mysq # 将容器的/var/lib/mysq挂载到datavolume数据卷 restart 默认值为 no ，即在任何情况下都不会重新启动容器。当值为 always 时，容器总是重新启动。当值为 on-failure 时，当出现 on-failure 报错容器退出时，容器重新启动 restart: \"no\" restart: always restart: on-failure restart: unless-stopped hostname 定义容器主机名 hostname: foo Compose 常用选项与命令up 命令：该命令十分强大，它将尝试自动完成包括构建镜像、创建服务、启动服务、并关联服务相关容器的一系列操作 -d：在后台运行服务容器 –force-recreate：强制重新创建容器，不能与 –no-recreate 同时使用 –no-recreate：如果容器已经存在了，则不重新创建，不能与 –force-recreate 同时使用 -no-build：不自动构建缺失的服务镜像 –no-deps：不启动服务所链接的容器 build 命令：可以随时在项目目录下运行 docker-compose build 来重新构建服务 –force-rm：删除构建过程中的临时容器 –no-cache：构建镜像过程中不使用 cache(这将加长构建过程) –pull：始终尝试通过 pull 来获取更新版本的镜像 ps 命令：列出项目中目前的所有容器 -q：只打印容器的 ID 信息 logs 命令：查看服务容器的输出，默认情况下，docker-compose 将对不同的服务输出使用不同的颜色来区分 docker-compose logs [选项] rm命令：删除所有(停止状态的)服务容器，推荐先执行 docker-compose stop 命令来停止容器 -f/–force：强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项 -v：删除容器所挂载的数据卷 scale 命令：设置指定服务运行的容器个数 docker-compose scale web=3 db=2 down 命令：删除容器、网络 start/stop/restart 命令：启动/停止/重启服务 docker-compose 编排 lnmp 环境docker-compose 目录设计 tree /docker-compose_lnmp/ /docker-compose_lnmp/ ├── docker-compose.yaml ├── mysql │ └── start ├── nginx │ ├── Dockerfile │ ├── nginx-1.16.1.tar.gz │ ├── nginx.conf │ ├── php.conf │ └── start └── php ├── Dockerfile ├── php-7.4.0.tar.gz ├── php-fpm.conf ├── php.ini ├── start └── www.conf 编写 docker-compose.yaml version: '3' services: php: hostname: php build: context: ./php dockerfile: Dockerfile networks: - \"lnmp\" volumes: - \"nginx:/usr/local/nginx/html\" nginx: hostname: nginx build: context: ./nginx dockerfile: Dockerfile ports: - \"80:80\" - \"443:443\" networks: - \"lnmp\" volumes: - \"nginx:/usr/local/nginx/html/\" mysql: hostname: mysql image: mysql:5.7 ports: - \"53306:3306\" networks: - \"lnmp\" volumes: - \"mysql:/var/lib/mysql/\" command: --character-set-server=utf8mb4 environment: MYSQL_ROOT_PASSWORD: mwj123456 MYSQL_DATABASE: wordpress networks: lnmp: {} volumes: nginx: {} # 把php代码放到这个数据卷的目录下 mysql: {} docker-compose 编排 nginx 反向代理 tomcat 集群docker-compose 目录设计 tree /docker-compose_lnmt/ /docker-compose_lnmt/ ├── docker-compose.yaml ├── nginx │ ├── Dockerfile │ ├── nginx-1.16.1.tar.gz │ ├── nginx.conf │ ├── start │ └── tomcat.conf └── tomcat ├── apache-tomcat-8.5.57.tar.gz ├── Dockerfile ├── jdk-8u211-linux-x64.tar.gz └── start 编写 docker-compose.yaml cat docker-compose.yaml version: '3' services: nginx: hostname: nginx build: context: ./nginx dockerfile: Dockerfile ports: - \"80:80\" - \"443:443\" networks: - \"lnmt\" volumes: - \"webapps:/usr/local/tomcat/webapps/\" tomcat1: hostname: tomcat1 build: context: ./tomcat dockerfile: Dockerfile networks: - \"lnmt\" volumes: - \"webapps:/usr/local/tomcat/webapps/\" tomcat2: hostname: tomcat2 build: context: ./tomcat dockerfile: Dockerfile networks: - \"lnmt\" volumes: - \"webapps:/usr/local/tomcat/webapps/\" tomcat3: hostname: tomcat3 build: context: ./tomcat dockerfile: Dockerfile networks: - \"lnmt\" volumes: - \"webapps:/usr/local/tomcat/webapps/\" mysql: hostname: mysql image: mysql:5.7 ports: - \"53306:3306\" networks: - \"lnmt\" volumes: - \"mysql:/var/lib/mysql/\" command: --character-set-server=utf8mb4 environment: MYSQL_ROOT_PASSWORD: mwj123456 MYSQL_DATABASE: test volumes: webapps: {} # 把war包放到这个数据卷的目录下,就会自动解压 mysql: {} networks: lnmt: {} 监听 Nginx 容器访问日志 tail -f /usr/local/nginx/logs/access.log # 点击浏览器刷新页面,可以看到upstream_addr的IP变化,这样就实现了反向代理Tomcat集群 {\"@timestamp\": \"2020-07-14T08:03:19+08:00\", \"clientRealIp\": \"10.10.110.1\", \"scheme\": \"http\", \"method\": \"GET\", \"host\": \"10.10.110.150\", \"url\": \"/\", \"size\": 1056, \"referrer\": \"-\", \"agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\", \"upstream_addr\": \"192.168.16.3:8080\", \"request_time\": 0.323, \"request_length\": 450, \"upstream_connect_time\": \"0.003\", \"upstream_response_time\": \"0.324\", \"upstream_status\": \"200\", \"status\": \"200\"} {\"@timestamp\": \"2020-07-14T08:03:28+08:00\", \"clientRealIp\": \"10.10.110.1\", \"scheme\": \"http\", \"method\": \"GET\", \"host\": \"10.10.110.150\", \"url\": \"/\", \"size\": 1056, \"referrer\": \"-\", \"agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\", \"upstream_addr\": \"192.168.16.6:8080\", \"request_time\": 0.345, \"request_length\": 450, \"upstream_connect_time\": \"0.000\", \"upstream_response_time\": \"0.345\", \"upstream_status\": \"200\", \"status\": \"200\"} {\"@timestamp\": \"2020-07-14T08:03:29+08:00\", \"clientRealIp\": \"10.10.110.1\", \"scheme\": \"http\", \"method\": \"GET\", \"host\": \"10.10.110.150\", \"url\": \"/\", \"size\": 1056, \"referrer\": \"-\", \"agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\", \"upstream_addr\": \"192.168.16.4:8080\", \"request_time\": 0.355, \"request_length\": 450, \"upstream_connect_time\": \"0.000\", \"upstream_response_time\": \"0.354\", \"upstream_status\": \"200\", \"status\": \"200\"}","categories":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker 图形化页面管理","slug":"Docker 图形化页面管理","date":"2020-07-10T02:03:55.000Z","updated":"2020-09-28T03:22:56.000Z","comments":true,"path":"post/92368be2.html","link":"","permalink":"https://www.missf.top/post/92368be2.html","excerpt":"","text":"Portainer 概述Portainer 是 Docker 的图形化管理工具，portainer 通过连接 /var/run/docker.sock 文件去管理容器，可让你轻松管理不同的 Docker 环境 (Docker主机或Swarm群集)。Portainer 提供状态显示面板、应用模板快速部署、容器镜像网络数据卷、事件日志显示、容器控制台操作、登录用户管理和控制等功能。Docker 图形化管理界面有很多实现的工具，但生态一直不温不火，这是由于 Docker 的很多操作都是直接在命令行进行，再加上 Docker 的操作也比较简单。一般这样的图形化管理平台都是交给开发和测试人员去使用的 Portainer 安装docker run -d -p 8000:8000 -p 9000:9000 --name \"portainer\" --restart=always \\ -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer # 这里portainer通过连接/var/run/docker.sock文件去管理容器,所以需要把宿主机的docker.sock文件挂载到portainer 登录 Portainer 页面 Portainer 连接容器的方式 Local：管理 Portainer 所在主机上的 Docker 主机，需要将宿主机的 docker.sock 文件挂载到 Portainer 容器内 Remote：管理远程主机上的 Docker 主机，但是要开启远程的 Docker 主机的 Docker API，允许 Portainer 通过 TCP 连接 Agent：直接连接到在 Swarm 集群中运行的 Portainer 代理 Azure：连接到 Microsoft Azure 这里我们先使用 Local 的方式连接到 Portainer 所在的主机 Portainer 管理界面通过下图可以看到 Portainer 提供了对容器、镜像、网络、数据卷、变量、主机的操作，App templates 是一些供我们下载的公共镜像，我们还可以看到正在运行的容器状态、日志、基于镜像、创建时间、映射端口等 Portainer 连接远程 Docker 主机首先需要在远程 Docker 主机上开启 Docker API vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 # 表示所有地址都能连接到Docker API,也可以指定IP连接,默认端口是2375 systemctl daemon-reload systemctl restart docker.service 然后在 Portainer 再创建一个连接远程 Docker 主机 API 的节点 这时候我们可以使用 Portainer 去管理本地和远程主机上的 Docker 资源了","categories":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker 企业级镜像仓库Harbor","slug":"Docker 企业级镜像仓库 Harbor","date":"2020-07-08T05:33:50.000Z","updated":"2020-09-28T03:09:28.000Z","comments":true,"path":"post/d46af348.html","link":"","permalink":"https://www.missf.top/post/d46af348.html","excerpt":"","text":"Harbor 概述Harbor 是由 VMWare 公司开源的容器镜像仓库。事实上，Harbor 是在 Docker Registry 上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP 集成以及审计日志等，足以满足基本企业需求 Harbor 官网 Harbor GitHub 地址 Harbor 部署条件服务器硬件配置 最低要求：CPU2核/内存4G/硬盘40GB 推荐：CPU4核/内存8G/硬盘160GB 软件 Docker 17.06 版本+ Docker Compose 1.18 版本+ 安装方式 在线安装：从 Docker Hub 下载 Harbor 相关镜像，因此安装软件包非常小 离线安装：安装包包含部署的相关镜像，因此安装包比较大 docker-compose 安装下载二进制文件 https://github.com/docker/compose/releases # docker-compose下载地址 # 下载docker-compose-Linux-x86_64这个二进制文件 配置二进制文件 mv docker-compose-Linux-x86_64 /usr/bin/docker-compose chmod +x /usr/bin/docker-compose docker-compose --help # 安装完成 Harbor HTTP部署下载 Harbor 安装包 wget https://github.com/goharbor/harbor/releases/download/v2.0.1/harbor-offline-installer-v2.0.1.tgz 解压安装包 tar xf harbor-offline-installer-v2.0.1.tgz 修改配置文件 cp harbor.yml.tmpl harbor.yml vim harbor.yml hostname: reg.missf.com # 修改Harbor默认域名 https: # 先注释https相关配置 harbor_admin_password: MF-yihan # 修改Harbor的密码 部署 Harbor ./prepare # 做一系列的准备工作 ./install.sh # 利用docker-compose拉取一系列的镜像,安装好之后就会直接启动 访问 Harbor # 通过本地电脑配置hosts,然后在浏览器访问我们的域名reg.missf.com 登录 Harbor vim /etc/hosts # 添加解析,登录时可以直接访问域名 10.10.110.151 reg.missf.com vim /etc/docker/daemon.json # 配置域名可信任,因为现在没有配置https,而docker默认是使用https协议去连接的,不配置不能登录成功 { \"registry-mirrors\": [\"https://265wemgl.mirror.aliyuncs.com\"], \"insecure registries\": [\"reg.missf.com\"] } systemctl restart docker.service # 修改了daemon.json需要重启docker docker-compose down &amp;&amp; docker-compose up -d # 重启docker之后容器有些会退出,重启harbor重启把容器拉起来 docker login reg.missf.com # 登录成功 Username: admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 推送镜像到 Harbor 仓库 docker tag nginx:1.0 reg.missf.com/library/nginx:1.0 # 推送之前修改镜像的标签(镜像中心/项目/镜像:标签) docker push reg.missf.com/library/nginx:1.0 # 推送镜像，pull拉取镜像也是使用这个标签去拉取 The push refers to repository [reg.missf.com/library/nginx] b1b653ec37ba: Pushed fe503a975c26: Pushed 60165efe909a: Pushed e098d2f9f0dd: Pushed ae9b67129281: Pushed d2039520c249: Pushed 034f282942cd: Pushed 1.0: digest: sha256:a4c155ecb6b7eee5d332764057c29a74d8965de19f9d739f1792cf479c2bf030 size: 1786 查看 Harbor 上推送成功的镜像 Harbor HTTPS部署由于 Harbor 不附带任何证书，它默认使用 HTTP 来提供注册表请求。但是强烈建议为生产环境配置 ssl 证书。这里我们由于是实验测试，使用自签名证书，到时候生产环境配置可以去阿里云购买 ssl 证书 生成自签名 ssl 证书，由于 kubernetes 使用 cfssl 自签证书，这里我们也使用 cfssl 生成自签证书 # 执行这个脚本,安装cfssl并将命令放到/usr/bin/下供我们直接使用 cat cfssl.sh wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 chmod +x cfssl* mv cfssl_linux-amd64 /usr/bin/cfssl mv cfssljson_linux-amd64 /usr/bin/cfssljson mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo # 执行这个脚本,生成两个json的ca配置文件并自动生成证书,cfssl是根据json的配置文件去生成ca证书的 cat certs.sh cat > ca-config.json &lt;&lt;EOF { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ] } } } } EOF cat > ca-csr.json &lt;&lt;EOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"L\": \"Beijing\", \"ST\": \"Beijing\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca - # 初始化ca配置文件 cat > reg.missf.com-csr.json &lt;&lt;EOF { \"CN\": \"reg.missf.com\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"L\": \"BeiJing\", \"ST\": \"BeiJing\" } ] } EOF cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes reg.missf.com-csr.json | cfssljson -bare reg.missf.com # 生成ca证书 # 执行完上面两个脚本之后我们会得到下面这两个文件 reg.missf.com-key.pem reg.missf.com.pem Harbor 启用 HTTPS https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /root/ssh/reg.missf.com.pem private_key: /root/ssh/reg.missf.com-key.pem 重新配置并部署Harbor systemctl restart docker.service ./prepare docker-compose down docker-compose up –d # 重新打开Harbor页面会自动跳转到https,但是由于是自签证书,所以仍会显示不安全 其他 Docker 主机连接 Harbor 仓库 一般 Harbor 仓库都是自己公司内部使用,但是有时候也会开放给别的 Docker 主机去 pull 镜像，如果其他的 Docker 主机需要连接 Harbor，必须要有证书才能连接 # 复制Harbor主机的证书到需要连接Harbor仓库的Docker主机上 mkdir -p /etc/docker/certs.d/reg.missf.com/ # 在Docker主机上创建目录 cp reg.missf.com.pem /etc/docker/certs.d/reg.missf.com/reg.missf.com.crt # 将Harbor主机的证书复制到Docker主机 echo \"10.10.110.151 reg.missf.com\" >> /etc/hosts # 这里由于是实验环境,需要配置域名解析 docker login reg.missf.com # 在其他的docker主机登录到Harbor,就可以pull拉取Harbor仓库的镜像了 Username: admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Harbor主从复制的三种方式学习过 MySQL 主从的同学可以看出，其实 Harbor 的主从复制和 MySQL 的主从复制方式非常相似 主节点的仓库管理新建目标 新建一个目标，就代表本地 Harbor 可以连接到这个远程 Harbor，当我们配置复制管理的目的 Registry 时，可以从新建目标里面填写复制镜像到那个 Harbor 节点 主节点的复制管理新建规则 配置复制模式和目的 Registry，将本地 Harbor 主节点上的镜像(可以使用过滤器进行选择性推送)推送到备用 Harbor 节点上 推送验证 这时候只有有镜像被推送到 Harbor 的主节点，那么 Harbor 主节点就会把镜像 push 到 Harbor 的备用节点，可以查看复制记录 Harbor 运行维护Harbor 容器功能介绍 容器 功能 harbor-core 配置管理中心 harbor-jobservice 负责镜像复制 harbor-log 记录操作日志 harbor-portal Web管理页面和API harbor-db PG数据库 registryctl 镜像存储 nginx 前端代理，负责前端页面和镜像上传/下载转发 redis 会话 Harbor 容器数据持久化目录：/data (这个目录需要定时备份) 日志文件目录：/var/log/harbor","categories":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Dockerfile 定制容器镜像","slug":"Dockerfile 定制容器镜像","date":"2020-06-30T05:33:50.000Z","updated":"2021-01-20T10:13:38.000Z","comments":true,"path":"post/44241b5a.html","link":"","permalink":"https://www.missf.top/post/44241b5a.html","excerpt":"","text":"Dockerfile 介绍Dockerfile 是由一行一行的命令语句组成，并且从上到下执行，支持以#注释行。一般 Dockerfile 的内容分为四个部分，基础镜像信息、维护者信息、镜像操作指令、容器启动时执行指令 Dockerfile 常用指令 指令 描述 FROM 指定构建新镜像时是基于那个镜像，Dockerfile的第一条指令必须为FROM指令，如果在同一个Dockerfile中创建多个镜像可以使用多个FROM指令 LABEL 为镜像添加标签 RUN 每条RUN指令将在当前镜像的基础上执行指定shell命令，并提交为新的镜像 COPY 拷贝宿主机(Dockerfile所在目录的相对路径)的文件或目录到镜像中 ADD 复制指定的&lt;src&gt;到容器中的&lt;dest&gt;，&lt;src&gt;可以是Dockerfile所在目录的文件或目录，可以是一个URL，还可以是一个tar文件(自动解压缩) ENV 指定一个环境变量，会被后续RUN指令使用，并在容器运行时保持 USER 指定运行容器时的用户名或UID，后续的RUN也会使用指定用户 EXPOSE 声明容器运行的服务端口，启动容器时可以将这些端口转发到宿主机或者指定宿主机那个端口映射过来 WORKDIR 为后续的RUN、CMD、ENTRYPOINT指令配置工作目录 VOLUME 在镜像中创建挂载点，这样只要通过该镜像创建的容器都有了挂载点，查看容器详细信息可以看到容器挂载点映射到宿主机的目录 CMD 容器启动时执行指令，每个Dockerfile只能有一条CMD指令，如果有多个CMD指令只有最后一个生效 ENTRYPOINT ENTRYPOINT如果与CMD一起使用，CMD将作为ENTRYPOINT的默认参数，如果有多个ENTRYPOINT指令只有最后一个生效 构建镜像Dockerfile demo # This dockerfile demo for project build to docker images FROM centos:7 LABEL maintainer www.missf.top USER root RUN yum install -y nginx EXPOSE 80 443 VOLUME [\"/usr/local/nginx/\"] CMD [\"/usr/local/nginx/bin\"] Docker build构建镜像 # 在Dockerfile所在的目录下构建镜像,后面的\".\"表示当前目录 docker build -t demo:1.0 . # 构建过程如下 Sending build context to Docker daemon 2.048kB Step 1/8 : FROM centos:7 7: Pulling from library/centos 524b0c1e57f8: Pull complete Digest: sha256:e9ce0b76f29f942502facd849f3e468232492b259b9d9f076f71b392293f1582 Status: Downloaded newer image for centos:7 ---> b5b4d78bc90c Step 2/8 : LABEL maintainer mownejie ---> Running in 7dbcab7ef3ce Removing intermediate container 7dbcab7ef3ce ---> 4db1e9da6977 Step 3/8 : ENV JAVA_HOME /usr/local/java ---> Running in b896cedee458 Removing intermediate container b896cedee458 ---> f8991838d97e Step 4/8 : USER root ---> Running in 8252457198f0 Removing intermediate container 8252457198f0 ---> 96ef213928ad Step 5/8 : RUN yum install -y nginx ---> Running in 8807973810c5...... # -t 指定这个镜像的tag # -f 指定这个Dockerfile文件的位置 CMD 与 ENTRYPOINT 区别CMD用法 # exec形式,首选形式,传参不支持引用变量 CMD [\"executable\", \"param1\", \"param2\"] # CMD作为ENTRYPOINT的默认参数 CMD [\"param1\", \"param2\"] # Shell形式 CMD command param1 param2 ENTRYPOINT用法 ENTRYPOINT [\"executable\", \"param1\", \"param2\"] # 假如配合CMD一起使用,那么[\"param1\", \"param2\"]可以写在CMD作为ENTRYPOINT的默认参数 ENTRYPOINT command param1 param2 总结 1. CMD和ENTRYPOINT指令都可以用来定义运行容器时所使用的命令 2. Dockerfile至少指定一个CMD或ENTRYPOINT 3. CMD可以用作ENTRYPOINT默认参数，或者用作容器的默认命令 4. docker run启动容器时指定&lt;command>，将会覆盖dockerfile定义的CMD 构建 Nginx 容器镜像Dockerfile 内容 FROM centos:7.7.1908 LABEL maintainer www.missf.top RUN yum install -y gcc gcc-c++ make \\ openssl-devel pcre-devel gd-devel \\ iproute net-tools telnet wget curl && \\ yum clean all && rm -rf /var/cache/yum/* ADD nginx-1.16.1.tar.gz / RUN cd nginx-1.16.1 && \\ ./configure --user=nginx --group=nginx \\ --prefix=/usr/local/nginx \\ --with-http_stub_status_module \\ --with-http_ssl_module \\ --with-http_gzip_static_module \\ --with-http_sub_module && \\ make -j4 && make install && \\ mkdir /usr/local/nginx/conf/vhost && \\ cd / && rm -rf nginx* && \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN useradd -s /sbin/nologin nginx ENV PATH $PATH:/usr/local/nginx/sbin ENV LANG=\"en_US.utf8\" COPY nginx.conf /usr/local/nginx/conf/nginx.conf COPY php.conf /usr/local/nginx/conf/vhost/php.conf WORKDIR /usr/local/nginx EXPOSE 80 443 CMD [\"nginx\", \"-g\", \"daemon off;\"] 目录结构 [root@localhost /Dockerfile/nginx]# ll total 1028 -rw-r--r-- 1 root root 890 Jul 6 18:58 Dockerfile -rw-r--r-- 1 root root 1032630 Jan 14 09:53 nginx-1.16.1.tar.gz -rw-r--r-- 1 root root 3297 Jul 6 18:46 nginx.conf -rw-r--r-- 1 root root 362 Jul 6 20:13 php.conf -rw-r--r-- 1 root root 128 Jul 6 18:51 start 构建PHP容器镜像Dockerfile 内容 FROM centos:7.7.1908 LABEL maintainer www.missf.top RUN yum install -y epel-release && \\ yum install -y sqlite-devel libmcrypt-devel mhash-devel libxslt-devel \\ libjpeg-devel libpng libpng-devel freetype freetype-devel \\ libxml2 libxml2-devel zlib zlib-devel glibc glibc-devel libjpeg \\ glib2 glib2-develbzip2 bzip2-devel ncurses ncurses-devel \\ curl-devel e2fsprogs e2fsprogs-devel krb5 gcc krb5-devel libidn \\ openssl-devel libsqlite3x-devel oniguruma-devel openssl libidn-devel \\ iproute net-tools telnet wget curl && \\ yum clean all && rm -rf /var/cache/yum/* ADD php-7.4.0.tar.gz / RUN cd /php-7.4.0 && \\ ./configure --prefix=/usr/local/php \\ --with-config-file-path=/usr/local/php/etc \\ --enable-opcache --with-curl --enable-fpm \\ --enable-gd --with-iconv --enable-mbstring \\ --with-mysqli --with-openssl --enable-static \\ --enable-sockets --enable-inline-optimization \\ --with-zlib --disable-ipv6 --disable-fileinfo \\ --with-mcrypt --enable-hash --with-jpeg-dir --with-png-dir \\ --with-freetype-dir --with-pdo-mysql --disable-debug && \\ make -j 4 && make install && \\ cp /php-7.4.0/php.ini-production /usr/local/php/etc/php.ini && \\ cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.conf && \\ cp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.conf && \\ sed -i \"90a \\daemonize = no\" /usr/local/php/etc/php-fpm.conf && \\ mkdir /usr/local/php/log && \\ cd / && rm -rf php* && \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ENV PATH $PATH:/usr/local/php/sbin ENV LANG=\"en_US.utf8\" COPY php.ini /usr/local/php/etc/ COPY php-fpm.conf /usr/local/php/etc/ COPY www.conf /usr/local/php/etc/php-fpm.d/ WORKDIR /usr/local/php EXPOSE 9000 CMD [\"php-fpm\"] 目录结构 [root@localhost /Dockerfile/php]# ll total 16144 -rw-r--r-- 1 root root 1758 Jul 6 18:53 Dockerfile -rw-r--r-- 1 root root 16418792 Jul 1 10:39 php-7.4.0.tar.gz -rw-r--r-- 1 root root 5394 Jul 1 21:51 php-fpm.conf -rw-r--r-- 1 root root 72953 Jul 1 22:09 php.ini -rw-r--r-- 1 root root 93 Jul 6 18:56 start -rw-r--r-- 1 root root 19616 Jul 6 18:53 www.conf 容器化搭建个人博客自定义网络 docker network create lnmp # 将多个容器加入到一个自定义网络 创建 MySQL 容器 docker volume create mysql docker run -e MYSQL_ROOT_PASSWORD=mwj123456 -e MYSQL_DATABASE=wordpress -p 53306:3306 --name \"mysql\" --network lnmp --mount src=mysql,dst=/var/lib/mysql/ -d mysql:5.7 # 将MySQL数据库的数据持久化到mysql这个数据卷 创建 PHP 容器 docker volume create nginx docker run --name php --network lnmp --mount src=nginx,dst=/usr/local/nginx/html/ -d php:1.0 # 这里先启动PHP容器再启动Nginx容器,因为Nginx要去连接PHP容器,如果PHP容器没有启动,那Nginx就因为无法连接到PHP所有退出了 # 这里需要把Nginx代码也挂载到PHP容器内,而且容器内的路径要与Nginx配置文件路径一致 # 因为Nginx配置文件将所有*.php的请求都通过fastcgi_pass代理到PHP容器去处理,所有需要把代码也挂载到PHP容器内,不然访问php文件会提示未找到文件 创建 Nginx 容器 docker container run --name \"nginx\" --mount src=nginx,dst=/usr/local/nginx/html --network lnmp -p 80:80 -p 443:443 -d nginx:1.0 部署 WordPress 代码 docker volume inspect nginx # 先查看数据卷在宿主机上的目录,然后把代码解压到对应的目录下 tar xf wordpress-5.4.2-zh_CN.tar.gz -C /var/lib/docker/volumes/nginx/_data/ # 这时候通过访问宿主机的IP就能看到WordPress的安装页面了,如果无法对wp-config.php文件写入,就手动创建并写入 构建 Tomcat 容器镜像Dockerfile 内容 FROM centos:7.7.1908 LABEL maintainer www.missf.top ADD jdk-8u211-linux-x64.tar.gz /usr/local/ ADD apache-tomcat-8.5.57.tar.gz /usr/local/ RUN mv /usr/local/jdk1.8.0_211 /usr/local/jdk && \\ mv /usr/local/apache-tomcat-8.5.57 /usr/local/tomcat && \\ rm -rf /usr/local/tomcat/webapps/* ENV JAVA_HOME /usr/local/jdk ENV CLASSPATH ${JAVA_HOME}/lib/dt.jar:${JAVA_HOME}/lib/tools.jar ENV CATALINA_HOME /usr/local/tomcat ENV PATH $PATH:${JAVA_HOME}/bin:${CATALINA_HOME}/lib:${CATALINA_HOME}/bin RUN sed -i '1a JAVA_OPTS=\"-Djava.security.egd=file:/dev/./urandom\"' ${CATALINA_HOME}/bin/catalina.sh && \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime WORKDIR ${CATALINA_HOME} EXPOSE 8080 CMD [\"catalina.sh\", \"run\"] 目录结构 [root@localhost /Dockerfile/tomcat]# ll total 200568 -rw-r--r-- 1 root root 10379806 Jul 7 11:19 apache-tomcat-8.5.57.tar.gz -rw-r--r-- 1 root root 728 Jul 7 19:41 Dockerfile -rw-r--r-- 1 root root 194990602 Jul 2 2019 jdk-8u211-linux-x64.tar.gz 部署测试代码 docker volume inspect tomcat # 查看Tomcat容器代码目录持久化到宿主机的目录 ll /var/lib/docker/volumes/tomcat/_data # 放到这个目录的war包会被自动解压 total 17840 drwxr-x--- 4 root root 37 Jul 7 21:34 ROOT -rw-r--r-- 1 root root 18265402 Jun 20 13:08 ROOT.war 构建 Java 微服务项目镜像Dockerfile 内容 # 一个容器内只跑一个jar包 FROM java:8-jdk-alpine LABEL maintainer www.missf.top ENV JAVA_OPTS=\"$JAVA_OPTS -Dfile.encoding=UTF8 -Duser.timezone=GMT+08 -Xms128m -Xmx128m\" RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories && \\ apk add -U tzdata && \\ mkdir /projects && \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime COPY hello.jar /projects/ EXPOSE 8888 CMD [\"/bin/sh\", \"-c\", \"java -jar $JAVA_OPTS /projects/hello.jar\"] Dockerfile 最佳实践减少镜像层：一次 RUN 指令形成新的一层镜像，shell 命令尽量写在一行，减少镜像层 优化镜像大小：在形成新的一层镜像之后，如果没有在同一层删除缓存或者没用的文件，那么这些文件都会被带到下一层，所有要在每一层清理对应的残留数据，减少镜像大小 减少网络传输：例如镜像所需要下载的软件包，mvn 仓库 多阶段构建：代码编译、部署在一个 Dockerfile 完成，只会保留部署阶段产生的数据 选择最小的基础镜像：例如 alpine","categories":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker 容器网络","slug":"Docker 容器网络","date":"2020-06-23T01:49:44.000Z","updated":"2020-09-28T02:52:36.000Z","comments":true,"path":"post/bc1d2f66.html","link":"","permalink":"https://www.missf.top/post/bc1d2f66.html","excerpt":"","text":"bridge 模式当启动 Docker 进程之后，Docker 会默认创建一个名为 docker0 的虚拟网桥，创建容器时如果不指定网络，默认就是添加到这个网桥中。这样 Docker 主机上的所有容器都可以通过交换机的方式连接在一个二层网络中。创建容器时，Docker 会先创建容器的虚拟网卡，容器的虚拟网卡去连接 Docker 主机的 docker0 虚拟网桥，相当于用一根网线将容器和 Docker 主机连接起来。虚拟网卡连接到 docker0 子网后，由 docker0 虚拟网桥分配 IP 给容器的虚拟网卡使用，并设置 docker0 虚拟网桥的 IP 地址为容器的默认网关。除了 Docker 启动时默认创建的 bridge 默认网络，我们还可以自定义 bridge 网络。相比默认的具备内部 DNS 发现，bridge 网络模式还可以通过容器名去实现容器之间的网络通信 查看 Docker 宿主机上的 docker0 虚拟网桥，默认网段是 172.17.0.1，安装 Docker 之后默认创建的 ip a s docker0 3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 02:42:9f:dc:ee:74 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:9fff:fedc:ee74/64 scope link valid_lft forever preferred_lft forever 查看默认定义好的网络模式，这里没有 container 模式是因为 container 是启动容器时直接指定的 docker network ls NETWORK ID NAME DRIVER SCOPE a42d2b0e12ec bridge bridge local 168bbf4b0447 host host local ec481d03e2a1 none null local 21be62f7b97e webserver bridge local 查看 bridge 网络模式的详细信息 docker network inspect bridge [ { \"Name\": \"bridge\", \"Id\": \"a42d2b0e12ec0e039e7c4686099468585b88c8df8b639eaa780700980adb9e1b\", \"Created\": \"2020-06-23T17:16:25.717600267+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.17.0.0/16\", \"Gateway\": \"172.17.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": { \"Network\": \"\" }, \"ConfigOnly\": false, \"Containers\": { \"253d0d8f196182eccaa52238068513cebfbf2abe69d2a7980e40d8c136b53960\": { \"Name\": \"nginx\", \"EndpointID\": \"7fd4576f90bc1d0fd966ed5794710dd43461d077ea32f99e54a8b3c56ba1de08\", \"MacAddress\": \"02:42:ac:11:00:02\", \"IPv4Address\": \"172.17.0.2/16\", \"IPv6Address\": \"\" }, \"8652448b6f9a99d9b9a6c70277ea23924b21df57289d4deb29a146974ad4c4dd\": { \"Name\": \"centos7\", \"EndpointID\": \"e112927463f07a606a3a019f3af7400c711b9a903fec19c130b27c7d5f53d359\", \"MacAddress\": \"02:42:ac:11:00:03\", \"IPv4Address\": \"172.17.0.3/16\", \"IPv6Address\": \"\" } }, \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, \"Labels\": {} } ] 安装网桥管理工具包 yum install -y bridge-utils.x86_64 查看虚拟网桥上的接口信息 brctl show docker0 bridge name bridge id STP enabled interfaces docker0 8000.02429fdcee74 no veth81bdc19 vetha8f66a7 创建类型为 bridge 的自定义网桥 docker network create webserver 21be62f7b97ebfc9ce6f6a1aaaffd59a4a220c6b778f36a98c72162023b5c5e5 启动容器时指定使用自定义创建的 webserver 网桥(具备DNS发现) docker container run -itd --name \"app1\" --network webserver centos:7.7.1908 98efd7fb3c63c0bd487039b7ef00925d786e0499f10d76003afa2277cc93b404 docker container run -itd --name \"app2\" --network webserver centos:7.7.1908 c81e58db50ca74111d46f460ff322378b45414a36804738597559ec3c06cf542 docker container run -itd --name \"app3\" --network webserver centos:7.7.1908 41fb1a7dd161c03a158a104da54dcfa3b226035feceecabd003f7a18e91bff61 查看容器的 IP 地址 docker inspect --format='{{.NetworkSettings.Networks.webserver.IPAddress}}' app1 172.18.0.2 docker inspect --format='{{.NetworkSettings.Networks.webserver.IPAddress}}' app2 172.18.0.3 docker inspect --format='{{.NetworkSettings.Networks.webserver.IPAddress}}' app3 172.18.0.4 容器之间的通信测试，自定义的 bridge 网桥相比默认的 bridge 网桥具备内部 DNS 发现， IP 和主机名都是可以 PING 通 ping 172.18.0.2 PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. 64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.203 ms 64 bytes from 172.18.0.2: icmp_seq=2 ttl=64 time=0.085 ms ping 98efd7fb3c63 # 如果启动容器时不指定自定义的网桥,那就会使用默认的bridge模式,这样是不能PING通主机名的 PING 98efd7fb3c63 (172.18.0.2) 56(84) bytes of data. 64 bytes from app1.webserver (172.18.0.2): icmp_seq=1 ttl=64 time=0.402 ms 64 bytes from app1.webserver (172.18.0.2): icmp_seq=2 ttl=64 time=0.100 ms host模式如果启动容器时指定 host 模式，那么这个容器将不会获得一个独立的 Network namespace，而是和宿主机共用一个 Network namespace。容器不会虚拟出自己的网卡，而是使用宿主机的 IP 和端口。这种无需 NAT 转换的网络模式无需再映射容器与宿主机之间的端口，在提高网络传输性能的同时，造成了网络环境隔离性弱化。容器之间不再拥有隔离独立的网络，Docker host 上已使用的端口就不能再用了 启动一个 Nginx 容器，再查看宿主机上的 80 端口是否被使用 docker container run -itd --name \"host_nginx\" --network=host nginx:1.1 查看宿主机上的 80 端口是否被 Nginx 容器所使用 netstat -lntup | grep 80 tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 7358/nginx: master 查看宿主机上 Nginx 进程的父进程是否为 Docker ps -afx | grep containerd -A 1 1100 ? Ssl 1:18 /usr/bin/containerd 7341 ? Sl 0:00 \\_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/adf66250b1fcd95c2531f04f8504bea614dd90903f4f074e150ce6202895a023 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc 7358 pts/0 Ss+ 0:00 \\_ nginx: master process nginx -g daemon off; # 这个nginx进程是容器中启动的nginx进程,这也正如我们前面所说,使用host模式启动容器,容器会和宿主机共用一个Network namespace 进入容器中查看网卡信息，可以看到宿主机上的网卡也会显示，这就是共用了一个 Network namespace 的结果 ifconfig br-21be62f7b97e: flags=4099&lt;UP,BROADCAST,MULTICAST> mtu 1500 inet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255 inet6 fe80::42:6fff:fe77:c9f0 prefixlen 64 scopeid 0x20&lt;link> ether 02:42:6f:77:c9:f0 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 docker0: flags=4099&lt;UP,BROADCAST,MULTICAST> mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fe80::42:9fff:fedc:ee74 prefixlen 64 scopeid 0x20&lt;link> ether 02:42:9f:dc:ee:74 txqueuelen 0 (Ethernet) RX packets 3 bytes 114 (114.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 677 (677.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ens32: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 10.10.110.150 netmask 255.255.255.0 broadcast 10.10.110.255 inet6 fe80::20c:29ff:fec4:cbac prefixlen 64 scopeid 0x20&lt;link> ether 00:0c:29:c4:cb:ac txqueuelen 1000 (Ethernet) RX packets 91694 bytes 118390130 (112.9 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 41857 bytes 2875558 (2.7 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73&lt;UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host> loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 none 模式容器启动时指定 none 模式是获取独立的 Network namespace，但不为容器进行任何网络配置。容器内部只有 loopback 网络设备不会再有其他的网络资源，将网络创建的责任完全交给用户。作为 Docker 开发者，才能在这基础做其他无限多可能的网络定制开发，这种方式可以实现更加灵活复杂的网络，同时也体现了Docker 设计理念的开放 启动一个 none 模式的容器 docker container run -itd --name \"none_centos\" --network=none centos:7.7.1908 进入容器查看网卡设备信息 docker container exec -it none_centos /bin/bash ifconfig # 这里只有一个回环口地址,因为none模式不会对容器进行任何网络配置 lo: flags=73&lt;UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host> loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 container 模式创建新的容器时指定和已存在的容器共享一个 Network namespace，这些容器之间共享 IP、端口范围等网络配置，容器之间传输效率高。两个容器除了网络资源共享之外，其他资源还是隔离的。虽然多个容器共享网络环境，但是多个容器形成的整体依然与宿主机以及其他容器形成网络隔离 启动一个名为 server1 的容器 docker container run -itd --name \"server1\" centos:7.7.1908 再启动两个容器，把它们加入到 server1 这个容器的 Network namespace docker container run -itd --name \"server2\" --network=container:server1 centos:7.7.1908 docker container run -itd --name \"server3\" --network=container:server1 centos:7.7.1908 查看各个容器的 IP 地址 docker inspect --format='{{.NetworkSettings.Networks.bridge.IPAddress}}' server1 172.17.0.3 docker inspect --format='{{.NetworkSettings.Networks.bridge.IPAddress}}' server2 &lt;no value> docker inspect --format='{{.NetworkSettings.Networks.bridge.IPAddress}}' server3 &lt;no value> 这里我们在查看 server2 和 server3 容器 IP 时，显示为 &lt;no value&gt;，其实它们是和 server1 共用一个 Network namespace 的 docker container exec -it server2 /bin/bash [root@41436b0be6f7 /]# ifconfig eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 172.17.0.3 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:ac:11:00:03 txqueuelen 0 (Ethernet) RX packets 10969 bytes 20985758 (20.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 6234 bytes 344851 (336.7 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 docker container exec -it server3 /bin/bash [root@41436b0be6f7 /]# ifconfig eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 172.17.0.3 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:ac:11:00:03 txqueuelen 0 (Ethernet) RX packets 10969 bytes 20985758 (20.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 6234 bytes 344851 (336.7 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 两个容器的IP、主机名都相同 容器虚拟网卡和 docker0 网桥的 veth pair 配对veth 是成对出现的虚拟网络设备， 发送到 veth 一端虚拟设备的请求会从另一端的虚拟设备中发出。创建一个容器的同时会为这个容器创建一对虚拟网卡 veth pair，这个成对出现的虚拟网卡 veth pair，分别放到宿主机和容器中，宿主机一端桥接到默认的 docker0 或者自定义的网桥上，容器一端放到新创建容器的 Network namespace 中，并把名字修改为 eth0。虚拟网卡 veth pair 就像是一根网线，将宿主机的 docker0 和容器连接起来 docker container run -itd --name \"server1\" centos:7.7.1908 # 创建容器 brctl show docker0 # 查看宿主机上的docker0网桥 bridge name bridge id STP enabled interfaces docker0 8000.02429fdcee74 no veth7459cf7 ip a s veth7459cf7 # 这是虚拟网卡veth pair在宿主机上的一端 34: veth7459cf7@if33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 86:54:3c:c6:70:6b brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::8454:3cff:fec6:706b/64 scope link valid_lft forever preferred_lft forever [root@ec94bfbd724f /]# ifconfig # 容器内部的eth0网卡是虚拟网卡veth pair在容器中的一端 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 172.17.0.2 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:ac:11:00:02 txqueuelen 0 (Ethernet) RX packets 5495 bytes 10346440 (9.8 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 3386 bytes 186731 (182.3 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 容器网络访问原理图 容器网络实现的核心技术: iptablesdocker 容器的跨网络隔离与通信，是使用 iptables 去实现的 源IP地址变换规则docker 在安装完成后，将默认在宿主机上增加一些 iptables 规则，以用于 docker 容器和容器之间的隔离与通信，可以使用使用 iptables-save 命令查看 iptables-save | grep docker -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE 参数说明: -s:源地址172.17.0.0/16 -o:指定数据报文流出接口为docker0 -j:动作为MASQUERADE(地址伪装) 上面这条规则关系着 Docker 容器和外界的通信，含义是源地址为 172.17.0.0/16 的数据包(即Docker容器发出的数据)，当不是从 docker0 网卡发出时做 SNAT(源地址转换)。这样使得 Docker 容器访问外网的流量，在外界看来就是从宿主机上发出的，外界感觉不到 Docker 容器的存在 目标IP地址变换规则从 Docker 容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到 Docker 容器的存在。其实这也是由相应的 iptables 规则去实现的 docker container run -itd --name \"nginx\" -p 80:80 nginx:1.17 查看创建容器之后生成的 iptables 规则 iptables-save | grep docker -A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80 -A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT 这两条规则将访问宿主机的 80 端口的流量转发到了 172.17.0.2 的 80 端口上(即真正提供服务的 Docker 容器的IP+端口)，所以外界访问 Docker 容器是通过 iptables 做DNAT (目的地址转换)实现的 etcd 和 flannel 实现 docker 跨主机通信flannel 是一种基于 overlay 网络的跨主机容器网络解决方案，也就是将 TCP 数据包封装在另一种网络包里面进行路由转发和通信，flannel 是 CoreOS 团队针对Kubernetes 设计的一个网络规划服务，让集群中的不同节点主机创建的容器都具有全集群唯一的虚拟 ip 地址，flannel 使用 go 语言编写 实现原理 flannel 为每个 host 分配一个 subnet，容器从这个 subnet 中分配 ip，这些 ip 可以在 host 间路由，容器间无需使用 nat 和端口映射即可实现跨主机通信。每个 subnet 都是从一个更大的 ip 池中划分的，flannel 会在每个主机上运行一个叫 flanneld 的 agent，其职责就是从池子中分配 subnet。etcd 相当于一个数据库，flannel 使用 etcd 存放网络配置、已分配的 subnet、host 的 IP 等信息 实验环境 节点 安装软件 系统 内核版本 docker版本 10.10.110.150(master) etcd、flannel、docker CentOS7.7.1908 3.10.0-1062.el7.x86_64 19.03.12 10.10.110.151(slave) flannel、docker CentOS7.7.1908 3.10.0-1062.el7.x86_64 19.03.12 master节点配置安装配置 etcd yum install -y etcd # 安装etcd,由于不配置etcd集群,所以只在10.10.110.150节点安装etcd就行了 sed -i \"s/localhost/10.10.110.150/g\" /etc/etcd/etcd.conf # 修改etcd配置文件 systemctl start etcd.service # 启动etcd 安装配置 flannel yum install -y flannel sed -i \"s/127.0.0.1/10.10.110.150/g\" /etc/sysconfig/flanneld # flannel连接到etcd,slave连接也是填写master的IP etcdctl --endpoints=\"http://10.10.110.150:2379\" set /atomic.io/network/config '{ \"Network\":\"172.17.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}} ' # 配置etcd的子网,如果这一步不配置,那么etcd无法启动 systemctl start flanneld.service # 启动flannel slave节点配置安装配置 flannel yum install -y flannel sed -i \"s/127.0.0.1/10.10.110.150/g\" /etc/sysconfig/flanneld # 这里是填写master节点的IP,让slave连接到master的etcd,多slave也一样 systemctl start flanneld.service # 确保slave节点能连接到master节点的etcd,如果不关闭防火墙,那必须打开2379端口 配置 Docker 使用 flannel 的网络master 节点 vim /usr/lib/systemd/system/docker.service EnvironmentFile=/run/flannel/docker # 加载这个文件里面的变量,这个文件记录了flannel分配给master节点的子网信息(slave也会有自己的子网) ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock $DOCKER_NETWORK_OPTIONS # 这个变量是上面文件中定义的,意思是在启动容器时指定使用flannel分配的子网去配置容器的网络 iptables -P FORWARD ACCEPT # 开启iptables转发,如不开启即使配置成功也不能通信 systemctl daemon-reload systemctl restart flanneld.service # 这里必须先重启flannel再重启docker,这时候启动容器就会使用flannel去配置容器的网络 systemctl restart docker.service slave 节点配置 vim /usr/lib/systemd/system/docker.service EnvironmentFile=/run/flannel/docker # 查看slave节点上这个文件,网段是和master节点不一样的 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock $DOCKER_NETWORK_OPTIONS iptables -P FORWARD ACCEPT systemctl daemon-reload systemctl restart flanneld.service systemctl restart docker.service 查看宿主机的IP变化master 节点 ip a 3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:e3:89:96:4e brd ff:ff:ff:ff:ff:ff inet 172.17.98.1/24 brd 172.17.98.255 scope global docker0 valid_lft forever preferred_lft forever 4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 02:6f:fa:71:67:f7 brd ff:ff:ff:ff:ff:ff inet 172.17.98.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::6f:faff:fe71:67f7/64 scope link valid_lft forever preferred_lft forever # docker0虚拟网卡和flannel虚拟网卡已经在同一网段，这时候说明配置成功 slave 节点 ip a 3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:f2:30:ba:34 brd ff:ff:ff:ff:ff:ff inet 172.17.75.1/24 brd 172.17.75.255 scope global docker0 valid_lft forever preferred_lft forever 4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default link/ether f6:ae:d1:c0:e1:a7 brd ff:ff:ff:ff:ff:ff inet 172.17.75.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::f4ae:d1ff:fec0:e1a7/64 scope link valid_lft forever preferred_lft forever 在两个节点创建容器相互 ping 验证master 节点 docker run -it busybox sh / # ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:62:02 inet addr:172.17.98.2 Bcast:172.17.98.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1450 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:656 (656.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) / # ping 172.17.75.2 PING 172.17.75.2 (172.17.75.2): 56 data bytes 64 bytes from 172.17.75.2: seq=0 ttl=62 time=0.492 ms 64 bytes from 172.17.75.2: seq=1 ttl=62 time=0.353 ms 64 bytes from 172.17.75.2: seq=2 ttl=62 time=0.342 ms slave 节点 docker run -it busybox sh / # ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:4B:02 inet addr:172.17.75.2 Bcast:172.17.75.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1450 Metric:1 RX packets:6 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:516 (516.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) / # ping 172.17.98.2 PING 172.17.98.2 (172.17.98.2): 56 data bytes 64 bytes from 172.17.98.2: seq=0 ttl=62 time=1.945 ms 64 bytes from 172.17.98.2: seq=1 ttl=62 time=0.344 ms 64 bytes from 172.17.98.2: seq=2 ttl=62 time=0.384 ms 注意：如果不能 ping 通，先重启 flannel 再重启 Docker 试试","categories":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker 容器数据持久化","slug":"Docker 容器数据持久化","date":"2020-06-19T02:14:10.000Z","updated":"2020-09-28T02:42:34.000Z","comments":true,"path":"post/a7b8d397.html","link":"","permalink":"https://www.missf.top/post/a7b8d397.html","excerpt":"","text":"容器数据持久化的三种方式由于容器的镜像分层机制，我们在容器里面创建文件或者修改文件，结果都会保存在容器的可读写层中，一旦容器被销毁，那么这个读写层也会随着容器销毁而消失。而且当一个容器需要和其他容器的读写层进行数据交互时，也会显得非常困难。于是在将容器数据持久化到宿主机方面，Docker 为我们提供了三种持久化的方式 volumes 持久化方式 volumes 由 Docker 负责创建、管理。用户可以显式的调用命令 docker volume create 创建 volume，也可以通过 container、service 的启动隐式创建。Docker 创建的 volumes 本质上还是宿主机文件系统中的一个目录，一个 volumes 可以供多个容器使用，即使没有容器使用此 volumes，它也不会自动删除，除非用户明确删除它。如果用户显式创建 volumes 则需要给它一个名称，如果是隐式创建 volumes 则 Docker 会为它分配一个在宿主机范围内唯一的名字。通过使用第三方提供的 volume driver，用户可以将数据持久到远程主机或者云存储中，也就是说存储空间可以不由宿主机提供 # 创建volumes docker volume create nginx_volumes # 查看volumes docker volume ls # 查看卷详细信息 docker volume inspect nginx_volumes [ { \"CreatedAt\": \"2020-06-19T18:47:49+08:00\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/nginx_volumes/_data\", # 这是volumes在宿主机上的真实路径 \"Name\": \"nginx_volumes\", \"Options\": {}, \"Scope\": \"local\" } ] # 清理volumes docker volume rm nginx_volumes 将 Nginx 容器的 html 目录映射到宿主机的 nginx_volumes 目录 # 创建数据持久化的容器,如果卷不存在则自动创建 docker container run -itd --name \"nginx1\" -p 80:80 -v nginx_volumes:/usr/share/nginx/html nginx:1.17 # -v方式 docker container run -itd --name \"nginx1\" -p 80:80 --mount src=nginx_volumes,dst=/usr/share/nginx/html nginx:1.17 # --mount方式 # 查看nginx_volumes在宿主机的真实目录 ll /var/lib/docker/volumes/nginx_volumes/_data total 8 -rw-r--r-- 1 root root 494 Apr 14 22:19 50x.html # 这时候nginx容器内部的文件已经被映射到宿主机上了 -rw-r--r-- 1 root root 612 Apr 14 22:19 index.html # 修改宿主机上的index.html文件 echo \"nginx_volumes test\" > /var/lib/docker/volumes/nginx_volumes/_data/index.html # 访问宿主机的80端口(前面启动容器时将容器的80端口绑定到宿主机的80端了) curl 10.10.110.150 nginx_volumes test # nginx容器内的文件确实被修改成功 bind mounts 持久化方式 bind mounts 本质上是容器共享宿主机文件系统，比如 Docker 将宿主机的 /etc/resov.conf 文件 bind mount 到容器里，两者会使用相同的 dns 服务器 # 创建容器,将宿主机的/nginx/app绑定到容器的/usr/share/nginx/html目录 docker container run -itd --name \"nginx1\" --mount type=bind,src=/nginx/app,dst=/usr/share/nginx/html nginx:1.17 docker container run -itd --name \"nginx1\" -v /nginx/app:/usr/share/nginx/html nginx:1.17 # 查看宿主机和容器的目录 ls /nginx/app docker exec -it nginx1 ls /usr/share/nginx/html # 两个目录都为空,这是因为bind mounts是将宿主机的目录绑定到容器的目录,容器目录已有的内容会被隐藏(bind mounts以宿主机目录为主) 注意：如果源文件或源目录不存在，则不会自动创建。如果容器目录为非空目录，则容器目录现有内容会被宿主机目录内容所隐藏 tmpfs 持久化方式 出于安全原因，或者容器性能优化的原因有时候不需要容器的数据长久保存时可以使用这种方式。将容器数据挂载存储在宿主机的内存中，避免写入容器可写层，提高容器性能 volumes 和 bind mounts 的使用场景和区别volumes 适合多个容器需要共享数据、将数据保存到远程主机或云上等场景。bind mounts 适合将宿主机的系统配置文件共享给容器。volumes 是将容器内部的数据映射到宿主机对应的 volumes 目录，如果容器内部是一个非空目录，volumes 目录也是一个非空目录，那么两个目录的文件会合并。而 bind mounts 是将宿主机上任意位置的目录或文件挂载到容器中，如果宿主机的目录非空，那么容器目录的数据将会被宿主机目录的数据隐藏，容器内的数据要卸除挂载后才会恢复 bind mounts 和volumes 都可以通过使用标志 “-v” 或 “–volume” 来挂载到容器中，只是格式有些许不同。然而，在 Docker17.06 及其以上版本中，我们推荐使用 “–mount” 来对容器或服务进行这三种方式的挂载，因为这种格式更加清晰","categories":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker 容器管理","slug":"Docker 容器管理","date":"2020-06-17T08:23:38.000Z","updated":"2020-09-28T02:39:56.000Z","comments":true,"path":"post/39a5b294.html","link":"","permalink":"https://www.missf.top/post/39a5b294.html","excerpt":"","text":"创建容器常用选项 选项 描述 -i, –interactive 交互式 -t, –tty 分配一个伪终端 -d, –detach 运行容器到后台 -e, –env 设置环境变量 -p, –publish list 发布容器端口到主机 -P, –publish-all 发布容器所有EXPOSE的端口到宿主机随机端口 –name string 指定容器名称 -h, –hostname 设置容器主机名 –ip string 指定容器IP,只能用于自定义网络 –network 连接容器到一个网络 –mount mount 将文件系统附加到容器 -v, –volume list 绑定挂载一个卷 –restart string 容器退出时重启策略,默认no,可选值:[always|on-failure] 创建容器示例 # 启动一个nginx容器,指定名字、映射端口、设置重启 # 如果不加-it分配一个交互式的伪终端,容器就会直接退出了,容器内的第一个程序必须一直处于前台运行(必须hang住) docker container run -itd --name \"nginx\" -p 80:80 --restart always nginx:1.17 容器资源限制 选项 描述 -m，–memory 容器可以使用的最大内存量 –memory-swap 允许交换到磁盘的内存量 –memory-swappiness=&lt;0-100&gt; 容器使用SWAP分区交换的百分比(0-100，默认为-1) –oom-kill-disable 禁用OOM Killer –cpus 可以使用的CPU数量 –cpuset-cpus 限制容器使用特定的CPU核心，如(0-3, 0,1) –cpu-shares CPU共享(相对权重) 内存限额示例 # 允许容器最多使用500M内存和600M的swap,并禁用OOM Killer docker container run -d --name \"nginx1\" --memory=\"500M\" --memory-swap=\"600M\" --oom-kill-disable nginx:1.17 CPU限额示例 # 允许容器最多使用两个的CPU docker container run -d --name \"nginx2\" --cpus=\"2\" nginx:1.17 # 允许容器最多使用50%的CPU docker container run -d --name \"nginx3\" --cpus=\".5\" nginx:1.17 容器资源配额扩容# 容器资源可更新选项 docker update --help Usage: docker update [OPTIONS] CONTAINER [CONTAINER...] Update configuration of one or more containers Options: --blkio-weight uint16 Block IO (relative weight), between 10 and 1000, or 0 to disable (default 0) --cpu-period int Limit CPU CFS (Completely Fair Scheduler) period --cpu-quota int Limit CPU CFS (Completely Fair Scheduler) quota --cpu-rt-period int Limit the CPU real-time period in microseconds --cpu-rt-runtime int Limit the CPU real-time runtime in microseconds -c, --cpu-shares int CPU shares (relative weight) --cpus decimal Number of CPUs --cpuset-cpus string CPUs in which to allow execution (0-3, 0,1) --cpuset-mems string MEMs in which to allow execution (0-3, 0,1) --kernel-memory bytes Kernel memory limit -m, --memory bytes Memory limit --memory-reservation bytes Memory soft limit --memory-swap bytes Swap limit equal to memory plus swap: '-1' to enable unlimited swap --pids-limit int Tune container pids limit (set -1 for unlimited) --restart string Restart policy to apply when a container exits # 更新正在运行中的容器内存限额 docker update --memory=\"400M\" --memory-swap=\"500M\" --restart=\"on-failure\" 4e860294d239 管理容器常用命令 选项 描述 ls 列出容器 inspect 查看一个或多个容器详细信息 exec 在运行容器中执行命令 commit 创建一个新镜像来自一个容器 cp 拷贝文件/文件夹到一个容器 logs 获取一个容器日志 port 列出或指定容器端口映射 top 显示一个容器运行的进程 stats 显示容器资源使用统计 stop/start/restart 停止/启动一个或多个容器 rm 删除一个或多个容器 prune 移除已停止的容器 管理容器示例 # 列出真正运行的所有容器 docker container ls -a # 获取一个容器日志 docker container logs --tail=\"5\" nginx # 仅列出最新N条容器log信息 docker container logs -f nginx # 跟踪log信息输出 docker logs --since=\"2020-06-18\" --tail=\"10\" nginx # 显示某个时间之后的最新十条log信息 # 进入正在运行的容器中执行命令 docker container exec -it nginx /bin/bash # 显示一个容器运行的进程 docker container top nginx # 删除一个或删除全部容器 docker container rm -f nginx docker container rm -f $(docker container ls -q) 容器实现核心技术：Namespace在容器化中，一台物理计算机可以运行多个不同操作系统(一个容器就类似于一个系统)，那就需要解决 “隔离性”，让彼此感知不到对方的存在，出现问题也互不影响 Linux 内核从 2.4.19 版本开始引入了 namespace 概念，其目的是将特定的全局系统资源通过抽象方法使得 namespace 中的进程看起来拥有自己隔离的资源。Docker 就是借助这个机制实现了容器资源隔离 Linux 的 namespace 机制提供了6种不同的命名空间 IPC：隔离进程间通信 MOUNT：隔离文件系统挂载点 NET：隔离网络协议栈 PID：隔离进程号，容器命名空间对父进程空间可见 USER：隔离用户 UTS：隔离主机名和域名 容器实现核心技术：CGroupsDocker 利用 namespace 实现了容器之间资源隔离，但是 namespace 不能对容器资源限制，比如 CPU、内存。如果某一个容器属于 CPU 密集型任务，那么会影响其他容器使用 CPU，导致多个容器相互影响并且抢占资源。如何对多个容器的资源使用进行限制就成了容器化的主要问题。所以容器引入了 Control Groups(简称CGroups)，限制容器资源 CGroups 以某种标准讲一组进程为目标进行资源分配和控制，例如 CPU、内存、带宽等，并且可以动态配置 限制进程组使用的资源数量(Resource limitation )：可以为进程组设定资源使用上限，例如内存 进程组优先级控制(Prioritization)：可以为进程组分配特定 CPU、磁盘 IO 吞吐量 记录进程组使用的资源数量(Accounting)：例如使用记录某个进程组使用的 CPU 时间 进程组控制(Control)：可以将进程组挂起和恢复 查看cgroups可控制的资源 资源 描述 blkio 对块设备的IO进行限制 cpu 限制CPU时间片的分配，与cpuacct挂载同一目录 cpuacct 生成cgroup中的任务占用CPU资源的报告，与cpu挂载同一目录 cpuset 给cgroup中的任务分配独立的CPU(多核处理器)和内存节点 devices 允许或者拒绝 cgroup 中的任务访问设备 freezer 暂停/恢复 cgroup 中的任务 hugetlb 限制使用的内存页数量 memory 对cgroup中任务的可用内存进行限制，并自动生成资源占用报告 net_cls 使用等级识别符(classid)标记网络数据包，这让 Linux 流量控制程序(tc)可以识别来自特定从cgroup任务的数据包，并进行网络限制 net_prio 允许基于cgroup设置网络流量的优先级 perf_event 允许使用perf工具来监控cgroup pids 限制任务的数量 资源控制在容器中的实际位置 ll /sys/fs/cgroup/\"资源名\"/docker/\"容器ID\"/ Docker 核心组件之间关系我们使用 Docker client 运行一个容器，其实容器运行时底层是需要依赖一系列组件的，我们完全可以通过调用这些组件去启动一个容器，而不使用 Docker 引擎的方式去启动。主要的组件有 docker client、docker daemon、containerd、container-shim、runC docker client docker 客户端程序，负责发送用户的请求给 docker daemon docker daemon docker daemon守护进程，也称 docker engine，负责处理 docker client 的请求，并返回处理结果 containerd containerd 是一个工业级标准的容器运行时，它强调简单性、健壮性和可移植性。Containerd 可以在宿主机中管理完整的容器生命周期:容器镜像的传输和存储、容器的执行和管理、存储和网络等。为 docker daemon 提供接口去管理容器，Docker 对容器的管理和操作基本都是通过 containerd 完成的。但是要注意的是：containerd 被设计成嵌入到一个更大的系统中，而不是直接由开发人员或终端用户使用 container-shim container-shim 是 containerd 的组件，是容器的运行时载体，我们在 Docker 宿主机上看到的 shim 也正是代表着一个个通过调用containerd 启动的 Docker 容器 ps axf | grep docker -A 1 10191 ? Sl 0:01 \\_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/4dffa5d5861899400770d6470618e4e051c5f1bf0c53034999b13821fc3fe93f -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc 10208 ? Ss 0:00 \\_ nginx: master process nginx -g daemon off; -- 4215 ? Ssl 2:06 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock runC RunC 是一个轻量级的工具，它是用来运行容器的。我们可以认为它就是个命令行小工具，可以不用通过 Docker 引擎，直接运行容器。事实上 runC 是标准化的产物，它根据 OCI 标准来创建和运行容器。而 OCI(Open Container Initiative) 组织，旨在围绕容器格式和运行时制定一个开放的工业化标准","categories":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker 镜像管理","slug":"Docker 镜像管理","date":"2020-06-16T07:00:10.000Z","updated":"2020-09-28T02:34:44.000Z","comments":true,"path":"post/ce2c2968.html","link":"","permalink":"https://www.missf.top/post/ce2c2968.html","excerpt":"","text":"镜像概述 镜像是一个分层存储的文件 镜像就是一个软件的运行环境 一个镜像可以重复使用，创建无数个容器 一个不包含 Linux 内核而又精简的 Linux 操作系统 镜像是一种标准化的交付，镜像内包含代码以及软件的运行环境 配置镜像加速阿里云为每一个开通容器镜像服务的用户免费提供一个镜像加速地址 # 配置镜像加速 tee /etc/docker/daemon.json &lt;&lt; EOF { \"registry-mirrors\": [\"https://265wemgl.mirror.aliyuncs.com\"] } EOF systemctl daemon-reload systemctl restart docker.service 拉取镜像我们拉取镜像一般是默认从 Docker Hub 拉取的，但是国内访问 Docker Hub 速度很慢，所以我们在前面配置了阿里云的镜像加速。在拉取镜像时，直接从阿里云的 Docker 镜像仓库拉取。我们假如要拉取一个镜像，但是不知道仓库中是否有这个镜像时，我们可以先搜索这个镜像名字，看是否有对应的镜像 # 搜索镜像 docker search nginx # 拉取镜像,如果不指定版本号,默认拉取最新(latest) docker pull nginx:1.17 镜像拉取到宿主机本地之后，会以分层的文件形式存储，下面是镜像的存放目录 [root@localhost ~]# ll /var/lib/docker/overlay2/ total 0 drwx------ 4 root root 55 Jun 17 19:04 5f4badc01c88554e78d4aaec269a84fb5e2028d42278d5f131dda81c4209622c drwx------ 3 root root 47 Jun 17 19:04 658e3b564ce9017b0bd507f1853702f6cdda4642fdc6fbf4b4d06e34cf9a8c25 drwx------ 3 root root 30 Jun 17 19:09 6d57028d1a60a66afc6959b02e0005ea424182908fadf6aa5ac90f3868c014f7 brw------- 1 root root 253, 0 Jun 17 18:31 backingFsBlockDev drwx------ 4 root root 72 Jun 17 19:04 d56648ebd71c9bdb68226b4021ec008db3ed537072b3c4f9e77afc51f8108c07 drwx------ 2 root root 142 Jun 17 19:09 l 镜像与容器的联系当启动一个新容器时，Docker 只加载只读镜像，并在这个只读镜像上面添加一个读写层，即容器层。但我们需要修改容器里面的文件时，会先从镜像层把这个文件拷贝到读写层，然后再执行修改操作 镜像存储核心技术:联合文件系统(UnionFS)联合文件系统(UnionFS)是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(uniteseveral directories into a single virtual filesystem)。联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。另外，不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的改动层，大大提高了存储的效率 Docker 中使用的 AUFS (AnotherUnionFS)就是一种联合文件系统。 AUFS 支持为每一个成员目录(类似 Git 的分支)设定只读(readonly)、读写(readwrite)和写出（whiteout-able）权限, 同时 AUFS 里有一个类似分层的概念, 对只读权限的分支可以逻辑上进行增量地修改(不影响只读部分的) Docker 目前支持的联合文件系统包括 OverlayFS、AUFS、Btrfs、VFS、ZFS 和 DeviceMapper 镜像存储核心技术：写时复制(COW)Docker 镜像由多个只读层叠加而成，启动容器时，Docker 会加载只读镜像层并在镜像层顶部添加一个读写层。如果运行中的容器修改了一个已存在的文件，那么该文件将会从只读层复制到读写层，该文件的只读版本任然存在，只是已经被读写层中该文件的副本所隐藏，这就是写时复制机制 镜像常用管理命令# 列出镜像,-a显示所有镜像 docker image ls # 在当前目录通过Dockerfile构建镜像 docker build -t \"nginx_tomcat\" . # 查看镜像历史 docker image history nginx:1.17 # 显示镜像的详细信息 docker inspect nginx:1.17 # 从镜像仓库拉取镜像 docker pull nginx:1.17 # 推送镜像到镜像仓库 docker pull centos:7.6.1810 # 移除一个或多个镜像 docker image rm centos docker image rm $(docker image ls -q) # 删除全部镜像 # 删除没有被标记或没有被任何容器引用的镜像 docker image prune -af # 创建一个引用源镜像标记目标镜像 docker tag centos:latest coentos:v1 # 为centos:latest这个镜像打一个标签为coentos:v1 # 导出容器文件系统为tar归档文件 docker export -o centos-export.tar [CONTAINER ID] # 导入容器文件系统tar归档文件来创建镜像 docker import centos-export.tar # 保存一个或多个镜像到一个tar归档文件 docker save -o database.tar redis mysql # 加载镜像来自tar归档或标准输入 docker load -i database.tar 相信许多的初学者看到这里肯定有疑问，这里说明一下 export &amp; import 和 save &amp; load 的区别在哪里 export 和 import 区别export 的应用场景主要用来制作基础镜像，比如你从一个 CentOS 镜像启动一个容器，然后安装一些软件和进行一些设置后，使用 Docker export 保存为一个基础镜像。然后把这个镜像分发给其他人使用，比如作为基础的开发环境 export:将容器导出为tar归档文件,生成的是该容器的快照，复刻了容器当前的Linux系统环境 import:将tar归档文件导入为镜像 整个过程即:容器-->tar归档文件-->镜像 save 和 load 区别如果你的应用是使用 docker-compose.yml 编排的多个镜像组合，但你要部署的客户服务器并不能连外网。这时，你可以使用 docker save将用到的镜像打个包，然后拷贝到客户服务器上使用 docker load 载入 save:将镜像导出为tar归档文件,该命令也可以作用于容器,但导出的是容器背后的images load:将tar归档文件导入为镜像 注意：save 命令生成的 tar 包比 export 命令生成的 tar 包大很多，两组命令不可交叉互用","categories":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Docker 核心概念与安装","slug":"Docker 核心概念与安装","date":"2020-06-15T02:56:03.000Z","updated":"2020-09-28T02:30:14.000Z","comments":true,"path":"post/f7aff4ce.html","link":"","permalink":"https://www.missf.top/post/f7aff4ce.html","excerpt":"","text":"为什么使用容器提供简单轻量的建模方式，非常容易上手，运行速度非常快 使开发和运维的职责分离，开发只需要关心容器中的程序，运维只需要管理容器 快速高效的开发生命周期，开发环境和生产环境一致，避免了额外的调试有效缩短上线时间 鼓励使用面向服务的架构，Docker 推荐单个容器只运行一个应用程序，使分布式扩展和调试变得简单 Docker 是什么容器技术 想要了解 Docker，首先要知道什么是容器。最早的容器技术来自于 BSD 的 jail 技术(jail一词是监狱的意思，这个技术的隔离思想来源于监狱的启发)，目的就是为了实现进程隔离，使得一个进程被攻陷后不会影响到其他进程，这是出于安全的目的 使用最为广泛的开源容器引擎 在近几年来，Docker 是一个非常火的名词。事实上 Docker 只是众多容器引擎其中一款优秀的容器引擎，但是它却几乎成为了容器的代名词。许多业外人士觉得 Docker 就是容器，这里大家要明白，Docker 只是属于容器技术的一种 容器是一种操作系统级别的虚拟化技术 使用 Docker 创建的容器，以特殊进程的方式在宿主机上运行，运行一个容器就像运行一个进程一样，宿主机上可以运行多个容器，容器间的资源是互相隔离的 依赖于 Linux 内核特性 Namespace &amp; Cgroups 容器之间运行的是一个隔离的环境，也可以理解类似于一个沙盒，使用 Namespace 进行资源的隔离，使用 Cgroups 进行资源的控制 Docker 基本组成Docker Client 客户端 docker 采用 C/S 架构 docker 客户端和 docker 服务器之间的通信访问可以是本地方式也可以是远程方式 docker 客户端向守护进程发送请求，守护进程的执行结果会传回给客户端 Docker Daemon 守护进程 docker 采用 C/S 架构 docker 客户端和 docker 服务器之间的通信访问可以是本地方式也可以是远程方式 docker 客户端向守护进程发送请求，守护进程的执行结果会传回给客户端 Docker Image 镜像 镜像是容器的基石，镜像包含了容器启动的一切条件，容器基于镜像去启动。镜像是层叠的只读文件系统，底层是bootfs引导文件系统，rootfs文件系统永远是只读状态，使用同一个镜像无论启动多少个容器，或者容器被如何修改，镜像都不会被改变。一个镜像可以放到一个镜像的顶部，最下面的镜像称为基础镜像，就是图中的centos/Ubuntu层。这里使用了写时复制技术(copy on write)，即通过一个镜像启动一个容器实例，这个镜像是以只读形式挂载的，即不允许任何修改操作，当在容器实例中修改一个文件时，会首先从镜像里把这个文件拷贝到可写层，然后执行更新操作。 Docker Container 容器 容器通过镜像启动 docker 守护进程执行命令就是在容器实例中执行 应用部署在容器中 在启动容器时会在镜像的最上层创建一个读写层，读写层加上下面的多个只读层从而构成一个容器 Docker Registry 仓库 随着我们项目的增加，我们构建的镜像也会越来越多。而镜像也是像代码一样的，需要一个镜像仓库来进行管理的，镜像仓库里面保存着我们构建的镜像。镜像仓库还分为公有仓库和私有仓库。公有仓库一般指 Docker Hub，Docker Hub 是一个由 Docker 公司运行和管理的基于云的存储库，它是一个在线存储库，Docker 镜像可以由其他用户发布和使用。而私有仓库一般是我们公司的组织内部拥有的一个私有仓库，仅允许公司内部用户使用 容器的关系图 容器 VS 虚拟机虚拟机是系统级别的虚拟化，而容器是进程级别的虚拟化，这是虚拟机和容器最核心的区别。虚拟机提供了物理机硬件级别的操作系统隔离，使用虚拟机部署应用，除了应用和应用依赖的库文件，还需要虚拟完整的操作系统，每个虚拟机拥有自己独立的内核，这会大量占用系统的硬件资源。而容器是进程级别的虚拟化，当我们运行 Docker 容器时，此时容器本身只是操作系统中的一个进程，利用了 Linux 系统的内核特性 (Namespace &amp; Cgroups) 实现了进程之间网络、空间、权限等隔离，使多个容器进程互相不知道彼此的存在。在这个追求速度的互联网时代，容器在许多方面要比虚拟机优秀。但是不意味着传统的虚拟机技术就过时了，虚拟机的操作系统级别隔离是容器无法替代的，容器的意义在于运行单个应用，如果在容器里面添加越来越多的功能，那不如一开始就直接使用虚拟机 虚拟技术的核心区别 容器 VS 虚拟机详细对比 Container VM 启动速度 秒级 分钟级 运行性能 接近原生 5%左右损失 磁盘占用 MB GB 数量 成百上千 一般几十台 隔离性 进程级 系统级(更彻底) 操作系统 主要支持Linux 几乎所有 封装程度 只打包项目代码和依赖关系，共享宿主机内核 完整的操作系统 Docker 应用场景应用程序打包和发布 应用程序环境隔离 持续集成 部署微服务 快速搭建测试环境 提供Pass产品(平台即服务) Linux 安装 Docker# 安装依赖包 yum install -y yum-utils device-mapper-persistent-data lvm2 # 添加Docker软件源 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 安装Docker CE yum install -y docker-ce # 启动Docker服务并设置开机启动 systemctl start docker systemctl enable docker # 查看docker版本 docker --version Docker version 19.03.11, build 42e35e61f3 # 查看更详细的信息 docker info","categories":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"CODING 持续集成 Java 项目","slug":"CODING 持续集成Java项目","date":"2020-06-11T06:16:18.000Z","updated":"2020-09-28T02:26:54.000Z","comments":true,"path":"post/1b979c3e.html","link":"","permalink":"https://www.missf.top/post/1b979c3e.html","excerpt":"","text":"CODING 介绍在说到持续集成这方面，相信所有做运维的小伙伴都知道 Jenkins，就是那个拿着托盘的老头子。但是说到 coding，可能很多人都没听说过。什么是 coding 呢？ coding 涵盖了软件开发从构想到交付的一切所需，使研发团队在云端高效协同，实践敏捷开发与 DevOps，提升软件交付质量与速度。这是来自官网的介绍，下面就让我们一起学习 coding 吧 注册 CODINGcoding 所有的东西都是在这个云平台上实现的，所谓的使研发团队在云端高效协同说的就是这个吧 创建项目选择 DevOps 项目模板 填写项目基本信息 下载若依的源码 若依源码gitee地址 配置若依数据库，将若依自带的两个 SQL 文件导入到 ry 数据库 初始化本地仓库 git init git add . git commit -m \"第一次提交\" 配置 CODING SSH 秘钥 在 Windows 电脑生成 SSH 密钥对，然后将 id_rsa.pub 公钥添加到 coding SSH 公钥 推送本地仓库到 coding 注意：如果已经在 coding 配置了 SSH 秘钥，git 添加远程仓库的时候不要使用 https 的地址，不然还是会提示需要输入 coding 的账号密码 git remote add origin git@e.coding.net:missf/RuoYi.git # 配置了SSH秘钥的，一定要填写项目的git地址 git push -u origin master # 这样推送时就不需要输入账号密码啦 持续集成创建持续集成任务 新建构建计划 在服务器生成SSH秘钥对，将私钥录入到coding的凭据管理，coding就能持续集成部署代码到服务器 编写静态配置的 Jenkinsfile 配置环境变量 这里附上完整 Jenkinsfile pipeline { agent any stages { stage('检出') { steps { checkout([$class: 'GitSCM', branches: [[name: env.GIT_BUILD_REF]], userRemoteConfigs: [[url: env.GIT_REPO_URL, credentialsId: env.CREDENTIALS_ID]]]) } } stage('构建') { steps { echo '构建中...' sh 'java -version' sh 'mvn package' echo '构建完成.' } } stage('压缩jar包') { steps { echo '压缩中...' sh 'cd /root/workspace/ruoyi-admin/target/ &amp;&amp; tar -zcf /tmp/ruoyi-admin.tar.gz ruoyi-admin.jar' echo '压缩完成.' } } stage('部署') { steps { echo '部署中...' script { def remote = [:] remote.name = 'java-server' remote.allowAnyHosts = true remote.host = \"${env.REMOTE_HOST}\" remote.port = 50312 remote.user = \"${env.REMOTE_USER_NAME}\" // 把「CODING 凭据管理」中的「凭据 ID」填入 credentialsId，而 id_rsa 无需修改 withCredentials([sshUserPrivateKey(credentialsId: \"${env.REMOTE_CRED}\", keyFileVariable: 'id_rsa')]) { remote.identityFile = id_rsa // SSH 上传文件到服务器 sshPut remote: remote, from: '/tmp/ruoyi-admin.tar.gz', into: '/tmp/' // 解压缩 sshCommand remote: remote, sudo: false, command: \"tar -zxf /tmp/ruoyi-admin.tar.gz -C /home/ruoyi/\" // 执行Java应用启停脚本 sshCommand remote: remote, sudo: true, command: \"sh /home/ruoyi/start.sh stop &amp;&amp; sh /home/ruoyi/start.sh start\" } } echo '部署完成' } } } } 本地仓库推送代码到 master 分支时就会自动触发持续集成任务 开启缓存目录后可以大大提升构建的速度 立即构建 查看构建过程，构建失败可以查看完整日志分析失败原因 服务器的启停脚本 [root@java-server ~]# cd /home/ruoyi/ [root@java-server ruoyi]# ll total 65080 drwxr-xr-x 2 root root 4096 May 18 10:18 logs -rw-r--r-- 1 root root 67 May 18 17:04 nohup.out -rw-r--r-- 1 root root 66627886 May 18 17:04 ruoyi-admin.jar -rwxr-xr-x 1 root root 760 May 18 14:29 start.sh [root@java-server ruoyi]# cat start.sh #!/bin/bash WORKSPACE=/home/ruoyi if [ -d \"${WORKSPACE}\" ]; then cd ${WORKSPACE} else echo \"${WORKSPACE} directory does not exist\" exit 1 fi APP_NAME='ruoyi-admin.jar' USE_JAVA_HOME='/usr/local/jdk1.8.0_211' JVM_OPTS='-Xms512m -Xmx512m' CONFIG_OPTS='' if [ $1 == 'start' ]; then echo 'start service '$APP_NAME nohup java -jar ${JVM_OPTS} ${APP_NAME} > ${WORKSPACE}/nohup.out 2>&amp;1 &amp; elif [ $1 == 'stop' ]; then echo 'stop service '$APP_NAME PID=$(ps -ef | grep -v grep | grep ${APP_NAME} | awk '{print $2}') if [ -z ${PID} ]; then echo ${APP_NAME} ' had stopped' else kill ${PID} sleep 2 if [ $? -ne 0 ]; then echo ${APP_NAME} ' stop failed' exit 1 fi fi fi 查看持续集成的效果","categories":[{"name":"CODING","slug":"CODING","permalink":"https://www.missf.top/categories/CODING/"}],"tags":[{"name":"CODING","slug":"CODING","permalink":"https://www.missf.top/tags/CODING/"},{"name":"持续集成","slug":"持续集成","permalink":"https://www.missf.top/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"}]},{"title":"Ansible jinja2 模板","slug":"Ansible jinja2 模板","date":"2020-06-10T12:16:53.000Z","updated":"2020-09-26T09:38:44.000Z","comments":true,"path":"post/347a3183.html","link":"","permalink":"https://www.missf.top/post/347a3183.html","excerpt":"","text":"Ansible jinja2 模板我们在多个管理节点部署服务时，很多服务的配置文件都是需要监听服务所在主机的 IP 地址，这时候就需要使用到 Ansible 的 jinja2 模板去分发动态的配置文件。我们先创建一份模板文件，将 IP 配置部分使用 Ansible 变量进行替换，然后使用 template 模块对模版文件进行渲染，将根据我们定义的变量而生成不同的配置文件发送到对应的管理节点 下面我们以安装 redis 为示例，为不同的管理节点分发动态配置文件 cat /etc/redis/conf/redis.conf bind {{ ansible_host }} # 将默认的127.0.0.1改为管理节点本机的IP，这样就能生成各自的管理节点的配置文件 准备模板配置文件之后，下面就来编写一个 Playbook，完成模板配置文件的渲染和分发 --- - hosts: all remote_user: root tasks: - template: src: /root/playbook/redis.conf # 控制节点上的模板文件,定义好变量,通过template模块进行渲染 dest: /etc/redis.conf # 管理节点上这个文件将被控制节点上的模板文件所替换 jinja2 语法Ansible 使用 Jinja2 模板来启用动态表达式和对变量的访问，就是说 Ansible 使用 Jinja 模板对含有动态表达式和变量的文件进行解析个渲染 变量&amp;表达式 可以使用点.来访问变量的属性，也可以使用下标语法 []，下面 2 种方式效果是一样的 {{ foo.bar }} {{ foo['bar'] }} # 如果变量或属性不存在，会返回一个未定义值。 除了变量， {{}} 中还可以包含一些表达式 {{ 1 == 1 }} {{ 2 != 3 }} {{ 2 > 3 }} {{ 4 &lt;= 3 }} # 根据对应的表达式返回true或false {{ 3 + 2 }} {{ 3 - 4 }} {{ 3 * 5 }} {{ 2 ** 3 }} # 2的3次方 # 根据算术运算返回结果 # 字符串、数值、列表、元组、字典、布尔值等数据类型均可在\"{{ }}\"使用 控制 用来装载控制语句，比如 if 控制结构，for循环控制结构 {% for item in seq %} ``` 注释 要把模板中一行的部分注释掉 ```yaml {# 这是被注释的内容 #} ``` 转义 简单的使用单引号('')进行转义，对于较大的段落，使用raw进行转义 ```yaml {% raw %} &lt;ul> {% for item in seq %} &lt;li>{{ item }}&lt;/li> {% endfor %} &lt;/ul> {% endraw %} 执行命令时传入变量 cat /root/test.j2 my blog is {{ site }} ansible dbserver -m template -e \"site=missf.top\" -a \"src=/root/test.j2 dest=/opt/test\" cat /opt/test my blog is missf.top if {% if num > 3 %} gtfr derew pllu {% endif %} 使用 template 模板进行渲染 ansible dbserver -m template -e \"num=4\" -a \"src=/root/test.j2 dest=/opt/test\" # 渲染后的输出结果 cat /opt/test gtfr derew pllu for jinja2 模板的 for 语法示例 {% if num > 3 %} {% for i in [5,65,7,23] %} {{ i }} {% endfor %} 使用 template 模板进行渲染 ansible dbserver -m template -a \"src=/root/test.j2 dest=/opt/test\" # 渲染后的输出结果 cat /opt/test 5 65 7 23 控制空白 从 for 循环的结果看出，使用 template 模板进行渲染时不会去处理空格或者换行符，在开始或结束放置一个减号(-)，可以移除块前或块后的空白 {% for i in [5,65,7,23] -%} {{ i }} # 这里可以使用{{ i }}{{' '}}定义分割符 {%- endfor %} ansible dbserver -m template -a \"src=/root/test.j2 dest=/opt/test\" cat /opt/test 565723 # 将换行符和空白都移除了 5 65 7 23 # 定义分割符的效果 ​ 我要去重新学习梳理 Docker了，Ansible 就先到这里吧！等到有空了，再重启吧","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible include","slug":"Ansible include","date":"2020-06-10T11:56:59.000Z","updated":"2020-09-26T09:35:20.000Z","comments":true,"path":"post/f03bbd11.html","link":"","permalink":"https://www.missf.top/post/f03bbd11.html","excerpt":"","text":"Ansible include在所有的编程语言中，在处理重复代码的时候，都会将重复的代码提取出来作为一个逻辑单元，这个逻辑单元通常被称为函数或者方法，这样可以让我们更加方便的、重复的调用这段代码。而且如果需要修改这段代码，只需要修改这段代码本身，那么在调用这段代码的地方的逻辑就会随之改变。同时，使用函数的方式编写代码，能让我们的逻辑更清晰，通过函数的名称，大概能推算出程序的主体作用和逻辑 在 Ansible 中，其实也有类似的功能，这种功能被称之为 include。通过 include，我们可以在 Playbook 中包含另一个文件，以便实现我们刚才所说的函数效果 在配置环境的时候，我们经常会有一些需要重复使用的 Playbook，就像下面的 LAMP 环境和 LNMP 环境 cat lamp.yaml --- - hosts: dbserver remote_user: root tasks: - name: install lamp yum: name: - mysql - php-fpm - httpd state: present cat lnmp.yaml --- - hosts: dbserver remote_user: root tasks: - name: install lnmp yum: name: - mysql - php-fpm - nginx state: present 我们可以把上面的两个 Playbook 改写层下面这样，便于我们直接调用 cat lamp.yaml - yum: name: - mysql - php-fpm - httpd state: present cat lnmp.yaml - yum: name: - mysql - php-fpm - nginx state: present 当我们需要执行这两个 Playbook 时，直接使用 include 调用，Playbook 中的任务就会在被调用处执行 cat lamp_lnmp.yaml - hosts: webserver remote_user: root tasks: - name: install lamp include_tasks: lamp.yaml # 这里执行的是lamp.yaml的内容 - hosts: dbserver remote_user: root tasks: - name: install lnmp include_tasks: lnmp.yaml 给 include 文件传参cat baidu.yaml - shell: ping -c 3 \"{{ baidu }}\" # 在include文件使用变量 cat include_baidu.yaml --- - hosts: dbserver remote_user: root vars: baidu: \"www.baidu.com\" # 定义include文件所需要的变量 tasks: - name: ping baidu include_tasks: baidu.yaml # 调用执行include文件 Import_playbook我们使用 include 关键字可以调用任务列表，但如果想要调用整个 Playbook，则需要 import_playbook 模块代替 include 模块 cat import_test.yaml --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"test task cat import_test.yaml\" - import_playbook: intest7.yaml # 调用intest7.yaml整个yaml cat intest7.yaml --- - hosts: webserver remote_user: root tasks: - debug: msg: \"test task cat intest7.yaml\"","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible lookups 插件","slug":"Ansible lookups 插件","date":"2020-06-10T11:46:58.000Z","updated":"2020-09-26T09:32:14.000Z","comments":true,"path":"post/8dfd5e56.html","link":"","permalink":"https://www.missf.top/post/8dfd5e56.html","excerpt":"","text":"Ansible 的 lookups 插件过滤器其实是 Ansible 中的一种插件，除了过滤器之外，Ansible 中还有很多其他种类的插件。而且我们一直都在使用这些插件，比如我们在配置 Ansible 的主机清单时，就用到了 Inventory 种类的插件。lookups 其实也是插件的一种，lookups 的所有操作都是在控制节点上进行的，与管理节点无关。Ansible 官网为我们总结了各个插件的作用，并且按照这些插件功能进行了分类 官网插件地址 lookups file 插件可以读取文件，插件的源码是使用 Python 读取文件然后把结果返回给变量 --- - hosts: dbserver remote_user: root vars: contents: \"{{ lookup('file','/etc/hostname') }}\" tasks: - name: debug lookup file debug: msg: \"the contents is {{ contents }}\" 如果不想将所有文件的内容变成一整个字符串，而是获取一个字符串列表，可以使用 wantlist 参数 --- - hosts: dbserver remote_user: root vars: contents: \"{{ lookup('file','/etc/hostname',wantlist=true) }}\" tasks: - name: debug lookup file debug: msg: \"the contents is {{ contents }}\" lookups password 插件会对传入的内容进行加密处理 --- - hosts: dbserver vars: contents: \"{{ lookup('password','ansible') }}\" # 将ansible这个字符串进行加密处理 tasks: - name: debug lookups debug: msg: \"the contents is {{ contents }}\" lookups pipe 插件运行命令并返回结果，pipe 这个插件底层是使用 Python 的 subprocess 库实现的 --- - hosts: dbserver remote_user: root vars: contents: \"{{ lookup('pipe','hostname') }}\" tasks: - name: debug lookup pipe debug: msg: \"the contents is {{ contents }}\" lookups redis_kv 插件是用来从本地 redis 中读取数据的 --- - hosts: dbserver remote_user: root vars: contents: \"{{ lookup('redis_kv', 'redis://127.0.0.1:6379,ansible') }}\" # 获取本地redis数据库ansible这个键的值 tasks: - name: debug lookup redis_kv debug: msg: \"the contents is {{ contents }}\" lookups dict 插件是用来获取变量的键值对 --- - hosts: dbserver vars: users: alice: female bob: male tasks: - debug: msg: \"{{ item.key }} = {{ item.value }}\" loop: \"{{ lookup('dict',users) }}\" lookups env 插件可以获取 Ansible 主机中指定变量的值 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"{{ lookup('env','PATH') }}\" lookups first_found 插件可以获取列表中第一个找到的文件，如果列表中的所有文件都没有找到，可以添加 errors=ignore 忽略报错 - hosts: master remote_user: root tasks: - name: debug lookup first found debug: msg: \"{{ lookup('first_found',looklist,errors='ignore') }}\" vars: looklist: - \"/abc.txt\" - \"/tmp/str.txt\" lookups dig 插件可以获取指定域名的 IP 地址，需要 Python 安装 dnspython 库 --- - hosts: master remote_user: root tasks: - debug: msg: \"{{ lookup('dig','www.baidu.com',wantlist=true) }}\"","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible 过滤器","slug":"Ansible 过滤器","date":"2020-06-09T11:46:58.000Z","updated":"2020-09-26T07:32:24.000Z","comments":true,"path":"post/f254b261.html","link":"","permalink":"https://www.missf.top/post/f254b261.html","excerpt":"","text":"我曾踏足山巅，也曾进入低谷，二者都让我受益良多。 ​ ——塔里克 Ansible 的过滤器过滤器可以帮助我们对数据进行处理，例如将获取到的变量值中的所有字母都变成大写，过滤器能帮我们实现这样的需求 --- - hosts: dbserver remote_user: root vars: testvar: fesf1dEcd tasks: - debug: msg: \"{{ testvar | upper }}\" # 执行playbook ok: [dbserver] => { \"msg\": \"FESF1DECD\" } 过滤器是一种能够帮助我们处理数据的工具，Ansible中的过滤器功能来自于 jinja2 模板引擎，我们可以借助 jinja2 的过滤器功能在 Ansible 中对数据进行各种处理，而上例中的 upper 就是一种过滤器，这个过滤器的作用就是将小写字母变成大写。当然还有很多其他的过滤器，这些过滤器有些是 jinja2 内置的，有些是 Ansible 特有的，如果这些过滤器都不能满足你的需求，jinja2 也支持自定义过滤器 字符串操作相关的过滤器--- - hosts: dbserver remote_user: root vars: testvar: fesf1dEcd tasks: - debug: msg: \"{{ testvar | upper }}\" # 将全部字母转换成大写 - debug: msg: \"{{ testvar | lower }}\" # 将全部字母转换成小写 - debug: msg: \"{{ testvar | capitalize }}\" # 首字母大写,其他小写 - debug: msg: \"{{ testvar | reverse }}\" # 将字符串反转 - debug: msg: \"{{ testvar | first }}\" # 返回字符串的第一个字符 - debug: msg: \"{{ testvar | last }}\" # 返回字符串的最后一个字符 - debug: msg: \"{{ testvar | trim }}\" # 将字符串开头和结尾的空格去除 - debug: msg: \"{{ testvar | center(width=30) }}\" # 将字符串居中并设置字符串的长度为30,字符串两边用空格填充 - debug: msg: \"{{ testvar | length }}\" # 返回字符串长度,length与count等效,可以写为count - debug: msg: \"{{ testvar | list }}\" # 将字符串转换成列表,每个字符作为一个元素 - debug: msg: \"{{ testvar | shuffle }}\" # 将字符串转换成列表,每个字符作为一个元素,并且随机打乱顺序(洗牌) 数字操作相关的过滤器--- - hosts: dbserver remote_user: root vars: tes: -8 tasks: - debug: msg: \"{{ 5 + ('8' | int) }}\" # 把字符串类型的'8'转换为整形后再作计算 - debug: msg: \"{{ '9' | int(default=6) }}\" # 把字符串类型的'a'转换为整形,如果无法转换则返回6 - debug: msg: \"{{ '8' | float }}\" # 将对应的值转换为浮点型,如果无法转换则默认返回'0.0' - debug: msg: \"{{ 'a' | float(8.88) }}\" # 当对应的值无法返回时则返回指定的'8.88' - debug: msg: \"{{ tes | abs }}\" # 获取这个变量的绝对值 - debug: msg: \"{{ 15.2 | round }}\" # 四舍五入 - debug: msg: \"{{ 3.14159 | round(2) }}\" # 保留小数点后两位 - debug: msg: \"{{ 100 | random }}\" # 从0到100中返回一个随机数 - debug: msg: \"{{ 10 | random(start=5) }}\" # 从5到10中返回一个随机数 - debug: msg: \"{{ 15 | random(start=5,step=2) }}\" # 从5到15中返回一个随机数,步长为2 - debug: msg: \"{{ 15 | random(step=5) }}\" # 从0到10中返回一个随机数,这个数是5的倍数 列表操作相关的过滤器--- - hosts: dbserver remote_user: root tasks: vars: var1: [32,45,63,76,58] var2: [23,[34,65],34,80] var3: [23,'r',87] var4: ['fr',['po','qE'],'tT','IO'] var5: ['bc',1,5,'b','c'] var6: ['bc',5,'b','g'] tasks: - debug: msg: \"{{ var1 | length }}\" # 返回列表长度,length与count等效 - debug: msg: \"{{ var1 | first }}\" # 返回列表中的第一个值 - debug: msg: \"{{ var1 | last }}\" # 返回列表中的最后一个值 - debug: msg: \"{{ var1 | min }}\" # 返回列表中最小的值 - debug: msg: \"{{ var1 | max }}\" # 返回列表中最大的值 - debug: msg: \"{{ var1 | sort }}\" # 将列表升序排序输出 - debug: msg: \"{{ var1 | sort(reverse=true) }}\" # 将列表降序排序输出 - debug: msg: {{ var1 | sum }}\" # 返回纯数字非嵌套列表中所有数字的和 - debug: msg: \"{{ var2| flatten }}\" # 如果列表中包含列表,就把列表拉平为一个列表 - debug: msg: \"{{ var2 | flatten(levels=1) }}\" # 如果列表中嵌套了多层列表,就把第一层列表拉平 - debug: msg: \"{{ var2 | flatten | max }}\" # 将嵌套列表拉平之后取列表中的最大值 - debug: msg: \"{{ var3 | join }}\" # 将列表中的元素连接成一个字符串 - debug: msg: \"{{ var3 | join(',') }}\" # 将列表中的元素以','分割连接成一个字符串 - debug: msg: \"{{ var3 | random }}\" # 从列表中返回一个随机值 - debug: msg: \"{{ var3 | shuffle }}\" # 随机打乱列表元素的顺序 - debug: msg: \"{{ var4 | upper }}\" # 将列表中的每个元素变成纯大写 - debug: msg: \"{{ var4 | lower }}\" # 将列表中的每个元素变成纯小写 - debug: msg: \"{{ var5 | unique }}\" # 去掉列表中重复的元素,重复的元素只留一个 - debug: msg: \"{{ var5 | union(var6) }}\" # 合并列表,重复元素只留一个(并集) - debug: msg: \"{{ var5 | intersect(var6) }}\" # 取出两个列表的交集元素,重复的元素只留一个 变量未定义相关的过滤器--- - hosts: dbserver remote_user: root vars: var1: '' tasks: - debug: msg: \"{{ var0 | default('missf.top') }}\" # 如果变量没有定义则返回一个默认值,如果定义了变量即使变量值为空还是会输出变量值 - debug: msg: \"{{ var1 | default('coding',boolean=true) }}\" # 如果变量未定义或者变量值为空,则返回默认值 - debug: \"{{ var0 | mandatory }}\" # 如果变量未定义,则报出\"Mandatory variable 'var0' not defined\"错误而不是默认错误 常用过滤器--- - hosts: dbserver remote_user: root vars: users: - name: mj age: 15 hobby: - egm - book - name: mk age: 17 hobby: - pq - jk tasks: - debug: msg: \"{{ users | map(attribute='name') | list }}\" # 从users列表中获取到每个元素所共有的某个属性的值,并将这些值组成一个列表 - debug: msg: \"{{ (name == 'missf') | ternary('Mr','Ms') }}\" # 如果name变量的值是missf,那么对应的值则为Mr,否则则为Ms vars: name: 'missf' - debug: msg: \"{{ tg | basename }}\" # 可以获取到一个路径字符串中的文件名 vars: tg: \"/etc/hosts\" - debug: msg: \"{{ win | win_basename }}\" # 可以获取到windows路径字符串中的文件名 vars: win: \"C:\\studio\\missf\" - debug: msg: \"{{ path | realpath }}\" # 可以获取软链接文件所指向的真正文件 vars: path: \"/tmp/linkfile\" - debug: msg: \"{{ path | splitext }}\" # 可以将文件名后缀带有'.'的部分分开 vars: path: \"/etc/yum.conf\" - debug: msg: \"{{ path | splitext | last }}\" # 将字符串以'.'分开后取最后一个 vars: path: \"/etc/yum.conf\" - debug: msg: \"{{ vt | to_uuid }}\" # 为字符串生成uuid vars: vt: \"this is test\" - debug: msg: \"{{ sk | bool }}\" # 字符串内容为'yes','1','True','true'则返回true,内容为其他则返回false vars: sk: \"yes\" - debug: msg: \"{{ ('2016-08-16 12:00:49' | to_datetime) - ('2012-03-25 19:03:15' | to_datetime) }}\" # 使用to_datetime关键字计算时间差,默认转换的字符串的格式必须是'%Y-%m-%d %H:%M:%S' - debug: msg: \"{{ ('20160814'| to_datetime('%Y%m%d')) - ('2012-12-25 19:00:00' | to_datetime) }}\" # 如果对应的字符串不是这种格式,则需要在to_datetime中指定与字符串相同的时间格式,才能正确的转换为时间类型 - debug: msg: \"{{ '123456' | hash('sha1') }}\" # 使用sha1算法对字符串进行哈希 - debug: msg: \"{{ '123456' | password_hash('md5','ffsfsfsfsfscs') }}\" # 使用md5算法加密，指定字符串作为盐值 密码验证示例--- - hosts: dbserver remote_user: root vars_prompt: - name: username prompt: \"input username\" # 用户输入用户名 - name: password prompt: \"input password\" # 用户输入密码 tasks: - debug: msg: \"{{ username | hash('md5') }}\" # 将用户名hash register: username_md5 - debug: msg: \"{{ password | hash('md5') }}\" # 将密码hash register: password_md5 - debug: msg: \"username yes\" when: username_md5.msg == 'fac2db1a64bc2a16887e9bdf17e15f8e' # 通过比对(用户输入的用户名)和(剧本写死的MD5值)确认密码 - debug: msg: \"password yes\" when: password_md5.msg == 'e10adc3949ba59abbe56e057f20f883e'","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible 条件判断","slug":"Ansible 条件判断","date":"2020-06-08T10:27:28.000Z","updated":"2020-09-28T05:57:14.000Z","comments":true,"path":"post/87ce98f8.html","link":"","permalink":"https://www.missf.top/post/87ce98f8.html","excerpt":"","text":"Ansible 的条件判断绝大数语言中，都使用 if 作为条件判断的方法，而在 Ansible 中，条件判断使用关键字 when 简单示例使用 when 关键字指明条件是：ansible_distribution == “CentOS”，这里的 ansible_distribution 是 facts 信息中的一个 key，我们在调用 ansible_distribution 变量时并没有为它添加 { { var } }，这是在 when 关键字中引用变量时，不需要加 { { var } }。如果 ansible_distribution == “CentOS” 这个条件成立，那么就调用 debug 模块，打印 msg 的内容，如果不成立则不执行 debug 模块 # 简单示例 --- - hosts: dbserver remote_user: root tasks: - debug: msg: 'system release is centos' when: ansible_distribution == \"CentOS\" 配合循环进行判断定义条件为 item &gt; 1，只有条件成立时 item 才会被打印，而 with_items 会循环列表中的值，item 变量的值不断变化 --- - hosts: dbserver remote_user: root tasks: - debug: msg: '{{ item }}' with_items: - 1 - 2 - 3 when: item > 1 条件判断通配符 运算符 条件 结果 == 两个值是否相等 相等则为真 != 两个值是否不相等 不等则为真 &gt; 左边的值大于右边的值 则为真 &lt; 左边的值小于右边的值 则为真 &gt;= 左边的值大于或等于右边的值 则为真 &lt;= 左边的值小于或等于右边的值 则为真 逻辑运算符 运算符 条件 结果 and 当左边与右边同时为真 则返回真 or 当左边与右边有任意一个为真 则返回真 not 对一个操作取反 () 将一组操作包装在一起 逻辑运算和分组示例--- - hosts: dbserver remote_user: root tasks: - debug: msg: 'System release is centos6 or centos7' when: ansible_distribution == \"CentOS\" and (ansible_distribution_major_version == \"6\" or ansible_distribution_major_version == \"7\") 判断模块执行返回的信息我们在执行 shell 命令时，通常需要获取命令的返回信息，这样才能够根据返回的信息，判断之后的操作如何进行下去 --- - hosts: dbserver remote_user: root tasks: - name: touch file1 shell: \"touch /file1\" register: returnmsg - name: debug file1 debug: msg: \"Command execution successful\" when: returnmsg.rc == 0 # 命令执行成功的返回值中rc的值为0 - name: debug file2 debug: msg: \"Command execution failed\" when: returnmsg.rc != 0 跳过执行遇到的错误有些时候我们的 tasks 执行到一半时，遇到错误之后就不再继续往下执行了，我们可以使用 ignore_errors 关键字去跳过这个错误，让剧本继续往下执行 --- - hosts: dbserver remote_user: root tasks: - name: touch file1 shell: \"ls /file123\" register: returnmsg ignore_errors: true - name: debug file1 debug: msg: \"Command execution successful\" when: returnmsg.rc == 0 - name: debug file2 debug: msg: \"Command execution failed\" when: returnmsg.rc != 0 条件判断与testsLinux 系统中可以使用一些常用的判断操作，例如使用 test 命令判断文件或者目录是否存在 test /etc/ echo $? 0 Ansible 中也有类似用于判断文件和目录的方法，注意这个是判断控制节点上的文件和目录，”is exists” | “is not exists” --- - hosts: dbserver remote_user: root vars: testpath: /etc tasks: - debug: msg: \"file exist\" when: testpath is exists # 判断testpath变量这个路径是否存在，存在则为真，打印msg内容 ok: [dbserver] => { \"msg\": \"file exist\" } 判断变量的 tests defined：判断变量是否已经定义，已经定义则返回真 undefind：判断变量是否已经定义，未定义则返回真 none：判断变量值是否为空，如果变量已经定义，但是变量值为空，则返回真 定义变量 f1 赋值为 test，定义 h2 但不赋值，debug 模块根据对应的条件是否为真打印具体 msg 内容 --- - hosts: dbserver remote_user: root tasks: vars: f1: 'test' h2: tasks: - debug: msg: 'varf1 is undefined' when: f1 is undefined - debug: msg: 'The variable is defined, but there is no value' when: h2 is none 判断执行结果的 tests success 或 succeeded：通过任务的返回信息判断任务的执行状态，任务执行成功则返回真 failure 或 failed：通过任务的返回信息判断任务的执行状态，任务执行失败则返回真 change 或 changed：通过任务的返回信息判断任务的执行状态，任务执行状态为 changed 则返回真 skip 或 skipped：通过任务的返回信息判断任务的执行状态，当任务没有满足条件而被跳过执行时，则返回真 使用 shell 模块执行一条命令，并且使用 register 将返回值存进 returnmsg 变量，如果命令执行报错就跳过继续往下执行，下面再判断这个变量的执行结果 --- - hosts: dbserver remote_user: root tasks: - shell: \"ls /etc/hosts\" register: returnmsg ignore_errors: true - debug: msg: \"success\" when: returnmsg is success - debug: msg: \"failed\" when: returnmsg is failure - debug: msg: \"changed\" when: returnmsg is changed - debug: msg: \"skip\" when: \"returnmsg is skip\" 判断路径的 tests file：判断路径是否是一个文件，如果是则返回真 directory：判断路径是否是一个目录，如果是则返回真 link：判断路径是否是一个软链接，如果是则返回真 mount：判断路径是否是一个挂载点，如果是则返回真 exists：判断路径是否存在，如果存在则返回真 判断字符串的 tests lower：判断包含字母的字符串中的字母是否是纯小写，如果是则返回真 upper：判断包含字母的字符串中的字母是否是纯大写，如果是则返回真 判断整除的 tests even：判断数值是否是偶数，如果是则返回真 odd：判断数值是否是奇数，如果是则返回真 divisibleby(num)：判断是否可以整除指定的数值，如果可以整除则返回真 条件判断与 block想要在条件成立时，执行多个任务，我们不需要在每个任务中都加入判断条件，我们可以使用 block 关键字将多个任务整合成一个块 --- - hosts: dbserver remote_user: root tasks: - shell: \"ls /loo\" ignore_errors: true - block: # 这个block块有两个任务 - debug: msg: \"run command failed\" - file: path: \"/oo\" state: touch when: 2 > 1 # 条件成立就执行block块的任务 block 与 rescue 的错误处理我们在处理某些复杂的任务时，需要使用到错误处理的判断，使我们的 Playbook 更加灵活。例如我需要执行多个任务，这多个任务中只要有一个执行失败，就会触发错误处理，执行我们提前定义好的救援任务进行补救 --- - hosts: dbserver remote_user: root tasks: - block: - shell: \"ls /123\" - debug: msg: \"tcodmf\" - file: path: \"/loo\" state: touch rescue: - debug: msg: \"error\" # 只要block块里面的三个任务有一个执行失败就会执行rescue定义好的任务 block 和 always 的错误处理如果 block 中的任务执行出错，那么就会执行 rescue 中的任务，如果 block 中的任务执行没有出错，那么 rescue 中的任务就不会执行，但是 always 中的任务是无论如何都会执行的，不管 block 中的内容是否出错 --- - hosts: dbserver remote_user: root tasks: - block: - debug: msg: \"echo 123\" - command: cat /etc/hosts | wc -l # command模块不能执行带特殊字符的命令，第一次出错 - debug: msg: \"echo 456\" rescue: - debug: msg: \"echo error1\" - command: cat /etc/hosts | wc -l # command模块不能执行带特殊字符的命令，第二次出错 always: - debug: msg: \"echo error2\" # 最后执行always 条件判断与错误处理我们有时候需要在判断条件成立时，执行退出的指令，使 Playbook 中断执行，这里我们需要使用到 fail 模块。我们知道，在执行 Playbook 时，如果 Playbook 中的任意一个任务执行失败，Playbook 都会终止执行，而 fail 模块就是天生用来执行失败的模块。只要Playbook 中执行fail模块，Playbook 就会认为有任务执行失败了 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"123\" - debug: msg: \"456\" - fail: # 执行fail模块中断playbook执行,后面的任务不再执行 - debug: msg: \"789\" 使用 fail 模块终止 Playbook，默认打印 “Failed as requested from task” 的错误提示，这个我们可以自定义 fail 模块的错误提示 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"123\" - debug: msg: \"456\" - fail: msg: \"stop operation playbook\" - debug: msg: \"789\" 利用条件判断去控制 fail 模块的执行 --- - hosts: dbserver remote_user: root tasks: - shell: \"echo 'This is a string for testing--error'\" register: return_value # 取到shell模块执行的返回值 - fail: msg: \"stop operation playbook\" when: \"'error' in return_value.stdout\" # 判断字符串是否存在于return_value.stdout这个输出信息 - debug: msg: \"playbook has stopped\" # 由于fail模块执行而中断playbook,这个将不会执行 failed_when 关键字 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"123456\" - shell: \"echo 'This is a string for testing--error'\" register: return_value failed_when: '\"error\" in return_value.stdout' # 如果条件成立,那么failed_when就会提示所对应的shell模块执行失败 - debug: msg: \"654321\" # 不会被打印 failed_changed 关键字 正常情况下，debug 模块正常执行的情况下只能是 “ok” 状态，我们可以使用 failed_changed 关键字改变执行后的状态定义为 changed --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"test message\" changed_when: 2 > 1 changed: [dbserver] => { \"msg\": \"test message\" }","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible 循环","slug":"Ansible 循环","date":"2020-06-01T02:47:20.000Z","updated":"2020-09-26T07:24:48.000Z","comments":true,"path":"post/b39a1b7d.html","link":"","permalink":"https://www.missf.top/post/b39a1b7d.html","excerpt":"","text":"Ansible 的循环我们在编写 Playbook 的时候，不可避免的要执行一些重复性操作，比如指定安装软件包，批量创建用户，操作某个目录下的所有文件等。Ansible 一门简单的自动化语言，所以流程控制、循环语句这些编程语言的基本元素它同样都具备。 Ansible 提供了两个用于创建循环的关键字: loop 和 with_，Ansible 2.5 中添加了 loop，但它还不是 with_ 的完全替代品。在官方推荐使用 loop，但我们现在还可以在大多数用例中使用 with_，但是随着 loop 语法的不断改进，with_ 以后可能会失效 标准循环使用 with_items 关键字创建一个循环的列表，with_items 会把列表的每一条信息，单独放到 item 变量里面，然后循环打印每次 item 变量的值 # 方式1 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"{{ item }}\" with_items: - 1 - 2 - 3 # 方式2 --- - hosts: dbserver remote_user: root tasks: - debug: msg={{ item }} with_items: [1,2,3] # 方式3 --- - hosts: dbserver remote_user: root vars: list: - a - b - c tasks: - debug: msg={{ item }} with_items: '{{ list }}' # 方式4 --- - hosts: dbserver remote_user: root vars: list: [1,2,3] tasks: - debug: msg={{ item }} with_items: '{{ list }}' # 添加多个用户 --- - hosts: dbserver remote_user: root tasks: - name: add server users user: name: \"{{ item }}\" state: present groups: server with_items: - server1 - server2 定义稍微复杂的列表自定义列表中的每一个键值对都是一个对象，我们可以通过对象的属性对应的 “键”，获取到对应的 “值”，执行下面的 Playbook 之后，mm 和 nn 都会被输出 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"{{ item.name }}\" with_items: - { name: mm, age: 23} - { name: nn, age: 24} ansible-playbook item.yaml # 执行playbook ok: [dbserver] => (item={u'age': 23, u'name': u'mm'}) => { \"msg\": \"mm\" } ok: [dbserver] => (item={u'age': 24, u'name': u'nn'}) => { \"msg\": \"nn\" } 利用循环创建多个文件# 没学习循环之前可能这样创建多个文件 --- - hosts: dbserver remote_user: root gather_facts: no tasks: - file: path: '/opt/a' state: touch - file: path: '/opt/b' state: touch - file: path: '/opt/c' state: touch - file: path: '/opt/d' state: touch # 使用循环的方式 --- - hosts: dbserver remote_user: root gather_facts: no vars: dirs: - '/opt/a' - '/opt/b' - '/opt/c' - '/opt/d' tasks: - file: path: '{{ item }}' state: touch with_items: '{{ dirs }}' 利用循环多次调用模块不使用循环的情况下调用模块，返回的信息是这样的 --- - hosts: dbserver remote_user: root tasks: - shell: 'ls /etc' register: returnvalue - debug: var: returnvalue ansible-playbook item.yaml ok: [dbserver] => { \"returnvalue\": { \"changed\": true, \"cmd\": \"ls /etc\", \"delta\": \"0:00:00.025062\", \"end\": \"2020-06-02 01:06:34.741709\", \"failed\": false, \"rc\": 0, \"start\": \"2020-06-02 01:06:34.716647\", \"stderr\": \"\", \"stderr_lines\": [],这里省略...... } 我们使用循环重复调用了 shell 模块两次，分别执行了两条命令，然后将 shell 模块的返回值存放到了 returnvalue 变量中，最后使用 debug 模块输出了 returnvalue 变量的值。当使用了循环之后，每次 shell 模块执行后的返回值都会放入一个名为 results 的序列中，其实，results 也是一个返回值，当模块中使用了循环时，模块每次执行的返回值都会追加存放到 results 这个返回值中，所以，我们可以通过 results 关键字获取到每次模块执行后的返回值 --- - hosts: dbserver remote_user: root tasks: - shell: '{{ item }}' with_items: - 'ls /etc' - 'ls /var' register: returnvalue - debug: var: returnvalue ansible-playbook item.yaml ok: [dbserver] => { \"returnvalue\": { \"changed\": true, \"msg\": \"All items completed\", \"results\": [ { \"ansible_loop_var\": \"item\", \"changed\": true, \"cmd\": \"ls /etc\", \"delta\": \"0:00:00.026532\", \"end\": \"2020-06-02 01:08:22.264277\", \"failed\": false, \"invocation\": { 这里省略...... } 先使用循环重复的调用了 shell 模块，然后将 shell 模块每次执行后的返回值注册到了变量 returnvalue 中，之后，在使用 debug 模块时，通过返回值 results 获取到了之前每次执行shell模块的返回值(shell 每次执行后的返回值已经被放入到 item 变量中)，最后又通过返回值 stdout 获取到了每次 shell 模块执行后的标准输出 --- - hosts: dbserver remote_user: root tasks: - shell: '{{ item }}' with_items: - 'ls /etc' - 'ls /var' register: returnvalue - debug: msg: '{{ item.stdout }}' with_items: '{{ returnvalue.results}}' 打印序列中的序列with_items 块序列下面有一个自定义列表 [1,2,3]，执行 Playbook 会循环打印 [1,2,3] 列表里的每一个值 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"{{ item }}\" with_items: [1,2,3] ok: [dbserver] => (item=1) => { \"msg\": 1 } ok: [dbserver] => (item=2) => { \"msg\": 2 } ok: [dbserver] => (item=3) => { \"msg\": 3 } with_items 块序列下面有两个自定义列表，执行 Playbook 还是会循环打印两个列表里的每一个值 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"{{ item }}\" with_items: - [1,2,3] - [4,5,6] ok: [dbserver] => (item=1) => { \"msg\": 1 } ok: [dbserver] => (item=2) => { \"msg\": 2 } ok: [dbserver] => (item=3) => { \"msg\": 3 } ok: [dbserver] => (item=4) => { \"msg\": 4 } ok: [dbserver] => (item=5) => { \"msg\": 5 } ok: [dbserver] => (item=6) => { \"msg\": 6 } 当 with_items 块序列下面有两个自定义的列表时，我们如何让 debug 模块将每个小列表作为一个小整体输出，而不应该输出小列表中的每个元素呢？我们可以使用 with_list 关键字，替换上例 Playbook 中的 with_items 关键字 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"{{ item }}\" with_list: - [1,2,3] - [4,5,6] ok: [dbserver] => (item=[1, 2, 3]) => { # with_list块序列只会循环最外层的每一项,而with_items则是循环处理每一个元素 \"msg\": [ 1, 2, 3 ] } ok: [dbserver] => (item=[4, 5, 6]) => { \"msg\": [ 4, 5, 6 ] } 元素对齐合并with_together 可以将两个列表中的元素对齐合并，如果两个列表元素不一致，缺少的元素值为 null --- - hosts: dbserver remote_user: root tasks: - debug: msg: '{{ item }}' with_together: - [1,2,3] - [a,b,c] ok: [dbserver] => (item=[1, u'a']) => { \"msg\": [ 1, \"a\" ] } ok: [dbserver] => (item=[2, u'b']) => { \"msg\": [ 2, \"b\" ] } ok: [dbserver] => (item=[3, u'c']) => { \"msg\": [ 3, \"c\" ] } 元素两两组合需求：我们需要创建三个目录，这三个目录下面都有相同的子目录，我们使用 ansible-playbook 的方式去循环创建，需要用到 with_cartesian 这个关键字 # 需要创建的目录结构如下: dir1/sofm dir1/bin dir2/sofm dir2/bin dir3/sofm dir3/bin --- - hosts: dbserver remote_user: root tasks: - file: state: directory path: '/{{ item[0] }}/{{ item[1] }}' with_cartesian: - [dir1,dir2,dir3] - [sofm,bin] 执行 Playbook 会将两个列表的元素两两组合，使用 item[0] 和 item[1] 来获取每一次循环的值 PLAY [dbserver] ****************************************************************************************************************** TASK [Gathering Facts] *********************************************************************************************************** ok: [dbserver] TASK [file] ********************************************************************************************************************** changed: [dbserver] => (item=[u'dir1', u'sofm']) changed: [dbserver] => (item=[u'dir1', u'bin']) changed: [dbserver] => (item=[u'dir2', u'sofm']) changed: [dbserver] => (item=[u'dir2', u'bin']) changed: [dbserver] => (item=[u'dir3', u'sofm']) changed: [dbserver] => (item=[u'dir3', u'bin']) PLAY RECAP *********************************************************************************************************************** dbserver : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 列表元素添加索引编号使用 with_indexed_items 关键字可以为列表的每一个元素添加索引编号，索引编号从0开始，我们可以在出来列表每一项元素的时候获取到索引编号 --- - hosts: dbserver remote_user: root tasks: - debug: msg: 'index is {{ item[0] }},value is {{ item[1] }}' with_indexed_items: - index1 - index2 - index3 ok: [dbserver] => (item=[0, u'index1']) => { \"msg\": \"index is 0,value is index1\" } ok: [dbserver] => (item=[1, u'index2']) => { \"msg\": \"index is 1,value is index2\" } ok: [dbserver] => (item=[2, u'index3']) => { \"msg\": \"index is 2,value is index3\" } 生成数字序列假如需要在管理节点创建 dir2、dir4、dir6 这样的目录，我们该如何使用循环去创建呢，这里就需要使用到 with_sequence 这个关键字去生成数字序列。debug 模块被调用了4次，从2开始，到8结束，每一次增加2(步长)，看到这是不是有了 Python 的感觉呢 --- - hosts: dbserver remote_user: root tasks: - debug: msg: '{{ item }}' with_sequence: start=2 end=8 stride=2 ok: [dbserver] => (item=2) => { \"msg\": \"2\" } ok: [dbserver] => (item=4) => { \"msg\": \"4\" } ok: [dbserver] => (item=6) => { \"msg\": \"6\" } ok: [dbserver] => (item=8) => { \"msg\": \"8\" } 创建 dir2、dir4、dir6 这样不连续的目录 --- - hosts: dbserver remote_user: root tasks: - file: path: /dir{{ item }} state: directory with_sequence: start=2 end=6 stride=2 输出更简单的连续序列--- - hosts: dbserver remote_user: root tasks: - debug: msg: '{{ item }}' with_sequence: count=5 注意：当我们不指定 start 的值时，start 的值默认为1，但是当 end 的值小于 start 时则必须指定 stride，而且 stride 的值必须是负数 返回一个随机值使用 with_random_choice 这个关键字可以让我们从一个列表的多个值中随机返回一个值 --- - hosts: dbserver remote_user: root tasks: - debug: msg: '{{ item }}' with_random_choice: - qwe - rtd - fdv - oki ok: [dbserver] => (item=oki) => { \"msg\": \"oki\" # 随机返回列表中的一个值 } 使用循环去操作字典这里我们学习一个叫 with_dict 的字典关键字，下面来看看字典的使用场景 定义一个 users 变量，users 有两个用户，我们使用 with_dict 关键字处理这个字典格式的变量 --- - hosts: dbserver remote_user: root vars: users: alix: feom boo: mair tasks: - debug: msg: '{{ item }}' with_dict: '{{ users }}' ok: [dbserver] => (item={'value': u'feom', 'key': u'alix'}) => { \"msg\": { \"key\": \"alix\", # users变量经过with_dict处理之后，键值对分别被放入key和value关键字中 \"value\": \"feom\" } } ok: [dbserver] => (item={'value': u'mair', 'key': u'boo'}) => { \"msg\": { \"key\": \"boo\", # 我们可以通过key关键字和value关键字分别获取到字典中键值对的键和值 \"value\": \"mair\" } } 字典定义和取值--- - hosts: dbserver remote_user: root vars: users: alix: name: feom gender: female phone: 155464615 boo: name: mair gender: male phone: 179444684 tasks: - debug: msg: '{{ item }} alix phone is {{ item.value.phone }}' # 使用item.value.phone的方法取某一项的值 with_dict: '{{ users }}' ok: [dbserver] => (item={'value': {u'gender': u'female', u'name': u'feom', u'phone': 155464615}, 'key': u'alix'}) => { \"msg\": \"{'key': u'alix', 'value': {u'gender': u'female', u'name': u'feom', u'phone': 155464615}} alix phone \\\\n is 155464615\" } ok: [dbserver] => (item={'value': {u'gender': u'male', u'name': u'mair', u'phone': 179444684}, 'key': u'boo'}) => { \"msg\": \"{'key': u'boo', 'value': {u'gender': u'male', u'name': u'mair', u'phone': 179444684}} alix phone \\\\n is 179444684\" } 遍历每一项子元素users 变量列表中有两个块序列，这两个块序列分别代表两个用户，bob 和alice，变量 users 经过 with_subelements 处理时还指定一个hobby属性，hobby 属性正是 users 变量中每个用户的子属性 --- - hosts: dbserver remote_user: root vars: users: - name: bob gender: male hobby: - skateboard - videogame - name: alice gender: female hobby: - music - name: qwe hobby: - da tasks: - debug: msg: \"{{ item }}\" with_subelements: - \"{{ users }}\" - hobby 上面的 Playbook 执行后得到如下结果，我们在使用 with_subelements 处理变量 users 时指定了 hobby 属性，hobby 属性中的每一个子元素都被当做一个整体，而其他的子元素作为另一个整体，组成了键值对 ok: [dbserver] => (item=[{u'gender': u'male', u'name': u'bob'}, u'skateboard']) => { \"msg\": [ { \"gender\": \"male\", \"name\": \"bob\" }, \"skateboard\" ] } ok: [dbserver] => (item=[{u'gender': u'male', u'name': u'bob'}, u'videogame']) => { \"msg\": [ { \"gender\": \"male\", \"name\": \"bob\" }, \"videogame\" ] } ok: [dbserver] => (item=[{u'gender': u'female', u'name': u'alice'}, u'music']) => { \"msg\": [ { \"gender\": \"female\", \"name\": \"alice\" }, \"music\" ] } ok: [dbserver] => (item=[{u'name': u'qwe'}, u'da']) => { \"msg\": [ { \"name\": \"qwe\" }, \"da\" ] } 获取控制节点的文件内容我想要获取控制节点上的几个文件的内容，那么可以使用 with_file 关键字，循环获取到文件的内容，这里 hosts 指定的是 dbserver 这个管理节点，但是无论管理节点写的是什么都不影响，因为我们读取的是管理节点的文件 --- - hosts: dbserver remote_user: root tasks: - debug: msg: '{{ item }}' with_file: - /etc/passwd - /etc/hosts ok: [dbserver] => (item=root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin adm:x:3:4:adm:/var/adm:/sbin/nologin lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin sync:x:5:0:sync:/sbin:/bin/sync shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown halt:x:7:0:halt:/sbin:/sbin/halt mail:x:8:12:mail:/var/spool/mail:/sbin/nologin operator:x:11:0:operator:/root:/sbin/nologin games:x:12:100:games:/usr/games:/sbin/nologin ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin nobody:x:99:99:Nobody:/:/sbin/nologin systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin dbus:x:81:81:System message bus:/:/sbin/nologin polkitd:x:999:998:User for polkitd:/:/sbin/nologin sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin postfix:x:89:89::/var/spool/postfix:/sbin/nologin ntp:x:38:38::/etc/ntp:/sbin/nologin) => { \"msg\": \"root:x:0:0:root:/root:/bin/bash\\nbin:x:1:1:bin:/bin:/sbin/nologin\\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\\nadm:x:3:4:adm:/var/adm:/sbin/nologin\\nlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\\nsync:x:5:0:sync:/sbin:/bin/sync\\nshutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\\nhalt:x:7:0:halt:/sbin:/sbin/halt\\nmail:x:8:12:mail:/var/spool/mail:/sbin/nologin\\noperator:x:11:0:operator:/root:/sbin/nologin\\ngames:x:12:100:games:/usr/games:/sbin/nologin\\nftp:x:14:50:FTP User:/var/ftp:/sbin/nologin\\nnobody:x:99:99:Nobody:/:/sbin/nologin\\nsystemd-network:x:192:192:systemd Network Management:/:/sbin/nologin\\ndbus:x:81:81:System message bus:/:/sbin/nologin\\npolkitd:x:999:998:User for polkitd:/:/sbin/nologin\\nsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin\\npostfix:x:89:89::/var/spool/postfix:/sbin/nologin\\nntp:x:38:38::/etc/ntp:/sbin/nologin\" } ok: [dbserver] => (item=127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6) => { \"msg\": \"127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4\\n::1 localhost localhost.localdomain localhost6 localhost6.localdomain6\" } 匹配控制节点的文件我们可以通过通配符去匹配控制节点上的文件，这里需要使用到 with_fileglob 这个关键字。注意 with_fileglob 只能是匹配到文件 --- - hosts: dbserver remote_user: root tasks: - debug: msg: '{{ item }}' with_fileglob: - /etc/* - /tmp/* # 这里写成/dir/的话是匹配不到文件的，需要使用*通配符 ok: [dbserver] => (item=/etc/fstab) => { \"msg\": \"/etc/fstab\" } ok: [dbserver] => (item=/etc/crypttab) => { \"msg\": \"/etc/crypttab\" } ok: [dbserver] => (item=/etc/mtab) => { \"msg\": \"/etc/mtab\" } ok: [dbserver] => (item=/etc/resolv.conf) => { \"msg\": \"/etc/resolv.conf\" } ok: [dbserver] => (item=/etc/magic) => { \"msg\": \"/etc/magic\" }...... Ansible 的 loop 循环在2.5版本之前的 Ansible 中，大多数人习惯使用 “with_X” 风格的关键字操作循环，从2.6版本开始，官方开始推荐使用 “loop” 关键字代替 “with_X” 风格的关键字。现在就来聊聊这种新的方式，以便能够更好的从老版本的使用习惯过渡过来 loop标准循环 --- - hosts: dbserver remote_user: root tasks: - debug: msg: \"{{ item }}\" loop: - abc - cde loop 循环安装软件 --- - hosts: dbserver remote_user: root tasks: - name: install packages yum: name: \"{{ item }}\" state: latest loop: - rsync - sl - psmisc loop 批量创建用户 --- - hosts: dbserver remote_user: tasks: - name: \"add user\" user: name: \"{{ item.name }}\" state: present groups: \"{{ item.groups }}\" loop: - {name: \"abc\",groups: \"root\"} - {name: \"cde\",groups: \"root\"}","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible 变量","slug":"Ansible 变量","date":"2020-05-28T02:13:41.000Z","updated":"2020-09-26T07:19:44.000Z","comments":true,"path":"post/e2d2af3c.html","link":"","permalink":"https://www.missf.top/post/e2d2af3c.html","excerpt":"","text":"Ansible的变量在 Ansible 中使用变量，能让我们的工作变得更加灵活 变量定义规则 变量名应该由字母、数字、下划线组成 变量名需要以字母开头 Ansible 内置的关键字不能作为变量名 在 Playbook 中使用变量使用 vars 关键字定义名为 Package1 值为 Nginx 的变量，在 task 中使用 进行调用 --- - hosts: all vars: package1: nginx remote_user: root tasks: - name: task1 yum: name: \"{{ package1 }}\" state: installed 使用块序列化语法定义变量 vars: - testvar1: a1 - testvar2: b2 使用属性的方式定义变量 --- - hosts: all remote_user: root vars: nginx: # 定义两个变量 conf80: /etc/nginx/conf.d/80.conf conf8080: /etc/nginx/conf.d/8080.conf tasks: - name: task1 file: path: \"{{ nginx.conf80 }}\" # 第一种调用方法 state: touch - name: task2 file: path: \"{{ nginx['conf8080']}}\" # 第二种调用方法 注意：如果引用变量时，变量处于开头的位置，那么变量必须要用双引号引起来，否则语法会报错 - name: task2 file: path: \"{{ nginx['conf8080']}}\" # 引用这种变量处于开头位置的必须使用引号引起来 - name: task2 file: path: /root/{{ nginx['conf8080']}} # 这样的不用 引入文件内的变量，创建 nginx_vars.yaml 文件，直接在文件中以自己喜欢的方式定义变量 testvar1: zxc testvar2: qwe - testvar3: rty - testvar4: poi nginx: conf1: /usr/local/nginx/conf/nginx1.conf conf2: /usr/local/nginx/conf/nginx2.conf 在 Playbook 中以 vars_files 关键字引入文件中的变量 --- - hosts: all remote_user: root vars: # vars关键字和vars_files关键字可以同时使用 - /root/vars.yaml vars_files: - /playbook/nginx_vars.yaml tasks: - name: task1 file: path: \"{{ nginx.conf80 }}\" state: touch - name: task2 file: path: \"{{ nginx['conf8080']}}\" 变量与 setup 模块前面我们说过在执行 Playbook 的时候，默认都会运行一个名为 Gathering Facts 的任务，这个任务会收集管理节点的相关信息(例如管理节点的 IP 地址，主机名，系统版本，硬件配置等信息)，这些被收集到的信息都会保存在对应的变量中，我们想要使用这些信息时，可以获取对应的变量，从而使用这些信息。关于 setup 模块具体查看前面 Ansible 模块学习 查看从管理节点收集到的所有相关信息 ansible all -m setup # 由于返回信息的比较多，这里不作示例 查看管理节点的内存使用情况 ansible all -m setup -a 'filter=ansible_memory_mb' 在管理节点创建自定义变量 除了 Ansible 默认收集的信息以外，我们还能够在管理节点写入一些自定义变量，这些自定义变量也是可以被 setup 模块收集到 首先在管理节点创建自定义变量的文件 hello.fact，此类文件必须以 *.fact 命名 # 管理节点 mkdir -p /etc/ansible/facts.d vim /etc/ansible/facts.d/hello.fact [info] name: mwj age: 24 # 控制节点 ansible dbserver -m setup -a \"filter=ansible_local\" # 使用ansible_local关键字过滤信息得到管理节点的自定义变量 10.10.110.122 | SUCCESS => { \"ansible_facts\": { \"ansible_local\": { \"hello\": { \"info\": { \"age\": \"24\", \"name\": \"mwj\" } } }, \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false } 注意：管理节点上的 hello.fact 文件必须不是可执行文件，不然这个文件不会被成功读取，具体可查看 Ansible 官方文档有详细说明。在管理节点的 /etc/ansible/facts.d/ 这个目录是使用 ansible_local 关键字过滤时的默认路径，如果想要自定义路径可以使用 fact_path 关键字定义 ansible dbserver -m setup -a \"fact_path=/tmp/facts.d/\" 变量与 debug 模块debug 模块是帮我们进行调试的，可以把对我们有用的信息输出到控制台上，以便能够定位问题 Playbook 中使用 debug 模块 --- - hosts: all remote_user: root tasks: - name: touch file file: path: /tmp/debug.txt state: touch - name: debug demo debug: msg: this is debug info,File created successfully 执行 Playbook 模块查看信息 如下图所示，在 touch 文件之后会输出我们定义好的 debug 信息 PLAY [all] *********************************************************************************************************************** TASK [Gathering Facts] *********************************************************************************************************** ok: [dbserver] ok: [webserver] TASK [touch file] **************************************************************************************************************** changed: [webserver] changed: [dbserver] TASK [debug demo] **************************************************************************************************************** ok: [webserver] => { \"msg\": \"this is debug info,File created successfully\" } ok: [dbserver] => { \"msg\": \"this is debug info,File created successfully\" } PLAY RECAP *********************************************************************************************************************** dbserver : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 webserver : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 使用 debug 模块输出变量信息 debug 模块除了能够使用 msg 参数输出自定义的信息，还能够使用 var 参数直接输出变量中的信息 --- - hosts: all remote_user: root vars: testvar: this is a debug variable tasks: - name: debug demo debug: var: testvar 使用 debug 模块的 msg 参数一样可以打印变量信息 --- - hosts: all remote_user: root tasks: - name: debug demo debug: msg: \"Remote host memory information: {{ ansible_memory_mb }}\" 执行结果如下 PLAY [all] *********************************************************************************************************************** TASK [Gathering Facts] *********************************************************************************************************** ok: [dbserver] ok: [webserver] TASK [debug demo] **************************************************************************************************************** ok: [webserver] => { \"msg\": \"Remote host memory information: {u'real': {u'total': 216, u'used': 213, u'free': 3}, u'swap': {u'cached': 0, u'total': 1023, u'free': 1022, u'used': 1}, u'nocache': {u'used': 163, u'free': 53}}\" } ok: [dbserver] => { \"msg\": \"Remote host memory information: {u'real': {u'total': 216, u'used': 212, u'free': 4}, u'swap': {u'cached': 0, u'total': 1023, u'free': 1013, u'used': 10}, u'nocache': {u'used': 170, u'free': 46}}\" } PLAY RECAP *********************************************************************************************************************** dbserver : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 webserver : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible_memory_mb 其中其实包含了 “nocache”、”real”、 “swap” 三个部分的信息，我们只想获得 “real” 部分的信息，在 Playbook 中引用变量时可以使用如下示例： msg: \"Remote host memory information: {{ ansible_memory_mb.real }}\" msg: \"Remote host memory information: {{ ansible_memory_mb['real'] }}\" 注册变量Ansible 的模块运行之后都会返回一些返回值，只是默认情况下，这些返回值并不会显示而已，我们可以把这些返回值写入到某个变量中，这样我们就能够通过引用对应的变量从而获取到这些返回值了，这种将模块的返回值写入到变量中的方法被称为注册变量 下面这个 Playbook 有两个任务，第一个任务使用 shell 模块执行了一条命令，然后在这个任务下使用 register 注册了一个 testvar 的变量，第二个任务是使用 debug 模块的 var 参数打印这个变量，最后输出 shell 模块的返回值 --- - hosts: dbserver remote_user: root tasks: - name: test shell shell: \"echo test > /tmp/test\" register: testvar - name: shell module return values debug: var: testvar Playbook 执行的结果如下图，返回的是一个 json 格式的数据 PLAY [dbserver] ****************************************************************************************************************** TASK [Gathering Facts] *********************************************************************************************************** ok: [dbserver] TASK [test shell] **************************************************************************************************************** changed: [dbserver] TASK [shell module return values] ************************************************************************************************ ok: [dbserver] => { \"testvar\": { \"changed\": true, \"cmd\": \"echo test > /tmp/test\", \"delta\": \"0:00:00.025987\", \"end\": \"2020-06-02 22:34:36.185101\", \"failed\": false, \"rc\": 0, \"start\": \"2020-06-02 22:34:36.159114\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"\", \"stdout_lines\": [] } } PLAY RECAP *********************************************************************************************************************** dbserver : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 如果你想要查看模块对应的返回值，可以先查找官方手册，但并不是所有模块的官方手册中都对模块的返回值进行了描述，你可以自己去官网查看模块的返回值，这些返回值不仅仅能够用于输出，通常我们会利用到这些返回值，比如通过模块的返回值决定之后的一些动作，所以注册变量在 Playbook 中还是会被经常用到的，在之后的文章中我们会给出示例 变量与用户交互信息在运行 shell 脚本时，有些时候需要用户输入信息，脚本再根据用户输入的信息决定下一步的动作，这种交互是必须的。我们也可以在Playbook 中实现这种交互，首先提示用户输入信息，然后将用户输入的信息存放到指定的变量中，当我们需要使用这些信息时，只要引用对应的变量即可 下面我们使用 vars_prompt 关键字定义了两个变量，变量名为别为 your_name 和 your_age，变量下面是提示用户输入时的信息 --- - hosts: dbserver remote_user: root vars_prompt: - name: \"your_name\" prompt: \"what is your name\" - name : \"your_age\" prompt: \"how old are you\" tasks: - name: output vars debug: msg: your name is {{your_name}},you are {{your_age}} years old. Playbook 执行如下图，提示用户输入信息时默认是不显示信息的，这和输入密码的场景类似 what is your name: how old are you: PLAY [dbserver] ****************************************************************************************************************** TASK [Gathering Facts] *********************************************************************************************************** ok: [dbserver] TASK [output vars] ************************************************************************************************************** ok: [dbserver] => { \"msg\": \"your name is mwj,you are 24 years old.\" } PLAY RECAP *********************************************************************************************************************** dbserver : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 如果想要用户输入信息时显示信息内容，可以将 private 参数设置为 no --- - hosts: dbserver remote_user: root vars_prompt: - name: \"your_name\" prompt: \"what is your name\" - name : \"your_age\" prompt: \"how old are you\" private: no # 显示用户输入的内容 tasks: - name: output vars debug: msg: your name is {{your_name}},you are {{your_age}} years old. 我们还可以为提示信息设置默认值，如果用户不输入任何信息就将默认值赋予变量，如果用户输入信息，就把输入的信息赋值给变量 --- - hosts: dbserver remote_user: root vars_prompt: - name: \"your_name\" prompt: \"what is your name\\n\" private: no default: mike tasks: - name: output vars debug: msg: your name is {{your_name}} 上面 Playbook 的执行过程如下，中括号内的内容是我们设置的默认值，如果用户直接回车那就将中括号内的内容直接赋值给变量 what is your name [mike]: mwj PLAY [dbserver] ****************************************************************************************************************** TASK [Gathering Facts] *********************************************************************************************************** ok: [dbserver] TASK [output vars] ************************************************************************************************************** ok: [dbserver] => { \"msg\": \"your name is mwj\" } PLAY RECAP *********************************************************************************************************************** dbserver : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 通过命令行传入变量我们可以在执行 Playbook 时直接传入需要使用到的变量。编写一个 Playbook，打印一个 passwd 的变量 --- - hosts: dbserver remote_user: root tasks: - name: \"passing in variables from the command line\" debug: msg: \"{{ passwd }}\" 执行 Playbook 时传入变量 ansible-playbook --extra-vars \"passwd=mdf123456\" passwd.yaml # --extra-vars参数可以简写成-e，还可以一次性传入多个变量，用空格隔开 ansible-playbook -e \"passwd=mdf123456 username=ewe\" passwd.yaml 注意：如果 Playbook 中并没有定义 passwd 变量，在执行 Playbook 时也没有传入 passwd 变量，则会报错。如果在 Playbook 中事先定义好了 passwd 变量，在执行时再次传入名字相同的变量，最终还是以传入的变量值为准，命令行传入的变量的优先级要高于 Playbook 中的变量 不仅 ansible-playbook 可以使用 “-e” 传递变量，Ansible 命令行一样可以，在执行 ad-hoc 命令时可以使用下面的方法传入变量 ansible dbserver -e \"name=mwj\" -m shell -a \"echo my name in {{ name }}\" json 格式传入变量，除了以键值对的方式传入变量，我们还可以传入 json 格式的变量 ansible-playbook passwd.yaml -e '{\"username\":\"mwj\",\"passwd\":\"123456\"}' ansible-playbook passwd.yaml -e '{\"countlist\":[\"one\",\"two\",\"three\",\"four\"]}' # {{countlist[0]}}或{{countlist.0}}引用变量 执行 Playbook 时传入变量文件，编写变量文件，可以是 json 格式或者 yaml 格式的文件 namevar: mwj countlist: - one - two - three - four Playbook 内容调用变量 --- - hosts: dbderver remote_user: root tasks: - name: \"name\" debug: msg: \"{{ namevar }} {{ countlist[0] }}\" 命令行传入对应的文件，使用 @ 符号加上变量文件的路径，变量文件中的所有变量都可以在 Playbook 中引用 ansible-playbook test.yaml -e '@/ansible/var1' 在主机清单中配置变量在主机清单中，可以配置我们的管理节点，也可以将部分管理节点分为一组，其实在配置清单时还可以为主机或主机组设置变量 主机变量：在主机清单中配置变量时，可以同时为管理节点配置对应的变量，当操作这个主机时，即可直接使用对应的变量，而其他主机不能引用到这个变量 # ini风格 dbserver ansible_host=10.1.1.70 name: mwj age: 24 # yaml风格 all: children: server: hosts: dbserver: ansible_host: 10.10.110.122 ansible_port: 22 name: mwj age: 24 webserver: ansible_host: 10.10.110.123 ansible_port: 22 可以在命令行引用主机变量，也可以在 Playbook 中引用主机变量 ansible dbserver -m shell -a 'echo {{name}}' 使用层级关系定义更复杂的主机变量 all: children: server: hosts: dbserver: ansible_host: 10.10.110.122 ansible_port: 22 name: n1: mike n2: masha n3: laki # 引用时使用{{ name.n1 }}或{{ name['n1'] }} 主机组变量：在主机清单中，我们可以将多个主机分为一组，这样方便我们同时去操作同一组的管理节点，我们可以为这个主机组定义变量，组内的所有主机都可以使用 # ini风格 [webserver] web01 ansible_host: 10.10.110.121 web02 ansible_host: 10.10.110.122 web03 ansible_host: 10.10.110.123 [webserver:vars] path=\"/usr/local/nginx/html/\" user=\"root\" # yaml格式 all: children: server: hosts: dbserver: ansible_host: 10.10.110.122 ansible_port: 22 webserver: ansible_host: 10.10.110.123 ansible_port: 22 vars: user: \"root\" path: \"/usr/local/nginx/html/\" set_fact 定义变量set_fact 是一个模块，我们可以通过 set_fact 模块在 tasks 中定义变量 testvar1，然后打印这个变量 --- - hosts: dbserver remote_user: root tasks: - set_fact: testvar1: mid - debug: msg: \"{{ testvar1 }}\" set_fact 定义变量的特殊性 通过 set_fact 模块创建的变量还有一个特殊性，通过 set_fact 创建的变量就像主机上的 facts 信息一样，可以在之后的 play 中被引用。而我们使用 vars 关键字创建的变量则不能被其他 Playbook 所引用到 下面这个 Playbook 有两个 play，第一个 play 中有两个变量分别是 ts1 和 ts2，它们分别用 vars 和 set_fact 定义，只有使用 set_fact 定义的 ts2 变量，才能被下面这个 play 所引用，而使用 vars 定义的 ts1 变量则不能被下面的 play 所引用 --- - hosts: dbserver remote_user: root vars: ts1: team1 tasks: - set_fact: ts2: team2 - debug: msg: \"{{ ts1 }}---{{ ts2 }}\" - hosts: webserver remote_user: root tasks: - name: get ts1 # 这里引用会报错 debug: msg: \"{{ ts1 }}\" - name: get ts2 debug: msg: \"{{ ts2 }}\" 注意：set_fact 变量类似于管理节点的全局变量，可以跨 play 获取变量，注册变量也能被之后的 play 所引用 内置变量除了我们各种各样的定义变量之外，Ansible 还有一些内置的变量供我们使用，这些内置变量的变量名是被 Ansible 所保留的，我们定义变量时不能使用这些变量名 内置变量 ansible_version，查看 Ansible 的版本 ansible all -m debug -a 'msg={{ansible_version}}' 内置变量 hostvars，hostvars 可以帮助我们在操作当前管理节点时获取到其他管理节点中的信息。下面 Playbook 有两个 play，第一个没有任何 task，只是将 webserver 主机的信息收集起来，供后面的 play 调用。第二个 play 则是使用了 debug 模块打印了 webserver 的内置变量 hostvars，输出了 webserver 的 IP 地址，这就是在操作 dbserver 管理节点时获取了 webserver 管理节点的信息 --- - name: \"gather facts of webserver\" hosts: webserver remote_user: root - name: \"get facts webserver\" hosts: dbserver remote_user: root tasks: - debug: msg: \"{{ hostvars['webserver'].ansible_ens32.ipv4 }}\" # 如果没有第一个play，在执行时调用[Gathering Facts]任务，将webserver的信息收集起来，后面dbserver调用这个变量就会报错 内置变量 inventory_hostname，通过 inventory_hostname 变量可以获取到管理节点的当前主机名称，注意这个不是指 Linux 系统的主机名，而是对应管理节点在控制节点的主机清单中的配置名称 # 主机清单 [abc] 10.10.110.122 dbserver ansible_host: 10.10.110.123 使用内置变量 inventory_hostname 获取各个主机的对应的主机名 ansible abc -m debug -a 'msg={{inventory_hostname}}' 10.10.110.122 | SUCCESS => { \"msg\": \"10.10.110.122\" } dbserver | SUCCESS => { \"msg\": \"dbserver\" } # 定义是IP则返回IP，定义是别名则返回别名 内置变量 inventory_hostname_short，与内置变量 inventory_hostname 类似，通过 inventory_hostname_short 也可以获取当前 play 操作的管理节点在清单中对应的名称，但是这个名称更加简短 [abc] 10.10.110.122 dbserver.com ansible_host=10.10.110.123 按上面主机清单的配置，我们可以使用 inventory_hostname_short 获取到管理节点的简短名称 ansible all -m debug -a 'msg={{inventory_hostname_short}}' 10.10.110.122 | SUCCESS => { \"msg\": \"10\" } dbserver.com | SUCCESS => { \"msg\": \"dbserver\" } # 可以看到无论是IP还是主机名，inventory_hostname_short都会取得主机名中第一个\".\"之前的字符作为主机的简短名称 内置变量 play_hosts，通过内置变量 play_hosts 可以获取到当前 play 所操作的所有管理节点的主机名列表 --- - hosts: 10.10.110.122,dbserver.com remote_user: root tasks: - name: debug debug: msg: \"{{ play_hosts }}\" # 返回的是所操作的所有管理节点的主机名列表 ok: [10.10.110.122] => { \"msg\": [ \"10.10.110.122\", \"dbserver.com\" ] } ok: [dbserver.com] => { \"msg\": [ \"10.10.110.122\", \"dbserver.com\" ] } 内置变量 inventory_dir，我们可以通过 inventory_dir 变量获取到 ansible 主机中清单文件的存放路径 ansible all -m debug -a 'msg={{inventory_dir}}' 10.10.110.122 | SUCCESS => { \"msg\": \"/etc/ansible\" } dbserver.com | SUCCESS => { \"msg\": \"/etc/ansible\" } 重新加载变量文件先来看一个小示例，假如 Playbook 中有三个任务，第一个任务调用了控制节点的一个变量文件，第二个任务在变量文件中新增了一个变量，第三个任务在变量文件中引用新增的那个变量，看看结果会如何 cat /root/playbook/var_file.yaml # 变量文件已有v1变量 v1: 111 --- - hosts: master remote_user: root tasks: vars_files: - /root/playbook/var_file.yaml tasks: - debug: msg: \"{{ v1 }}\" - lineinfile: path: \"/root/playbook/var_file.yaml\" line: \"v2: 222\" # 往变量文件新增v2变量 - debug: msg: \"{{ v1 }},{{ v2 }}\" # 输出v1和v2变量,这里输出v2变量会出错 fatal: [master]: FAILED! => {\"msg\": \"The task includes an option with an undefined variable. The error was: 'v2' is undefined\\n\\nThe error appears to be in '/root/playbook/include.yaml': line 13, column 5, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n line: \\\"v2: 222\\\"\\n - debug:\\n ^ here\\n\"} 上面的示例中，其实 v2 变量已经成功添加到变量文件中了，但是由于我们是先读取了变量文件，再写入 v2 变量到文件，这时候我们没有重新读取变量文件，那么就会报错 v2 变量未定义了，我们可以使用 include_vars 关键字从新加载变量文件 --- - hosts: master remote_user: root tasks: vars_files: - /root/playbook/var_file.yaml tasks: - debug: msg: \"{{ v1 }}\" - lineinfile: path: \"/root/playbook/var_file.yaml\" line: \"v2: 222\" - include_vars: \"/root/playbook/var_file.yaml\" # 重新加载变量文件 - debug: msg: \"{{ v1 }},{{ v2 }}\" # 这时候输出v2变量就不会出错","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible tags","slug":"Ansible tags","date":"2020-05-27T02:38:40.000Z","updated":"2020-09-26T06:31:20.000Z","comments":true,"path":"post/8ecedcd4.html","link":"","permalink":"https://www.missf.top/post/8ecedcd4.html","excerpt":"","text":"Ansible 的 tags 用法我们学习 Ansible，以后都是要编写各种各样的 Playbook 的。假如我们有一天，写了一个很长很长的 Playbook，其中包含了非常多的任务，这其实没有什么问题，但是我有时候可能只是需要执行这个 Playbook 的一部分任务而已，而非每一次都执行 Playbook 的全部任务，这个时候我们可以借助 tags 实现这个需求 --- - hosts: all remote_user: root tasks: - name: task1 file: path: /tmp/task1 state: touch tags: t1 - name: task2 file: path: /tmp/task2 state: touch tags: t2 上面定义了两个 task 任务，每个任务有自己的 tags，我们可以在执行 Playbook 时借助标签指定只执行那些任务，而忽略其他任务 ansible-playbook --tags=t2 testtags.yaml # 只执行t2标签的task任务 ansible-playbook --skip-tags=t1 testtags.yaml # 跳过t1标签任务，其他的任务都会执行 tags 的三种语法语法一: tags: - t1 - t2 语法二: tags: t1,t2 语法三: tags: ['t1','t2'] 我们可以为一个任务添加多个标签,下面两个 task 任务都有一个共同的 tag1 标签，当执行时指定 tag1 标签，下面两个任务都会执行 --- - hosts: all remote_user: root tasks: - name: task1 file: path: /tmp/task1 state: touch tags: t1,tag1 - name: task2 file: path: /tmp/task2 state: touch tags: ['t2','tag1'] 具有共同标签的任务，可以将共同标签从 task 中提取出来写在 play 中，下面的两个 task 任务分别有自己的 t1 和 t2 标签，同时又具有共同的 t3 标签，tags 写在 tasks 上面时，tasks 会继承当前 play 中的 tags --- - hosts: all remote_user: root tags: t3 tasks: - name: task1 file: path: /tmp/task1 state: touch tags: t1 - name: task2 file: path: /tmp/task2 state: touch tags: t2 调用标签时，可以一次性指定多个标签，调用多个标签需要用逗号隔开 ansible-playbook --tags=t1,t2 testtags.yaml 我们还可以在调用标签时先概览一下 Playbook 中的标签 ansible-playbook --list-tags testtags.yaml tags 的五个内置标签 always：当把任务的 tags 的值指定为 always 时，那么这个任务就总是会被执行，除非你使用 “–skip-tags” 选项明确指定跳过这个任务 never：当把任务的 tags 的值指定为 never 时，那么这个任务就总是不会被执行，2.5版本中新加入的特殊 tag tagged：调用标签时使用的，只执行有标签的任务，没有任何标签的任务不会被执行 untagged：只执行没有标签的任务，但是如果某些任务包含always标签，那么这些任务也会被执行 all：执行所有标签 只执行有标签的任务，没有任何标签的任务不会被执行 ansible-playbook --tags tagged testtag.yml 跳过包含标签的任务，即使对应的任务包含 always 标签，也会被跳过 ansible-playbook --skip-tags tagged testtag.yml 只执行没有标签的任务，但是如果某些任务包含 always 标签，那么这些任务也会被执行 ansible-playbook --tags untagged testtag.yml","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible Handlers","slug":"Ansible Handlers","date":"2020-05-26T07:13:48.000Z","updated":"2020-09-26T06:29:10.000Z","comments":true,"path":"post/b39bea85.html","link":"","permalink":"https://www.missf.top/post/b39bea85.html","excerpt":"","text":"Ansible 的 Handlers 用法许多的 Linux 服务在修改配置文件后都是需要重启服务的，以便能够重新读取配置文件，使新的配置能够生效。那怎么用 Playbook 实现这个简单的功能呢？下面我们来编写一个修改 Nginx 端口的 Playbook，并且在修改完之后重启 Nginx --- - hosts: all remote_user: root tasks: - name: modify config replace: path: /etc/nginx/nginx.conf regexp: \"listen(.*)80;\" # 替换nginx端口为8080 replace: \"listen 8080;\" backup: yes - name: restart nginx # 重启nginx服务 service: name: nginx state: restarted 注意思考：这个 Playbook 虽然可以帮助我们成功修改 Nginx 端口并重启 Nginx 服务，但是大家请注意如果我再次执行这个 Playbook 的话，Nginx 端口已经是8080了，由于 Ansible 幂等性的缘故，所以 modify config 这个 task 没有发生状态的改变，所以这一步返回了绿色的信息，但是 Nginx 的服务还是被重启了，其实我们并没有真正去改变 Nginx 的配置文件，但是却还是重启了 Nginx 服务，这是因为重启服务这个任务是写死了的。这种多余的重启是不需要的。那么在 Playbook 中就是使用 Handlers 来解决这种问题的，下面我们就继续以 Nginx 服务这个小例子来学习 Playbook 的 Handlers 用法 --- - hosts: all remote_user: root tasks: - name: modify config replace: path: /etc/nginx/nginx.conf regexp: \"listen(.*)80;\" replace: \"listen 8080;\" backup: yes notify: # 在modify config这个任务调用handlers任务列表的restart nginx任务(认真理解这句话) restart nginx handlers: # 定义一个handlers任务列表 - name: restart nginx service: name: nginx state: restarted 上面示例我们使用 Handlers 用法，如果 modify config 这个 task 的状态被真正修改过了，notify 就会调用 Handlers 任务列表的 restart Nginx 任务，就会执行重启 Nginx 服务，这样就能达到只有 Nginx 配置文件被真正修改了，才会去重启 Nginx 服务 Handlers 是一种任务列表在 Playbook 中 Handlers 和 tasks 是同级别的，这是因为 Handlers 也是任务列表的一种。只不过Handlers 中的任务是被用于 tasks 任务列表的 notify 调用而已 --- - hosts: all remote_user: root tasks: - name: make testfile1 file: path: /testdir/testfile1 state: directory notify: ht2 - name: make testfile2 file: path: /testdir/testfile2 state: directory notify: ht1 handlers: - name: ht1 file: path: /testdir/ht1 state: touch - name: ht2 file: path: /testdir/testfile2 state: touch 上面 Playbook 的执行过程如下： PLAY [all] *********************************************************************************************************************** TASK [Gathering Facts] *********************************************************************************************************** ok: [webserver] ok: [dbserver] TASK [make testfile1] ************************************************************************************************************ changed: [webserver] changed: [dbserver] TASK [make testfile2] ************************************************************************************************************ changed: [webserver] changed: [dbserver] RUNNING HANDLER [ht1] ************************************************************************************************************ changed: [webserver] changed: [dbserver] RUNNING HANDLER [ht2] ************************************************************************************************************ changed: [dbserver] changed: [webserver] PLAY RECAP *********************************************************************************************************************** dbserver : ok=5 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 webserver : ok=5 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 默认情况下，所有 task 执行完毕后，才会执行各个 Handler，而且 Handler 的执行顺序与 Handler 在 Playbook 中的定义顺序是相同的，与 Handler 被 notify 调用的顺序无关，这一点大家要注意。如果你想要在执行完某些 task 以后立即执行对应的 Handler，则需要使用 meta 模块 --- - hosts: all remote_user: root tasks: - name: make testfile1 file: path: /testdir/testfile1 state: directory notify: ht2 - meta: flush_handlers # 定义一个meta任务，表示立即执行之前task任务对应的handlers - name: make testfile3 file: path: /testdir/testfile3 state: directory notify: ht1 handlers: - name: ht1 file: path: /testdir/testfile4 state: touch - name: ht2 file: path: /testdir/testfile2 state: touch Handlers 分组我们可以将 Handlers 任务列表分组，将多个 Handlers 任务组成一个组，然后在 task 任务列表 notify 一个 Handlers 组，这时候 task 任务执行完之后就会一次性执行多个 Handlers 任务 --- - hosts: all remote_user: root tasks: - name: make testfile1 file: path: /testdir/testfile1 state: directory notify: handlers group1 - meta: flush_handlers handlers: - name: ht1 listen: handlers group1 file: path: /testdir/testfile4 state: touch - name: ht2 listen: handlers group1 file: path: /testdir/testfile2 state: touch 将 ht1 和 ht2 这两个 Handlers 任务都监听 Handlers group1 这一个组，这时候在 task 任务列表 notify “handlers group1” 这个组名时，就执行这个组的所有 Handlers 任务","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible Playbook","slug":"Ansible Playbook","date":"2020-05-24T06:24:36.000Z","updated":"2020-09-26T06:26:20.000Z","comments":true,"path":"post/11b6c727.html","link":"","permalink":"https://www.missf.top/post/11b6c727.html","excerpt":"","text":"Ansible Playbook 初识前一章我们学习了 Ansible 的模块，在控制节点上使用了很多 Ansible 的命令对管理节点进行配置和管理，但是在我们真正的工作场景中，如果需要配置一个 Nginx 服务，其实并不是在控制节点执行 Ansible 命令去实现的，你可以想象一下，如果我们需要对管理节点做大量的操作，是不是就是要在控制节点执行非常多的命令呢，而且直接执行命令的方式对管理不同的管理节点时，命令又是需要修改的，这并不是我们想要的。其实 Ansible 是可以写成”脚本”的，注意这里所说的脚本，并不是说将大量的 Ansible 命令放到 shell 脚本里面去，Ansible 在部署较为复杂的任务时，有自己的一套执行流程，称为 “剧本”，剧本翻译过来就是我们所说的 Playbook。编写 Playbook 需要遵循 YAML 语法，那什么又是 YAML 语法呢，它是为了方便人类读写而设计出来的一种通用的数据串行化格式 编写第一个 PlaybookPlaybook 文件都以 “yaml” 或 “yml” 作为文件后缀，这里我们创建一个名为 first.yaml 的 Playbook 文件 # 将下面的ansible命令转化为playbook ansible all -m ping ansible all -m file -a 'path=/etc/nodes state=directory' # playbook的写法: --- - hosts: all remote_user: root tasks: - name: ping nodes ping: - name: mkdir directory file: path: /etc/nodes state: directory 第一行：使用三个横杠作为开始，在 YAML 语法中，”-“ 表示文档开始 第二行：使用 “-“ 作为开头表示一个块序列的节点，后面使用 hosts 关键字指定了要操作的主机 第三行：使用 remote_user 关键字可以指定在管理节点进行操作时使用哪个用户进行操作 第四行：使用 tasks 关键字指明要进行操作的任务列表，之后的行都属于 tasks 键值对中的值 tasks 之后的行都属于任务列表的任务，可以看出任务列表一共有两个任务，每个任务以 “-“ 开头，每个任务都有自己的名字，任务名字使用 name 关键字进行指定，第一个任务使用 ping 模块，ping 模块在使用时不需要指定任何参数。第二个任务使用 file 模块，使用 file 模块时，指定了 path 参数和 state 参数的值 执行 Playbook[root@localhost ~/playbook]# ansible-playbook first.yaml PLAY [all] ********************************************************************************************************************************************************* TASK [Gathering Facts] ********************************************************************************************************************************************* ok: [10.10.110.123] ok: [10.10.110.122] TASK [ping nodes] ************************************************************************************************************************************************** ok: [10.10.110.123] ok: [10.10.110.122] TASK [mkdir directory] ********************************************************************************************************************************************* ok: [10.10.110.122] ok: [10.10.110.123] PLAY RECAP ********************************************************************************************************************************************************* 10.10.110.122 : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 10.10.110.123 : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 如上所示，Playbook 执行后返回了一些信息，这些信息是这次剧本运行的概况。PLAY [all] 表示这次运行的 Playbook 有一个 “play” 是针对所有主机运行的，一个 Playbook 可以是由一个或者多个 play 组成的。这个 play 包含了三个任务，这三个任务分别是 TASK [Gathering Facts]，TASK [ping nodes]，TASK [mkdir directory]。我们只创建了两个任务，为什么却执行了三个任务呢？其实每个 paly 在执行前都会执行一个默认任务，这个默认任务就是 TASK [Gathering Facts]，它会收集当前 play 对应的目标主机的相关信息，收集完这些基础信息后，才会执行我们指定的任务，这里它是收集我们这个 play 的所有主机的信息，然后返回主机的 IP 地址。第二个任务是用 ping 模块去测试管理节点的状态，给我们返回的是绿色的信息，表示管理节点的状态没有发生改变。第三个任务是创建目录，这里如果管理节点没有 /etc/nodes 目录，则会返回黄色的信息，表示在管理节点上创建了目录，管理节点的状态发生了改变。这是再次执行 Playbook，发现创建目录任务的返回信息变成了绿色的，是因为已经创建过目录了，由于幂等性的原因，管理节点的状态没有发生改变。返回信息的最后一个PLAY RECAP 中可以对所有主机的执行情况进行回顾 检查 Playbook 语法ansible-playbook --syntax-check first.yaml 如果执行语法检查命令之后，只返回了 Playbook 的名称，就表示没有语法错误 模拟执行 Playbookansible-playbook --check first.yaml 除了对 Playbook 进行语法测试，我们还能够模拟执行 Playbook，模拟执行并不是真正的执行，只是假装执行一下， Playbook 中的任务并不会真正在目标主机中运行，所以你可以放心大胆的进行模拟，模拟运行功能可以帮助我们 预估 Playbook 是否能够正常执行 注意：使用上述命令进行模拟时，一些任务可能会报错，这可能是因为报错的任务在执行时需要依赖之前的其他任务的完成结果，但是因为是模拟执行，所以之前的任务并不会真正的执行，既然之前的任务没有真正的执行，自然不会产生对应的结果，所以后面的任务就报错了。也就是说，我们并不能完全以模拟的反馈结果作为 Playbook 是否能够正常运行的判断依据，只能通过模拟大概的预估一下而已 使用 Playbook 安装 Nginx目录文件规划 tree /root/playbook/ /root/playbook/ ├── index.html.j2 ├── nginx.conf └── nginx.yaml 编写 Playbook --- - hosts: all remote_user: root vars: # 定义变量，可以在nginx.conf文件中调用 http_port: 80 max_clients: 65535 tasks: - name: ensure nginx is at the latest version yum: name: nginx state: installed - name: write the nginx config file template: # 模板模块，将当前目录下的nginx.conf文件(文件里面定义的变量会自动赋值再拷贝)拷贝到管理节点 src: nginx.conf dest: /etc/nginx/nginx.conf - name: write the site file template: src: index.html.j2 dest: /usr/share/nginx/html/index.html notify: - restart nginx - name: ensure nginx is running service: name: nginx state: started handlers: - name: restart nginx service: name=nginx state=restarted 编写 Nginx 配置文件 #user nobody; worker_processes auto; #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info; #pid logs/nginx.pid; events { worker_connections {{ max_clients }}; # 调用nginx.yaml中定义的变量 } http { include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server { listen {{ http_port }}; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root /usr/share/nginx/html/; index index.html index.htm; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } } 编写 index.html.j2 文件 Hello Ansible! This is {{ansible_all_ipv4_addresses}} 在 Ansible 控制节点上查看 curl 10.10.110.122 Hello Ansible! This is [u'10.10.110.122'] # 这个是可变变量 curl 10.10.110.123 Hello Ansible! This is [u'10.10.110.123']","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible YAML 基本语法","slug":"Ansible YAML 基本语法","date":"2020-05-23T03:54:36.000Z","updated":"2020-09-26T06:21:00.000Z","comments":true,"path":"post/cbb0a0c0.html","link":"","permalink":"https://www.missf.top/post/cbb0a0c0.html","excerpt":"","text":"Ansible 的 YAML 基本语法大小写敏感 使用缩进表示层级关系 缩进时不允许使用 Tab 键，只允许使用空格 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 YAML 文件以 “-“ 作为文档的开始，”…” 作为文档的结束 # 表示注释，从这个字符一直到行尾，都会被解析器忽略 相同缩进级别的行以 “-“ (破折号和空格)开头的组成一个列表 YAML 支持的三种数据结构 数组：一组按次序排列的值，又称为序列 (sequence) / 列表 (list) 对象：键值对的集合，又称为映射 (mapping)/ 哈希 (hashes) / 字典 (dictionary) 纯量：单个的、不可再分的值 数组相同缩进级别的行以 “-“ (破折号和空格) 开头组成一个列表就是数组 --- fruits: - Apple - Banana - orange - melon # 行内表示法 fruits: ['Apple', 'Banana', 'orange', 'melon'] 对象对象的一组键值对，使用冒号结构表示(冒号后面要有个空格) sb: name: Alex job: python skill: brag # 行内表示法 sb: {name: Alex, job: python, skill: brag} 纯量数值 number: 12 float: 15.20 布尔值 表示true的值 true, True, TRUE, yes, Yes, YES, on, On, ON, y, Y 表示false的值 false, False, FALSE, no, No, NO, off, Off, OFF, n, N 强制类型转换 a: !!str 123 d: !!str true # 这个true的数据类型不再是布尔值，而是str类型 字符串 str: 这是字符串 s1: '内容\\n字符串' # 如果字符之中包含空格和特殊字符，需要放在引号之中，单引号和双引号都可以使用，双引号不会对特殊字符转义 空值 # null: 用~表示 parent: ~ 引用&amp; 用来建立锚点(defaults)，&lt;&lt; 表示合并到当前数据，* 用来引用锚点 defaults: &amp;defaults adapter: postgres host: localhost development: database: myapp_development &lt;&lt;: *defaults test: database: myapp_test &lt;&lt;: *defaults # 上面的写法等同于下面的代码: defaults: adapter: postgres host: localhost development: database: myapp_development adapter: postgres host: localhost test: database: myapp_test adapter: postgres host: localhost 参考Palybooks 更多的 YAML 语法请参考：http://docs.ansible.com/YAMLSyntax.html","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible 模块学习","slug":"Ansible 模块学习","date":"2020-05-12T03:12:59.000Z","updated":"2020-09-26T06:12:32.000Z","comments":true,"path":"post/8704a1a2.html","link":"","permalink":"https://www.missf.top/post/8704a1a2.html","excerpt":"","text":"setup模块setup 模块主要用于获取主机信息，每个管理节点在接收控制节点命令之前，会将主机的信息告知控制节点 filter：用于进行条件过滤，如果设置，仅返回匹配过滤条件的信息 关键字 说明 返回值例子 ansible_nodename 节点名 “6-dns-1.hunk.tech” ansible_fqdn FQDN名 “6-dns-1.hunk.tech” ansible_hostname 主机短名称 “6-dns-1” ansible_domain 主机域名后缀 “hunk.teh” ansible_memtotal_mb 总物理内存 “ansible_memtotal_mb”: 222 ansible_swaptotal_mb SWAP总大小 “1023” ansible_processor CPU信息 Intel(R) Core(TM) i5-5200U CPU @ 2.20GHz ansible_processor_cores CPU核心数量 4 ansible_processor_vcpus CPU逻辑核心数量 2 ansible_all_ipv4_addresses 有所IPV4地址 192.168.0.200 ansible_all_ipv6_addresses 所有IPV6地址 ansible_default_ipv4 默认网关的网卡配置信息 ansible_eth2 具体某张网卡信息 不同系统名称需要变化 ansible_dns DNS设置信 ansible_architecture 系统架构 x86_64 ansible_machine 主机类型 x86_64 ansible_kernel 内核版本 “2.6.32-696.el6.x86_64” ansible_distribution 发行版本 “CentOS” ansible_distribution_major_version 操作系统主版本号 “6” ansible_distribution_release 发行版名称 “Final” ansible_distribution_version 完整版本号 “7.4.1708” ansible_pkg_mgr 软件包管理方式 “yum” ansible_service_mgr 进行服务方式 “systemd” ansible_os_family 家族系列 “RedHat” ansible_cmdline 内核启动参数 ansible_selinux SElinux状态 “disabled” ansible_env 当前环境变量参数 ansible_date_time 时间相关 ansible_python_version python版本 “2.6.6” ansible_lvm LVM卷相关信息 ansible_mounts 所有挂载点 ansible_device_links 所有挂载的设备的UUID和卷标名 ansible_devices 所有/dev/下的正在使用的设备的信息 ansible_user_dir 执行用户的家目录 “/root” ansible_user_gecos 执行用户的描述信息 “The root “ ansible_user_gid 执行用户的的GID 0 ansible_user_id 执行用户的的用户名 “root” ansible_user_shell 执行用户的shell类型 “/bin/bash” ansible_user_uid 执行用户的UID 0 查看管理节点的 Python 版本信息 ansible all -m setup -a 'filter=ansible_python_version' 查看管理节点的发行版本 ansible all -m setup -a 'filter=ansible_distribution' command 模块Ansible 的默认模块，可以不用 “-m” 指定，”-a” 是 command 的参数 free_form：其实没有名为 “free form” 的实际参数，command 模块接受自由格式的命令运行 chdir：在执行对应的命令之前，会先进入到 chdir 参数指定的目录中 creates：如果指定的文件存在时，就不执行对应命令 removes：当指定的文件不存在时，就不执行对应命令 查看管理节点 /etc/ 目录下的 hosts 文件内容 ansible all -a \"chdir=/etc cat hosts\" 查看管理节点 /etc/ 目录下的 hosts 文件内容，如果存在 /etc/passwd 文件则不执行 ansible all -a \"chdir=/etc creates=/etc/passwd cat hosts\" command 模块不支持调用 “$HOME” 这样的变量，还有像( &lt; &gt; | ; &amp; )这些正则和通配符都将不可用，但是 command 模块更安全，因为他不受用户环境的影响。 也很大的避免了潜在的 shell 注入风险 shell 模块shell 模块可以帮助我们在远程主机上执行命令。与 command 模块不同的是，shell 模块在远程主机中执行命令时，会经过远程主机上的 /bin/sh 程序处理，能够使用( &lt; &gt; | ; &amp; )这些符号和环境变量 free_form：其实没有名为 “free form” 的实际参数，command 模块接受自由格式的命令运行 chdir：在执行对应的命令之前，会先进入到 chdir 参数指定的目录中 creates：如果指定的文件存在时，就不执行对应命令 removes：当指定的文件不存在时，就不执行对应命令 executable：默认 shell 模块会调用远程主机中的 /bin/sh 去执行对应的命令，也可以指定 shell，需要使用绝对路径 shell 模块在管理节点上执行命令时，支持管道和重定向等符号 ansible all -m shell -a 'chdir=/etc executable=/bin/bash cat hosts >/tmp/hosts.bak' script 模块script 模块可以帮助我们在管理节点上执行控制节点上的脚本，也就是说在管理节点上执行脚本不需要把脚本拷贝过去 free_form：指定需要执行的脚本，其实没有名为 “free form” 的实际参数 chdir：在执行对应的脚本之前，会先进入到 chdir 参数指定的目录中 creates：如果指定的文件存在时，就不执行脚本 removes：当指定的文件不存在时，就不执行脚本 在管理节点上执行控制节点的 /root/test.sh 脚本，执行之前切换到 /opt 目录 ansible all -m script -a 'chdir=/opt /root/test.sh' copy 模块copy 模块的作用就是将 Control node 的文件拷贝到 Managed nodes scr：用于指定控制节点上被 copy 的文件或目录 dest：用于指定文件将被拷贝到管理节点的路径，dest 为必须参数 content：当不使用 src 指定拷贝的文件时，可以使用 content 直接指定文件内容，src 与 content 两个参数必有其一 force：当管理节点的目标路径存在同名文件，并且两个文件内容不同，是否强制覆盖，可选值有 yes 和 no，默认值为 yes backup：当管理节点的目标路径存在同名文件，并且两个文件内容不同，是否对管理节点的文件进行备份，可选值有 yes 和 no owner：指定文件拷贝到管理节点后的属主，但是管理节点上必须有对应的用户 group：指定文件拷贝到管理节点后的属组，但是管理节点上必须有对应的组 mode：指定文件拷贝到管理节点后的权限，可以使用 mode=0644 表示，也使用 mode=u+x 表示 将控制节点的 /etc/hosts 文件复制到管理节点的 /root 目录下，如果管理节点的 /root 目录已经存在文件，则会默认覆盖 ansible all -m copy -a \"src=/etc/hosts dest=/root/\" # 如无意外这里你看到的字体颜色是黄色的，这是成功执行并且状态发生了改变的 复制文件，指定文件的属主和属组，需要注意的是管理节点必须存在对应的用户和组 ansible all -m copy -a \"src=/etc/hosts dest=/tmp/ owner=mwj group=mwj\" 复制文件，如果管理节点的目标路径已存在同名文件且内容不相同，则对管理节点的文件先进行备份，再把控制节点的文件复制到管理节点 ansible all -m copy -a \"src=/etc/hosts dest=/tmp/ backup=yes\" # 在返回的结果列表能看到: \"backup_file\": \"/tmp/hosts.15575.2020-05-12@22:28:50~\" # ansibel是用哈希值去校验两个文件的内容是否一致的 file 模块file 模块可以完成对文件增删查改的基本操作 path：用于指定要操作的文件或目录，必须参数 state：ansible无法从 path=/test/a/b 得知我们想要创建目录还是文件，所以需要使用 state 参数配和 path 来声明操作的类型 state=directory 创建的是目录 state=touch 创建的是文件 state=link 创建的是软连接文件 state=hard 创建的是硬链接文件 state=absent 删除文件或者目录，absent 意为 “缺席” src：当 state 设置为 link 或者 hard 时，我们必须指明软硬链链接到哪个文件，通过 src 参数即可指定链接源 force：当 state=link 的时候，可配合 force=yes 参数强制创建链接文件，但是强制创建会有两种情况 情况一：当要创建的链接文件所指向的源文件并不存在时，使用此参数可以先强制创建出链接文件 情况二：当要创建链接文件的路径中已经存在与链接文件同名的文件时，将 force 设置为 yes，会将同名文件覆盖为链接文件 owner：用于指定被操作文件或目录的属主 group：用于指定被操作文件或目录的属组 mdoe：用于指定被操作文件或目录的权限，使用 mode=755，设置特殊权限则可以使用 mode=4700 recurse：当要操作的对象为目录，将 recurse 设置为 yes，可以递归的修改目录中文件的属性 在管理节点上创建一个名为 testdir 的目录，如果目录已存在则不进行任何操作 ansible all -m file -a \"path=/testdir/ state=directory\" 在管理节点上创建一个名为 testfile 的文件，如果文件已存在则会更新文件的时间戳 ansible all -m file -a \"path=/testdir/testfile state=touch\" 在管理节点创建一个名为 /testdir/linkfile 的链接文件，链接的源文件 /testdir/testfile 已存在 ansible all -m file -a \"path=/testdir/linkfile state=link src=/testdir/testfile\" 在管理节点上删除指定的文件或目录 ansible all -m file -a \"path=/testdir/testfile state=absent\" fetch 模块从管理节点拉取文件到控制节点 dest：用来存放从管理节点拉取到的文件 src：管理节点被拉取的文件，必须是文件不能是目录 flat：默认为 no，会将拉取到控制节点的文件以 hostname/file 的命名存放在 dest 目录，如果为 yes，则直接按文件名存放 Validate_checksum：拉取文件之后进行 MD5 检查 拉取管理节点的 /etc/hosts 文件到控制节点的 /data/ 目录 ansible all -m fetch -a \"src=/etc/hosts dest=/data/\" # 这里flat默认为no，所以拉取之后存放的方式是这样的 tree /data/ /data/ ├── 10.10.110.122 │ └── etc │ └── hosts └── 10.10.110.123 └── etc └── hosts ansible all -m fetch -a \"src=/etc/hosts dest=/data/ flat=yes\" # flat=yes是直接按文件名存放 tree /data/ /data/ └── hosts # 只有一个hosts文件是因为第一个hosts被覆盖掉了 blockinfile 模块blockinfile 模块可以帮助我们在指定的文件中插入 “一段文本”，这段文本是被标记过的，我们在这段文本上做了记号，以便在以后的操作中可以通过 “标记” 找到这段文本，然后修改或者删除它 path：指定要操作的文件 block：此参数用于指定我们想要插入的那 “一段文本”，此参数有一个别名叫 “content”，使用 content 或 block 的作用是相同的 marker：自定义开始和结束的标记，marker=#{mark}test:开始为# BEGIN test，结束为# END test insertafter：在插入一段文本时，默认会在文件的末尾插入文本，如果你想要将文本插入在某一行的后面，可以使用此参数指定对应的行 insertbefore：在插入一段文本时，默认会在文件的末尾插入文本，如果你想要将文本插入在某一行的前面，可以使用此参数指定对应的行 backup：是否在修改文件之前对文件进行备份 create：当要操作的文件并不存在时，是否创建对应的文件 在管理节点的 /testdir/rc.local 文件末尾插入一行 systemctl start mariadb ansible all -m blockinfile -a 'path=/testdir/rc.local block=\"systemctl start mariadb\"' 自定义插入的开始和结束的标记 ansible all -m blockinfile -a 'path=/testdir/rc.local block=\"systemctl restart mysqld\\nnginx -s reload\" marker=\"#{mark} serivce to start\"' # 查看被插入的文本 #BEGIN serivce to start systemctl restart mysqld nginx -s reload #END serivce to start 使用 create 参数，如果指定的文件不存在则创建它 ansible all -m blockinfile -a 'path=/testdir/date block=\"今天是5月12号\\n汶川地震12周年\" marker=\"#{mark} 日期记录\" create=yes' 使用 backup 参数，可以在操作修改文件之前对文件进行备份 ansible all -m blockinfile -a 'path=/testdir/date block=\"今天是5月12号\\n汶川地震12周年\" marker=\"#{mark} 日期记录\" backup=yes' lineinfile 模块我们可以借助 lineinfile 模块，确保 “某一行文本” 存在于指定的文件中，还可以根据正则表达式替换 “某一行文本” path：指定要操作的文件 line：使用此参数指定文本内容 regexp：使用正则表达式匹配对应的行 state：当想要删除对应的文本时，需要将 state 参数的值设置为 absent backrefs：开启后向引用，line 参数中就能对 regexp 参数中的分组进行后向引用了 insertafter：借助 insertafter 参数可以将文本插入到 “指定的行” 之后 insertbefore：借助 insertbefore 参数可以将文本插入到 “指定的行” 之前 backup：是否在修改文件之前对文件进行备份 create：当要操作的文件并不存在时，是否创建对应的文件 确保 “test lineinfile” 这行文本存在于 /testdir/date 文件中，如果存在则不做任何操作，如果不存在则在末尾插入 ansible all -m lineinfile -a 'path=/testdir/date line=\"test lineinfile\"' 根据正则表达式替换 “某一行”，如果多行能够匹配正则，只有最后匹配的行才会被替换，如果没有匹配到则会在末尾插入 line 的内容 ansible all -m lineinfile -a 'path=/testdir/date regexp=\"^test\" line=\"被替换后的内容\"' 根据正则匹配删除对应的行，如果文件多行都与正则匹配，则删除多行 ansible all -m lineinfile -a 'path=/testdir/date regexp=\"^#.*-$\" state=absent' # 删除#开头-结尾中间有任意个字符的行 在管理节点的 /testdir/date 文件的 “#Hello saltstack,Hiiii” 这一行之后插入 123 ansible all -m lineinfile -a 'path=/testdir/date line=\"123\" insertafter=\"#Hello saltstack,Hiiii\"' find 模块find 模块可以帮助我们在管理节点中查找符合条件的文件，就像 find 命令一样 paths：必须参数，指定在哪个目录中查找文件，可以指定多个路径，路径间用逗号隔开 recurse：默认只会在指定的目录中查找文件，当 recurse 参数设置为 yes 时，表示会递归的查找文件 hidden：默认不会去查找隐藏文件，只有当 hidden 参数的值设置为 yes 时才会查找隐藏文件 file_type：默认只会根据条件查找 “文件”，可以通过 file_type 指定文件类型，any | directory | file | link patterns：使用此参数指定需要查找的文件名称，支持使用 shell (比如通配符)或者正则表达式去匹配文件名称 use_regex：当 use_regex 设置为 yes 时，表示使用 python 正则解析 patterns 参数中的表达式 contains：使用此参数可以根据文章内容查找文件，此参数的值为一个正则表达式 age：用此参数可以根据时间范围查找文件，默认以文件的 mtime 为标准与指定的时间进行对比 age_stamp：文件的时间属性中有三个时间种类：atime、ctime、mtime，当我们根据时间范围查找文件时，可以指定以哪个时间种类为准 size：使用此参数可以根据文件大小查找文件 get_checksum：当有符合查找条件的文件被找到时，会同时返回对应文件的 sha1 校验码 在管理节点的 /etc 目录中查找包含 www 字符串的文件，不进行递归并忽略隐藏文件 ansible all -m find -a 'paths=/etc contains=\".*www.*\"' 在管理节点的 /etc 目录查找以 .sh 结尾的文件，包括隐藏文件并进行递归查找 ansible all -m find -a 'paths=/etc patterns=\"*.sh\" hidden=yes recurse=yes' 在管理节点的 /etc 目录查找链接文件，包括隐藏文件并进行递归查找 ansible all -m find -a 'paths=/etc file_type=link hidden=yes recurse=yes' 在管理节点的 /etc 目录查找以 .sh 结尾的文件，只不过patterns 对应的表达式为正则表达式，包括所有文件类型 ansible all -m find -a 'paths=/etc patterns=\"\\*.sh\" file_type=any use_regex=yes' 在管理节点的 /etc 目录递归查找 mtime 在 4 天以内的文件 ansible all -m find -a 'paths=/etc age=-4d recurse=yes' 在管理节点的 /etc 目录递归查找大于 2G 的文件 ansible all -m find -a 'paths=/etc size=2g recurse=yes' 在管理节点的 /etc 目录递归查找 .conf 结尾的文件，并且返回符合条件的文件的 sha1 校验码 ansible all -m find -a 'paths=/etc patterns=\"*.conf\" recurse=yes get_checksum=yes' replace 模块replace 模块可以根据我们指定的正则表达式替换文件中的字符串，文件中所有被正则匹配到的字符串都会被替换 path：必须参数，指定要操作的文件，别名：dest | destfile | name regexp：必须参数，指定一个 python 正则表达式，文件中与正则匹配的字符串将会被替换 replace：指定最终要替换成的字符串 backup：是否在修改文件之前对文件进行备份，最好设置为 yes 将管理主机的 /testdir/date 文件中所有的 ansible 替换为 saltstack，操作前进行文件备份 ansible all -m replace -a 'path=/testdir/date regexp=\"ansible\" replace=saltstack backup=yes' cron 模块cron 模块可以帮助我们配置管理节点的计划任务，功能相当于 crontab 命令 minute：用于设置分钟值，格式为 minute=5，如不指定此参数，则分钟值默认为 * hour：用于设置小时值，格式为 hour=5，如不指定此参数，则小时值默认为 * day：用于设置日值，如不指定此参数，则日值默认为 * month：用于设置月值，如不指定此参数，则月值默认为 * weekday：用于设置周值，如不指定此参数，则月值默认为 * special_time：时间设定格式为 @reboot 或者 @hourly，这种 @ 开头的时间设定格式则需要使用 special_time 参数进行设置 注意：如果以上参数都不设置，则默认使用 * * * * * ，表示每分钟都执行一次，我们应该谨慎设置时间参数 user：设置当前计划任务属于哪个用户，不指定则默认为管理员用户 job：执行计划任务中需要实际执行的命令或脚本 name：设置计划任务的名称，方便我们以后根据名称修改或者删除计划任务 state：可以根据已有名称的计划任务进行修改和删除，当删除时需要将 state 的值设置为 absent disabled：可以将已有名称的计划任务注释，但使用此参数除了指定任务名称还需要指定 job 以及时间的设定，否则注释任务时，任务的时间会被修改 backup：当此参数设置为 yes，那么修改和删除计划任务时，会在管理节点的 tmp 目录下创建备份文件 在管理节点创建名为 test cron 计划任务，每天的12点5分，任务内容为将 test 重定向到 /tmp/test ansible all -m cron -a 'name=\"test cron\" minute=5 hour=12 job=\"echo test > /tmp/test\"' # 管理节点的计划任务构建如下: #Ansible: test cron 5 12 * * * echo test > /tmp/test 在管理节点创建名为 day cron 计划任务，每三天执行一次。与执行当天的14点5分开始执行，任务内容为输出 test ansible all -m cron -a 'name=\"day cron\" minute=5 hour=14 day=*/3 job=\"echo test\"' # 管理节点的计划任务构建如下: #Ansible: day cron 5 14 */3 * * echo test 在管理节点创建名为 day cron 计划任务，任务在重启时执行，任务内容为输出 test ansible all -m cron -a 'name=\"day cron\" special_time=reboot job=\"echo test\"' # 由于已存在day cron任务，ansible就会认为我们是需要修改这个任务，计划任务被修改为: #Ansible: day cron @reboot echo test 在管理节点注释掉我们之前创建的 test cron 任务，注释时进行备份 ansible all -m cron -a 'name=\"test cron\" minute=5 hour=12 job=\"echo test > /tmp/test\" disabled=yes backup=yes' # 符合注释条件的计划任务就会被注释掉: #Ansible: test cron #5 12 * * * echo test > /tmp/test 如果你注释计划任务时，设置了错误的时间和 job，那么注释对应任务时(以name去对应)，时间和 job 的设定也会发生改变 ansible all -m cron -a 'name=\"test cron\" hour=23 job=\"echo test > /tmp/test\" disabled=yes backup=yes' #Ansible: test cron #* 23 * * * echo test > /tmp/test # 注释的同时，时间设定也会改变 service 模块service 模块可以对管理节点上的服务进行管理，例如启动或停止管理节点的 Nginx 服务。但前提是这个服务必须被 BSD init | OpenRC | SysV | Solaris SMF | systemd | upstart 中的任意一种所管理，意思就是这个服务在 CentOS6 管理节点能以 service nginx start 启动，在CentOS7 管理节点能以 systemctl start nginx 启动。如果管理节点上的服务无法通过这样的方式启动，那么 service 模块也无法对它进行管理 name：用于指定操作的服务名称，例如 name=nginx state：用户指定服务的状态，可用值有 started | stopped | restarted | reloaded enabled：用于指定是否将服务设置为开机启动项，设置为 yes 则表示开机启动，设置为 no 表示不会开机启动 在管理节点上启动 Nginx 服务 ansible all -m service -a 'name=nginx state=started' 在管理节点上启动 mysql 服务并设置为开机启动 ansible all -m service -a 'name=mysql state=started enabled=yes' user 模块user：模块可用帮助我们在管理节点上创建用户、修改用户、删除用户、为用户创建密钥对等操作 name：必须参数，用于指定要操作的用户名称 group：用于指定用户所在的基本组 shell：用于指定用户的默认 shell uid：用于指定用户的 uid 号 expires：用于指定用户的过期时间 comment：用于指定用户的注释信息 state：用于指定用户是否存在于远程主机中，默认值为 present，表示用户需要存在，当设置为 absent 时表示删除用户 remove：默认值为 no，表示删除用户时不会删除家目录，设置为 yes 则表示删除用户时删除用户家目录 password：用于指定用户的密码，但是这个密码不能是明文的密码 generate_ssh_key：默认值为 no，如果设置为 yes 则表示为用户在家目录的 .ssh 下创建密钥对，如果对应的路径已有同名密钥对则不进行任何操作 ssh_key_file：默认值为 yes，使用此参数自定义生成 ssh 私钥的路径和名称 ssh_key_passph rase：当 generate_ssh_key 参数的值为 yes 时，在创建证书时使用此参数设置私钥的密码 ssh_key_type：当 generate_ssh_key 参数的值为 yes 时，在创建证书时使用此参数设置密钥对的类型 在管理节点上创建 mis 用户，并把用户添加到 root 组，如果用户已存在则不做任何操作 ansible all -m user -a 'name=mis group=root' 在管理节点上删除 mis 用户，同时把用户家目录也删除 ansible all -m user -a 'name=mis state=absent remove=yes' 在管理节点上创建 mis 用户，指定用户的注释信息，设置用户过期时间是 2020-06-15 ansible all -m user -a 'name=mis comment=\"missf.top\" expires=1592150400' # 先使用\"date -d 2020-06-15 +%s\"命令得到Unix时间戳 在管理节点上为 mis 用户设置密码，加密字符串可以使用 python 得到 ansible all -m user -a 'name=mis password=\"$6$d62UFoKtSRA9Yaq4$qtvyr5atLdoXgvXOhktU.baVqbtlcaWc9dizmM41Bc9XOaTZW/Pqaxb8pofS5Wo4n5Nu/CEk8GEsKnC2zTfEl1\"' 可以使用 import crypt; crypt.crypt(\"123456\") 得到123456加密之后的字符串 在管理节店上为 mis 用户生成密钥对，同时指定私钥密码为 123456，密钥对的类型为 dsa，如不指定密钥对类型默认为 rsa ansible all -m user -a 'name=mis generate_ssh_key=yes ssh_key_passphrase=\"123456\" ssh_key_type=dsa' group 模块group 模块可以帮助我们在管理节点上管理用户组 name：用于指定操作的服务名称，例如 name=nginx state：用户指定服务的状态，可用值有 started | stopped | restarted | reloaded enabled：用于指定是否将服务设置为开机启动项，设置为 yes 则表示开机启动，设置为 no 表示不会开机启动 确保管理节点上存在 mkd 组，如果没有则创建，如果已存在则不做任何操作 ansible all -m group -a 'name=mkd' 在管理节点上删除 mkd 组，前提是不能有用户把被删除的组当成主组，不然不能成功删除 ansible all -m group -a 'name=mkd state=absent' yum_repository 模块yum_repository：模块可以帮助我们在管理节点上管理yum仓库 name：必须参数，指定要操作的唯一仓库 ID，repo 配置文件中括号的仓库 ID baseurl：用于设置 yum 仓库的 baseurl description：用于设置仓库的注释信息，repo 配置文件中 name 字段对应的内容 file：用户设置仓库的配置文件名称，就是 repo 配置文件的前缀，如不指定则默认以仓库 ID 命名 enabled：用于设置是否激活对应的 yum 源 gpgcheck：用于设置是否开启 rpm 包验证功能，默认值为 no 表示不开启包验证，设置为 yes 表示开启 gpgcakey：当开启包验证功能时，使用此参数指定验证包所需的公钥 state：默认值为 present，设置为 absent 表示删除对应的 yum 源 在管理节点上创建前缀为 aliepel 的 repo 文件，设置注释信息和不验证包功能 ansible all -m yum_repository -a 'name=aliepel description=\"alibaba_epel\" baseurl=https://mirrors.aliyun.com/epel/$releasever\\Server/$basearch/ gpgcheck=no' 在管理节点创建指定名称为 ali 的 repo 文件，但是不启用它 ansible all -m yum_repository -a 'name=aliepel description=\"alibaba_epel\" file=ali baseurl=https://mirrors.aliyun.com/epel/$releasever\\Server/$basearch/ gpgcheck=no enabled=no' yum 模块yum 模块可以帮助我们在管理节点上管理软件包 name：必须参数，用于指定需要管理的软件包名字 state：用户指定软件包的状态，默认是 present，表示确认已安装软件包，installed 与 present 等效，absent 和 removed 等效，表示删除对应的软件包 disable_gpg_check：用于禁用对 rpm 包的公钥 gpg 验证，默认值为no表示不禁用验证，设置为 yes 表示禁用验证，如果 yum 源没有开启验证需要将此参数设置为 yes enablerepo：用于安装软件包时临时启用 yum 源，想要从A源安装软件，但是A源没有启用时，这个参数设置为 yes 表示临时启用 disablerepo：用于安装软件包时临时禁用 yum 源，当多个源中同时存在软件包时，可以临时禁用某个源 确保管理节点上安装了 Nginx，禁用 rpm 包验证 ansible all -m yum -a 'name=nginx state=installed disable_gpg_check=yes' 确保管理节点上安装了 Telnet，并禁用 rpm 包验证和临时禁用 local 源 ansible all -m yum -a 'name=telnet disable_gpg_check=yes disablerepo=local' template 模块 src：控制节点上的模板文件 dest：管理节点上将被控制节点上的模板文件所替换的文件 owner：指定控制节点拷贝到管理节点的文件属主 group：指定控制节点拷贝到管理节点的文件属组 mode：指定控制节点拷贝到管理节点的文件权限 force：如果管理节点已存在同名文件并且内容不同时，是否强制覆盖，默认值为 yes 表示覆盖 backup：如果管理节点已存在同名文件并且内容不同时，是否对管理节点源文件进行备份 将控制节点配置好的模板文件分发到管理节点的 /etc/redis.conf，设置不强制覆盖 ansible all -m template -a 'src=/root/redis.conf dest=/etc/redis.conf force=no'","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible 主机清单","slug":"Ansible 主机清单","date":"2020-05-10T02:12:59.000Z","updated":"2020-09-25T09:30:52.000Z","comments":true,"path":"post/794bbd49.html","link":"","permalink":"https://www.missf.top/post/794bbd49.html","excerpt":"","text":"主机清单介绍Ansible 可同时操作属于一个组的多台主机， 组和主机之间的关系通过 inventory 文件配置. 默认的文件路径为 /etc/ansible/hosts，执行命令的时候使用 “-i” 参数即可指定主机清单 主机清单示例主机清单文件主要有 ini 和 yaml 格式两种语法格式 mail.example.com # 定义主机fqdn地址, 需要已经与控制节点ssh互信 localhost ansible_connection=local # ansible_connection可以定义连接类型, local是在本地执行,默认是smart host4 ansible_host=10.10.110.123 ansible_port=50312 ansible_user=root ansible_password=12345 # 指定别名，定义主机ssh连接信息 www[1:50].example.com # 定义 1-50范围内的主机 www-[a:d].example.com # 定义 a-d 范围内的主机 [dbservers] three.example.com ansible_python_interpreter=/usr/local/bin/python3 # 定义python执行ansible，这个是指定被控节点的python 192.168.77.123 ansible_ruby_interpreter=/usr/bin/ruby.1.9.3 # 定义ruby执行文件 webservers:vars] # 定义webservers组的变量 ntp_server=ntp.example.com proxy=proxy.example.com [server:children] # 定义server组的子成员，执行server组时，webservers和dbservers组内的管理节点也会执行 webservers dbservers ini 和 yaml 格式对比# 先写出ini风格 [dbserver] db1 ansible_host=10.10.110.122 ansible_port=22 ansible_user=root ansible_password=0 [webserver] web1 ansible_host=10.10.110.123 ansible_port=22 ansible_user=root ansible_password=0 [server:children] dbserver webserver # 定义子组成员时，需要children关键字 # 和上面一样的配置，这是yaml风格的写法 all: children: server: children: dbserver: hosts: 10.10.110.122 webserver: hosts: 10.10.110.123 yaml 格式配置的还是挺复杂的，可读性也差，建议使用 ini 方式来设置主机清单 默认组在主机清单中，Ansible 会自动的生成两个组 all：所有主机 ungrouped：包含没有组的主机 尽管这两个组是永远存在的，但也有可能是隐藏的，不会出现 group_names 之类的组列表中 主机变量和组变量如果你不想在主机清单中定义主机的变量或者组的变量，Ansible 还支持在特定的目录中定义变量，变量文件必须以 YAML 语法定义 默认在 /etc/ansible/host_vars/ 目录中定义主机变量，文件名称以主机名称命名，结束可以用 “.yml”、”.yaml”、”.json” 三种格式 cat /etc/ansible/host_vars/db1 ntp_server: acme.example.org database_server: storage.example.org 变量优先级问题，如果在各个环节都设置了变量，到底哪个变量生效呢？优先顺序：all 最低，host 最高： all group parent group child group host 使用多个主机清单在命令参数中，使用多个 “-i” 就可以指定多个主机清单 ansible all -i staging -i production -m ping ansible all -i /tmp/staging -i /tmp/production -m ping 使用 SSH 秘钥连接主机# 生成秘钥 ssh-keygen -t rsa # 发送公钥文件到管理节点 ssh-copy-id -i /root/.ssh/id_rsa.pub -p 22 root@10.10.110.122 # 现在主机清单里不用再填写账号密码了","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible 快速开始","slug":"Ansible 快速开始","date":"2020-05-09T01:12:59.000Z","updated":"2020-09-25T09:27:02.000Z","comments":true,"path":"post/f57ff704.html","link":"","permalink":"https://www.missf.top/post/f57ff704.html","excerpt":"","text":"Ansible 快速开始Control OS：CentOS 7.7 x64 Ansible version：2.9.7 Python version：2.7.5` 任务Control node 连接 Managed node 定义主机清单定义一个简单的通过 SSH 认证的主机清单 cat /etc/ansible/hosts 10.10.110.122 ansible_user=root ansible_pass=0 ansible_port=22 主机清单的配置含义: ansible_host 定义管理节点 ip 地址 ansible_user 连接管理节点的用户 ansible_pass 连接管理节点的用户密码 ansible_port 连接端口号默认是 22 执行 Ansible 命令测试 Control node 和 Managed nodes 的连接状态 ansible 10.10.110.122 -m ping # 命令中的含义 -192.168.77.135 用于匹配主机清单中的主机名称 -m ping 指定 ping 模块，用于测试与管理节点的连接状态 如果提示如下错误： 10.10.110.122 | FAILED! => { \"msg\": \"Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this. Please add this host's fingerprint to your known_hosts file to manage this host.\" } 这是因为 Control node 和 Managed nodes 第一次连接需要先添加指纹信息，可以先使用 SSH 连接一次，如果机器太多的话，可以在 Ansible 配置文件开启 host_key_checking = False cat /etc/ansible/ansible.cfg host_key_checking = False 再次测试连接状态 ansible 10.10.110.122 -m ping 10.10.110.122 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } # 看到\"ping\": \"pong\"表示连接成功","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible 安装","slug":"Ansible 安装","date":"2020-05-08T03:12:59.000Z","updated":"2020-10-20T02:28:58.000Z","comments":true,"path":"post/4e2cec72.html","link":"","permalink":"https://www.missf.top/post/4e2cec72.html","excerpt":"","text":"安装 Ansible对控制节点的要求： 目前，只要机器上安装了 Python 2 (2.6或更高版本) 或 Python 3 (3.5或更高版本)，都可以运行 Ansible ，windows系统不可以做控制节点，控制节点的系统可以是 Red Hat、Debian、CentOS、macOS、BSD的各种版本 对管理节点的要求： 通常我们使用 ssh 与节点通信，默认使用 sftp. 如果 sftp 不可用，可在 ansible.cfg 配置文件中配置成 scp 的方式. 在节点上也需要安装 Python 2 (2.6或更高版本) 或 Python 3 (3.5或更高版本) 控制节点上安装 Ansible# Centos/RHEL wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo yum install -y ansible # Ubuntu sudo apt update sudo apt install software-properties-common sudo apt-add-repository --yes --update ppa:ansible/ansible sudo apt install ansible Bash命令行自动补全 在Ansible 2.9之后，就支持了命令行参数补齐功能 # Centos/RHEL yum install -y epel-release yum install -y python-argcomplete 将补全加入环境变量activate-global-python-argcomplete source /etc/profile","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible 介绍","slug":"Ansible 介绍","date":"2020-05-07T08:12:59.000Z","updated":"2020-09-25T09:17:26.000Z","comments":true,"path":"post/b5ccc3ed.html","link":"","permalink":"https://www.missf.top/post/b5ccc3ed.html","excerpt":"","text":"Ansible 介绍Ansible 是 2012 年推出的一种通用自动化工具，Ansible 也是我接触的第一个自动化运维工具，Ansible 可以帮助我们完成一些批量任务，或者完成一些经常性的重复工作，在服务器集群场景下，Ansible 是我们运维的利器，Ansible 在2015年时被 Redhat 公司收购。Ansible 是使用 Python 语言编写的，它使用 SSH 协议在不同的机器上执行命令。Ansible 是无代理的，这使得更容易入手。你只需要在相关机器上安装 SSH 和 Python环境。Ansible 使用声明式 YAML 语言，编写 “playbook” 将一组主机 “hosts” 映射到定义明确的角色 也许你会说，我写个 shell 脚本不也一样能实现批量服务器的管理吗？这里我想说的是，Ansible 还支持一些很优秀的特性： 支持幂等性 No Agent 支持 Palybook 实现复杂的任务 使用 YAML 语言 先来说说什么是幂等性。假如我要在目标主机安装 Nginx 服务，但是我不确定这个主机是否已经安装了 Nginx 服务，当使用 Ansible 完成这个任务时，问题就会变得简单，如果目标主机已经安装 Nginx 服务，则 Ansible 不会进行任何操作，如果目标主机未安装 Nginx 服务， Ansible 才会开始工作，Ansible 是以导向为结果的，我们指定一个状态，Ansible 就会自动判断，把服务器的状态调整为我们指定的状态，我多次执行结果都是一样的，这就是幂等性 使用 Zabbix 监控一百台服务器，这一百台服务器都需要安装 Zabbix Agent，但是 Ansible 是不需要在管理节点上安装客户端代理程序的，因为它基于 SSH 工作，只要 Control node 能通过 SSH 连接到 Managed nodes 就能通过 Ansible 管理对应的管理节点，还有就是 Ansible 的控制节点不用单独启动服务，能直接运行命令 Ansible 的目标实现一切自动化 Ansible 的应用场景自动化部署应用 自动化管理配置 自动化的持续交付 自动化的云服务管理 自动化网络设备管理 Ansible 的工作原理 安装 Ansible 到控制节点，定义主机清单，编写好 Palybook，就能运行 Ansible 批量管理管理节点。步骤如下： 1.在控制节点上安装 Ansible 2.配置主机清单: 将管理节点的连接信息配置到控制节点的主机清单中 3.定义 Playbook：指定运行主机和执行任务 对节点主机的要求通常我们使用 SSH 与节点通信，默认使用 sftp 协议，如果 sftp 协议不可用，可在 ansible.cfg 配置文件中配置成 scp 的方式，在管理节点上也需要安装 Python 2 (2.6或更高版本) 或 Python 3 (3.5或更高版本) Ansible 的概念控制节点 (Control node)： 任何装有 Ansible 的机器可称为控制节点 ，你可以从任何控制节点运行命令和剧本，并调用 /usr/bin/ansible 或 /usr/bin/ansible-playbook 命令，你可以将任何安装了 Python 的计算机用作控制节点，笔记本电脑，共享桌面和服务器都可以运行 Ansible， 但是不能将 Windows 计算机用作控制节点 管理节点 (Managed nodes)： 使用 Ansible 管理的网络设备或服务器可称为管理节点，受管节点有时也称为主机 ，受管节点上不需要安装 Ansible 主机清单 (Inventory)： 托管节点的列表，库存文件有时也称为主机文件。你的目录可以为每个托管节点指定诸如 IP 地址之类的信息，库存还可以组织托管节点，创建和嵌套组，以便于扩展 模块 (Modules)： Ansible 执行的具体代码，每个模块都有特定的用途，从管理特定类型数据库的用户到管理特定类型网络设备上的 VLAN 接口。您可以使用任务调用单个模块，也可以调用剧本中的几个不同模块 任务 (Tasks)： Ansible 的行动单位，tasks 包含一组由 module 组成的任务列表，你可以使用特别的命令一次性执行单个任务 剧本 (Playbooks)： 保存了已排序的任务列表，因此可以按此顺序重复运行这些任务。剧本可以包括变量和任务。剧本是用 YAML 编写的，易于阅读、编写、共享和理解","categories":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Hello World","slug":"Hello World","date":"2019-03-28T04:14:29.000Z","updated":"2020-06-02T07:51:24.000Z","comments":true,"path":"post/4a17b156.html","link":"","permalink":"https://www.missf.top/post/4a17b156.html","excerpt":"","text":"所有无法深入问题本质的那些人，最终都将离开这个行业。","categories":[{"name":"Hello World","slug":"Hello-World","permalink":"https://www.missf.top/categories/Hello-World/"}],"tags":[{"name":"Hello World","slug":"Hello-World","permalink":"https://www.missf.top/tags/Hello-World/"}]},{"title":"我在人间凑数的日子","slug":"我在人间凑数的日子","date":"2019-03-28T04:14:29.000Z","updated":"2021-06-03T02:24:16.000Z","comments":true,"path":"post/world.html","link":"","permalink":"https://www.missf.top/post/world.html","excerpt":"","text":"语言这东西，在表达爱意的时候如此无力，在表达伤害的时候，却如此锋利。 你住的城市下雨了，想问你有没有带伞，可我不敢。因为我怕你说没带，而我又无能为力，就像是我爱你，却给不了你想要的。 十年太长，什么都会变。一辈子太短，一件事也有可能做不完。回忆永远站在背后，你无法抛弃，只能拥抱。 没有回音的山谷不值得纵身一跃。 世界上只有一种英雄、看透了生活的真相，却依然热爱生活。 你联系我，我就听你说，你不联系我，我就顺其自然；实不相瞒，我很想你，但我能控制，因为这样很酷。 我不知道凌晨五点该说晚安还是早安，也不知道这个年龄是该说爱还是喜欢。 曾经我发誓要把生命献给爱情，后来我没死，只是青春替我偿了命。 我与春风皆过客，你携秋水揽星河。 我曾踏足山巅，也曾进入低谷，二者都让我受益良多。 从此无心爱良夜，任他明月下西楼。 仅一夜之间，我的心判若两人。 真正的离别没有长亭古道，也没有劝君更尽一杯酒，只是在一个和往常一样的清晨，有的人留在昨天了。 我吹过你吹过的晚风，那我们算不算相拥。 明智的放弃胜过盲目的执着，去吹吹风吧，能清醒的话，感冒也没关系。 以后不见面的日子要按年算了。 没有特别挚爱的东西，没有一定要得到的人，也没有非做不可的事。 艺术，值得为之痛苦。 我于杀戮之中盛放，亦如黎明中的花朵。 生如蝼蚁，当立鸿鹄之志，命薄如纸，应有不屈之心。 好像什么都还来得及，又好像什么都无能为力。 旧时王谢堂前燕，飞入寻常百姓家。 越过山丘，才发现无人等候。 没有好好告别的人一定会重逢。 如果不是刻意相见，原来真的不会遇见。 有时候生活没那么好，有时候生活也没那么坏。 可能给不了你世间所有温柔，但有个词叫尽我所能。 真正爱你的人会督促你变的优秀，而不是蹉跎你的青春。 人间忽晚，山河已秋。 人间度日，何缘其身。我喜欢这种孤身只影的感觉，它让我孤独得像一个死去多年的人。 我原本可以接受所有的黑暗，如果我不曾见过光明。 我最遗憾的是，从未拥有过一个女孩的青春。 逢人不必言深，孤独本是常态。 种下一棵树最好的时间是十年前，其次是现在。 巅峰产生虚伪的拥护，黄昏见证真正的使徒。 所有命运馈赠的礼物，都早已在暗中标好了价格。 如不抽出时间来创造自己想要的生活，你最终将不得不花费大量的时间来应付自己不想要的生活。 往往最简单的东西里面，藏着最深刻的道理。 俄罗斯方块让我明白，成功会消失，错误会积累。贪吃蛇让我明白，越到后面越危险，最大的敌人是自己。 从来没有什么岁月静好，只不过是有人在替你负重前行！ 书上说，如果有一天你梦见了一个很久没见的人，代表她正在遗忘你。 虽然分手是我提的，但我很清楚是谁要走。 真实发生在身边的事情，让我瞬间明白了许多道理。 总有人会洗去生来的泥土，站在云端与诸神共舞。 等人是会上瘾的，因为等着等着，你会发现，如果你不等了，不是放弃了对方，而是背叛了自己。 人难免天生有自怜情绪，唯有时刻保持清醒，才能看清真正的价值在哪里。 可能是未来的架构师，也可能送外卖。 永恒燃烧的羽翼，带我脱离凡间的沉沦。 这世人的喧嚣之上，我追寻着荣光飞翔。 我拿起了母亲的剑，还有她的决心。 无意者烈火焚身，悲索者该当死罪，欺诈者判你无能，忤逆者烈焰加身，堕落者不可饶恕。 我们登上并非我们所选择的舞台，演出并非我们所选择的剧本。 读书是对平庸生活的一次越狱。 未觉池塘春草梦，阶前梧叶已秋声。 重读一本书也许会有新的感悟，但不会有新的结局。 人有时候真的挺奇怪的，不仅会沉迷虚容和宠溺，也会对委屈和痛苦上瘾。 如果失败的感觉不是那么糟的话，那么胜利的感觉不会那么美好。 肩负重担的感觉，就是一千个人翘首以盼你说的每一个字。 为何青春过得好似烂泥，活到最后才知痛苦与失败。 慢慢大家会明白的，无法跟喜欢的人在一起，其实是人生的常态。 水因受阻而出声，人因经历而成熟。 一个人最大的恶意，就是将自己的理解强加于别人，并一直认为自己是正确的。 自负，会让每个人都屈膝下跪。 爱意随风起却又随风散，这路遥马急的人间，我在你心里又能待几年。 你当自卑视己，切勿狂妄自大。 人这一生都在找寻曾经失去的东西。 真正要做的事，连神明也不要说。 你总不能三年都做不好一件事吧。 我在寺里偷偷替你求了平安，世人都不知道，只有菩萨知道。 Devops 不只是一个工具，还是一种文化，一项运动，一场革命。 如果我是那张不及格的试卷，那我想看一看标准答案。 人一定要愛著點什麼，才會活得心安理得。 人要走多远的路，才能把过去留在身后。 明明自己过得不尽人意，却偏偏见不得这人间的疾苦。 生活总是让人遍体鳞伤，但重要的不是治愈，而是带着病痛活下去。 年轻时最好不要遇上太惊艳的人，不然余生都是遗憾。 总有一天，你会笑着说出曾令你痛苦的事情。 总有人间一两风，填我十万八千梦。","categories":[{"name":"荒原饮露","slug":"荒原饮露","permalink":"https://www.missf.top/categories/%E8%8D%92%E5%8E%9F%E9%A5%AE%E9%9C%B2/"}],"tags":[{"name":"荒原饮露","slug":"荒原饮露","permalink":"https://www.missf.top/tags/%E8%8D%92%E5%8E%9F%E9%A5%AE%E9%9C%B2/"},{"name":"记忆","slug":"记忆","permalink":"https://www.missf.top/tags/%E8%AE%B0%E5%BF%86/"},{"name":"语录","slug":"语录","permalink":"https://www.missf.top/tags/%E8%AF%AD%E5%BD%95/"}]}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://www.missf.top/categories/Kubernetes/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://www.missf.top/categories/Ubuntu/"},{"name":"Backlog","slug":"Backlog","permalink":"https://www.missf.top/categories/Backlog/"},{"name":"Vim","slug":"Vim","permalink":"https://www.missf.top/categories/Vim/"},{"name":"性能测试","slug":"性能测试","permalink":"https://www.missf.top/categories/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"name":"k8s","slug":"k8s","permalink":"https://www.missf.top/categories/k8s/"},{"name":"Kubeadm","slug":"k8s/Kubeadm","permalink":"https://www.missf.top/categories/k8s/Kubeadm/"},{"name":"MariaDB","slug":"MariaDB","permalink":"https://www.missf.top/categories/MariaDB/"},{"name":"Maxscale","slug":"MariaDB/Maxscale","permalink":"https://www.missf.top/categories/MariaDB/Maxscale/"},{"name":"Elastic Stack","slug":"Elastic-Stack","permalink":"https://www.missf.top/categories/Elastic-Stack/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/categories/Prometheus/"},{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/categories/Docker/"},{"name":"CODING","slug":"CODING","permalink":"https://www.missf.top/categories/CODING/"},{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/categories/Ansible/"},{"name":"Hello World","slug":"Hello-World","permalink":"https://www.missf.top/categories/Hello-World/"},{"name":"荒原饮露","slug":"荒原饮露","permalink":"https://www.missf.top/categories/%E8%8D%92%E5%8E%9F%E9%A5%AE%E9%9C%B2/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://www.missf.top/tags/Kubernetes/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://www.missf.top/tags/Ubuntu/"},{"name":"Backlog","slug":"Backlog","permalink":"https://www.missf.top/tags/Backlog/"},{"name":"高并发","slug":"高并发","permalink":"https://www.missf.top/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"Vim","slug":"Vim","permalink":"https://www.missf.top/tags/Vim/"},{"name":"编辑器","slug":"编辑器","permalink":"https://www.missf.top/tags/%E7%BC%96%E8%BE%91%E5%99%A8/"},{"name":"性能测试","slug":"性能测试","permalink":"https://www.missf.top/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"name":"Wrk","slug":"Wrk","permalink":"https://www.missf.top/tags/Wrk/"},{"name":"k8s","slug":"k8s","permalink":"https://www.missf.top/tags/k8s/"},{"name":"Kubeadm","slug":"Kubeadm","permalink":"https://www.missf.top/tags/Kubeadm/"},{"name":"集群","slug":"集群","permalink":"https://www.missf.top/tags/%E9%9B%86%E7%BE%A4/"},{"name":"MariaDB","slug":"MariaDB","permalink":"https://www.missf.top/tags/MariaDB/"},{"name":"数据库","slug":"数据库","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"读写分离","slug":"读写分离","permalink":"https://www.missf.top/tags/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/"},{"name":"ELK","slug":"ELK","permalink":"https://www.missf.top/tags/ELK/"},{"name":"企业级日志系统","slug":"企业级日志系统","permalink":"https://www.missf.top/tags/%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"},{"name":"数据收集分析","slug":"数据收集分析","permalink":"https://www.missf.top/tags/%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90/"},{"name":"云计算","slug":"云计算","permalink":"https://www.missf.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.missf.top/tags/Prometheus/"},{"name":"监控","slug":"监控","permalink":"https://www.missf.top/tags/%E7%9B%91%E6%8E%A7/"},{"name":"Docker","slug":"Docker","permalink":"https://www.missf.top/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://www.missf.top/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"CODING","slug":"CODING","permalink":"https://www.missf.top/tags/CODING/"},{"name":"持续集成","slug":"持续集成","permalink":"https://www.missf.top/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"name":"Ansible","slug":"Ansible","permalink":"https://www.missf.top/tags/Ansible/"},{"name":"自动化运维","slug":"自动化运维","permalink":"https://www.missf.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"},{"name":"Hello World","slug":"Hello-World","permalink":"https://www.missf.top/tags/Hello-World/"},{"name":"荒原饮露","slug":"荒原饮露","permalink":"https://www.missf.top/tags/%E8%8D%92%E5%8E%9F%E9%A5%AE%E9%9C%B2/"},{"name":"记忆","slug":"记忆","permalink":"https://www.missf.top/tags/%E8%AE%B0%E5%BF%86/"},{"name":"语录","slug":"语录","permalink":"https://www.missf.top/tags/%E8%AF%AD%E5%BD%95/"}]}